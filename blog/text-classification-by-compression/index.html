<!doctype html><html lang=en><head><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogImMwMjJhMjg1LWJmY2YtNDc0OC1hYzczLTJhMDQ1Njk3NTI0YyIsICJpZCI6ICIzNjc3NjQ3Ny04MTE2LTRmYWQtYjcwMy1iZmM3YjMwZGJjMjMifQ.A0vHm-VWbXG6uBFZiwuspN_AyfSYNrdZE3IgwgWSt4g></script><meta charset=utf-8><meta name=generator content="Hugo 0.113.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/text-classification-by-compression/"><link rel=canonical href=https://maxhalford.github.io/blog/text-classification-by-compression/><meta property="og:title" content="Text classification by data compression"><meta property="og:description" content="Edit &ndash; I posted this on Hackernews and got some valuable feedback. Many brought up the fact that you should be able to reuse the internal state of the compressor instead of recompressing the training data each time a prediction is made. There&rsquo;s also some insightful references to data compression theory and its ties to statistical learning
Last night I felt like reading Artificial Intelligence: A Modern Approach. I stumbled on something fun in the natural language processing chapter."><meta property="og:type" content="article"><meta property="og:url" content="https://maxhalford.github.io/blog/text-classification-by-compression/"><meta property="og:image" content="https://maxhalford.github.io/img/beach.jpg"><meta property="article:section" content="blog"><meta property="article:published_time" content="2021-06-08T00:00:00+00:00"><meta property="article:modified_time" content="2021-06-08T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://maxhalford.github.io/img/beach.jpg"><meta name=twitter:title content="Text classification by data compression"><meta name=twitter:description content="Edit &ndash; I posted this on Hackernews and got some valuable feedback. Many brought up the fact that you should be able to reuse the internal state of the compressor instead of recompressing the training data each time a prediction is made. There&rsquo;s also some insightful references to data compression theory and its ties to statistical learning
Last night I felt like reading Artificial Intelligence: A Modern Approach. I stumbled on something fun in the natural language processing chapter."><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Text classification by data compression","headline":"Text classification by data compression","description":"Edit \u0026ndash; I posted this on Hackernews and got some valuable feedback. Many brought up the fact that you should be able to reuse the internal state of the compressor instead of recompressing the training data each time a prediction is made. There\u0026rsquo;s also some insightful references to data compression theory and its ties to statistical learning\nLast night I felt like reading Artificial Intelligence: A Modern Approach. I stumbled on something fun in the natural language processing chapter.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2021","datePublished":"2021-06-08 00:00:00 \u002b0000 UTC","dateModified":"2021-06-08 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/text-classification-by-compression\/","keywords":["machine-learning","text-processing"]}</script><title>Text classification by data compression â€¢ Max Halford</title><meta property="og:title" content="Text classification by data compression â€¢ Max Halford"><meta property="og:type" content="article"><meta name=description content="Edit &ndash; I posted this on Hackernews and got some valuable feedback. Many brought up the fact that you should be able to reuse the internal state of the compressor instead of recompressing the training data each time a prediction is made. There&rsquo;s also some insightful references to data compression theory and its ties to statistical learning
Last night I felt like reading Artificial Intelligence: A Modern Approach. I stumbled on something fun in the natural language processing chapter."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford ðŸ¦”</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>Text classification by data compression</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2021-06-08 00:00:00 UTC">2021-06-08</time>
<span class=posts-line-tag>machine-learning</span>
<span class=posts-line-tag>text-processing</span></div></div></header><div class="post-content markdown-body"><p><strong>Edit</strong> &ndash; <em>I posted this <a href="https://news.ycombinator.com/item?id=27440093">on Hackernews</a> and got some valuable feedback. Many brought up the fact that you should be able to reuse the internal state of the compressor instead of recompressing the training data each time a prediction is made. There&rsquo;s also some insightful references to data compression theory and its ties to statistical learning</em></p><p>Last night I felt like reading <a href=http://aima.cs.berkeley.edu/><em>Artificial Intelligence: A Modern Approach</em></a>. I stumbled on something fun in the natural language processing chapter. The section I was reading dealt with classifying text. The idea of the particular subsection I was reading was to classify documents by using a <a href=https://www.wikiwand.com/en/Data_compression>compression algorithm</a>. This is such a left field idea, and yet it does make sense when you think about it. To quote the book:</p><blockquote><p>In effect, compression algorithms are creating a language model. The <a href=https://www.wikiwand.com/en/Lempel%E2%80%93Ziv%E2%80%93Welch>LZW algorithm</a> in particular directly models a maximum-entropy probability distribution.</p></blockquote><p>In other words, a compression algorithm has some knowledge of the distribution of words in a corpus. It can thus be used to classify documents. The learning algorithm is quite straightforward:</p><ol><li>Take a labeled training set of documents.</li><li>Build a single document per label by concatenating the texts that belong to that label.</li><li>Compress each obtained document and measure the size of each result.</li></ol><p>To classify a document, proceed as so:</p><ol><li>Concatenate the document with the concatenated document of each label.</li><li>Compress each of these concatenations and measure the size.</li><li>Return the label for which the size increased the least.</li></ol><p>The idea is that if a document is similar to the training texts associated with a particular label, they will share patterns that will get exploited by the compression algorithm. Ideally, the size increase of compressing the training texts with the new text should be correlated with the similarity between the text and the training texts. The smaller the increase, the more likely the label should be assigned.</p><p>I think this is an elegant idea. It&rsquo;s not sophisticated, and I don&rsquo;t expect it to perform better than a plain and simple logistic regression. Moreover, it&rsquo;s expensive because the training texts of each label have to be recompressed for each test document. Still, I found the idea intriguing and decided to implement it in Python.</p><p>Here I&rsquo;ll use the <a href=http://qwone.com/~jason/20Newsgroups/>newsgroup20 dataset</a> from scikit-learn. I&rsquo;m using the same four categories they use in their <a href=https://scikit-learn.org/stable/datasets/real_world.html#converting-text-to-vectors>user guide</a> to have something to compare against.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>fetch_20newsgroups</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>categories</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;alt.atheism&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;talk.religion.misc&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;comp.graphics&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;sci.space&#39;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train</span> <span class=o>=</span> <span class=n>fetch_20newsgroups</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>subset</span><span class=o>=</span><span class=s1>&#39;train&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>categories</span><span class=o>=</span><span class=n>categories</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>The first step is to concatenate the texts that belong to each of the four categories. I&rsquo;m adding a space before each text so that the last word isn&rsquo;t glued with the first word of the next text.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>defaultdict</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>label_texts</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>str</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>text</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train</span><span class=p>[</span><span class=s1>&#39;data&#39;</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>    <span class=n>label</span> <span class=o>=</span> <span class=n>train</span><span class=p>[</span><span class=s1>&#39;target_names&#39;</span><span class=p>][</span><span class=n>train</span><span class=p>[</span><span class=s1>&#39;target&#39;</span><span class=p>][</span><span class=n>i</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>    <span class=n>label_texts</span><span class=p>[</span><span class=n>label</span><span class=p>]</span> <span class=o>+=</span> <span class=s1>&#39; &#39;</span> <span class=o>+</span> <span class=n>text</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span>
</span></span></code></pre></div><p>The next step is to compress each of these big texts and measure the size of the compressed result. It&rsquo;s quite easy to do this in Python. Indeed, Python provides high-level functions that compress a sequence of bytes into a smaller sequence of bytes. The <code>len</code> method gives us the number of bytes in the sequence. I picked <a href=https://docs.python.org/3/library/gzip.html><code>gzip</code></a> at random from the compression methods listed <a href=https://docs.python.org/3/library/archiving.html>here</a>. Each of these provides an easy-to-use <a href=https://docs.python.org/3/library/gzip.html#gzip.compress><code>compress</code></a> method.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gzip</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>METHOD</span> <span class=o>=</span> <span class=n>gzip</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>original_sizes</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>label</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>METHOD</span><span class=o>.</span><span class=n>compress</span><span class=p>(</span><span class=n>text</span><span class=o>.</span><span class=n>encode</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>label</span><span class=p>,</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>label_texts</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>original_sizes</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;comp.graphics&#39;</span><span class=p>:</span> <span class=mi>252268</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;talk.religion.misc&#39;</span><span class=p>:</span> <span class=mi>224228</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;sci.space&#39;</span><span class=p>:</span> <span class=mi>310524</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;alt.atheism&#39;</span><span class=p>:</span> <span class=mi>266440</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>That&rsquo;s all there is to the training phase. The training texts have to be kept in memory because they have to be used for the prediction phase. Let&rsquo;s say we&rsquo;re given a list of unlabeled texts. The idea for each text is to concatenate it with each training text, compress the result, and then measure the size of the compressed result.</p><p>When that&rsquo;s done, we just need to compare the obtained sizes with the original and return the label for which the size increased the least. Note that there is no notion of probability. There might be some weird way to cook up some probabilities, but they wouldn&rsquo;t be <a href=https://scikit-learn.org/stable/modules/calibration.html>calibrated</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>test</span> <span class=o>=</span> <span class=n>fetch_20newsgroups</span><span class=p>(</span><span class=n>subset</span><span class=o>=</span><span class=s1>&#39;test&#39;</span><span class=p>,</span> <span class=n>categories</span><span class=o>=</span><span class=n>categories</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>test</span><span class=p>[</span><span class=s1>&#39;data&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>sizes</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>label</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>METHOD</span><span class=o>.</span><span class=n>compress</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>label_text</span><span class=si>}</span><span class=s1> </span><span class=si>{</span><span class=n>text</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span><span class=si>}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>encode</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>label</span><span class=p>,</span> <span class=n>label_text</span> <span class=ow>in</span> <span class=n>label_texts</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>predicted_label</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>sizes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>label</span><span class=p>:</span> <span class=n>sizes</span><span class=p>[</span><span class=n>label</span><span class=p>]</span> <span class=o>-</span> <span class=n>original_sizes</span><span class=p>[</span><span class=n>label</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>predicted_label</span><span class=p>)</span>
</span></span></code></pre></div><p>So how well does this fair? Well, it takes over 5 minutes to run on my laptop for only 1,353 test cases. That&rsquo;s 0.2 seconds per document, which is rather slow! Here&rsquo;s the classification report:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>classification_report</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>test_labels</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>test</span><span class=p>[</span><span class=s1>&#39;target_names&#39;</span><span class=p>][</span><span class=n>label</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>test</span><span class=p>[</span><span class=s1>&#39;target&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>classification_report</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>test_labels</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>digits</span><span class=o>=</span><span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=p>))</span>
</span></span></code></pre></div><pre tabindex=0><code>                    precision    recall  f1-score   support

       alt.atheism      0.678     0.680     0.679       319
     comp.graphics      0.784     0.897     0.837       389
         sci.space      0.878     0.746     0.807       394
talk.religion.misc      0.609     0.614     0.611       251

          accuracy                          0.749      1353
         macro avg      0.737     0.734     0.733      1353
      weighted avg      0.754     0.749     0.749      1353
</code></pre><p>Is that good? No, not really. A multinomial Naive Bayes classifier achieves a macro F1-score of 0.88 with the same categories and train/test split. It&rsquo;s also dramatically faster. Is there anything we can do? We can try other compression algorithms. Here are the results for <a href=https://docs.python.org/3/library/zlib.html><code>zlib</code></a>, which was a bit faster and took just over 4 minutes to run:</p><pre tabindex=0><code>                    precision    recall  f1-score   support

       alt.atheism      0.656     0.687     0.671       319
     comp.graphics      0.782     0.902     0.838       389
         sci.space      0.880     0.744     0.806       394
talk.religion.misc      0.612     0.578     0.594       251

          accuracy                          0.745      1353
         macro avg      0.732     0.728     0.727      1353
      weighted avg      0.749     0.745     0.744      1353
</code></pre><p>The performance with <code>zlib</code> seems to be worse than with <code>gzip</code>. What about <a href=https://docs.python.org/3/library/bz2.html><code>bz2</code></a>?</p><pre tabindex=0><code>                    precision    recall  f1-score   support

       alt.atheism      0.648     0.110     0.188       319
     comp.graphics      0.732     0.584     0.649       389
         sci.space      0.923     0.305     0.458       394
talk.religion.misc      0.271     0.928     0.420       251

          accuracy                          0.455      1353
         macro avg      0.644     0.482     0.429      1353
      weighted avg      0.682     0.455     0.442      1353
</code></pre><p>The performance is awful in comparison. Also, it took more than 7 minutes to run. Now how about <a href=https://docs.python.org/3/library/lzma.html><code>lzma</code></a>?</p><pre tabindex=0><code>                    precision    recall  f1-score   support

       alt.atheism      0.851     0.875     0.862       319
     comp.graphics      0.923     0.959     0.941       389
         sci.space      0.927     0.934     0.930       394
talk.religion.misc      0.866     0.773     0.817       251

          accuracy                          0.897      1353
         macro avg      0.892     0.885     0.888      1353
      weighted avg      0.897     0.897     0.896      1353
</code></pre><p>The performance is surprisingly good! But this comes at a cost: it took 32 minutes to complete, which is ~1.4 seconds per document. Still, it&rsquo;s very interesting to see this kind of result for an algorithm that wasn&rsquo;t at all designed to classify documents. Well, the <code>lzma</code> in fact implements the LZW algorithm that is mentioned in the <em>Artificial Intelligence: A Modern Approach</em> book, which might explain why it does so well!</p><p>There&rsquo;s probably some fine-tuning that could be done to improve this kind of approach. However, it would most probably not be worth using such an approach because of the prohibitive computational cost. This should simply be seen as an interesting example of thinking outside the box. It also goes to show that statistical modelling and information theory are very much intertwined.</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px;margin-bottom:30px><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div><iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=32 width=114 style=border:0;border-radius:6px></iframe></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script>
<script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href=/blog/lightgbm-focal-loss/>Focal loss implementation for LightGBM</a></li><li><a href=/blog/undersampling-ratios/>Under-sampling a dataset with desired ratios</a></li><li><a href=/blog/textract-table-to-pandas/>Converting Amazon Textract tables to pandas DataFrames</a></li></ul></div></div></div></article><script></script></body></html>