<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Max Halford</title><link>https://maxhalford.github.io/</link><description>Recent content on Max Halford</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor>maxhalford25@gmail.com (Max Halford)</managingEditor><webMaster>maxhalford25@gmail.com (Max Halford)</webMaster><lastBuildDate>Thu, 21 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://maxhalford.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Organising a Kaggle InClass competition with a fairness metric</title><link>https://maxhalford.github.io/blog/fairness-competition/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/fairness-competition/</guid><description>Some context I co-organised a data science competition during the second half of 2020. This was in fact the 5th edition of the &amp;ldquo;D√©fi IA&amp;rdquo;, which is a recurring event that happens on a yearly basis. It is essentially a supervised machine learning competition for students from French speaking universities and engineering schools. This year was the first time that Kaggle was used to host the competition. Before that we used a custom platform that I wrote during my student years.</description></item><item><title>Converting Amazon Textract tables to pandas DataFrames</title><link>https://maxhalford.github.io/blog/textract-table-to-pandas/</link><pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/textract-table-to-pandas/</guid><description>I&amp;rsquo;m currently doing a lot of document processing at work. One of my tasks is to extract tables from PDF files. I evaluated Amazon Textract&amp;rsquo;s table extraction capability as part of this task. It&amp;rsquo;s very well documented, as is the rest of Textract. I was slightly disappointed by the examples, but nothing serious.
I wanted to write this short blog post to share a piece of code I use to convert tables extracted through Amazon Textract to pandas.</description></item><item><title>What my PhD was about</title><link>https://maxhalford.github.io/blog/phd-about/</link><pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/phd-about/</guid><description>I defended my PhD thesis on the 12th of October 2020, exactly 3 years and 11 days after having started it. The title of my PhD is Machine learning for query selectivity estimation in relational databases. I thought it would be worthwhile to summarise what I did. Note sure anyone will read this, but at least I&amp;rsquo;ll be able to remember what I did when I grow old and senile.</description></item><item><title>Computing cross-correlations in SQL</title><link>https://maxhalford.github.io/blog/sql-cross-correlations/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sql-cross-correlations/</guid><description>Introduction I&amp;rsquo;m currently working on a problem at work where I have to measure the impact of a growth initiative on a performance metric. Hypothetically, this might to answer the following kind of question:
I&amp;rsquo;ve spent X amount of money, what is the impact on the number of visitors on my website?
Of course, there are many measures that can be taken to answer such a question. I decided to measure the correlation between the initiative and the metric, with the latter being shifted forward in time.</description></item><item><title>Unsupervised text classification with word embeddings</title><link>https://maxhalford.github.io/blog/unsupervised-text-classification/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/unsupervised-text-classification/</guid><description>Addendum: since writing this article, I have discovered that the method I describe is a form of zero-shot learning. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.
I recently watched a lecture by Adam Tauman Kalai on stereotype bias in text data. The lecture is very good, but something that had nothing to do with the lecture&amp;rsquo;s main topic caught my intention.</description></item><item><title>Focal loss implementation for LightGBM</title><link>https://maxhalford.github.io/blog/lightgbm-focal-loss/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/lightgbm-focal-loss/</guid><description>Edit &amp;ndash; 2021-01-26
I initially wrote this blog post using version 2.3.1 of LightGBM. I&amp;rsquo;ve now updated it to use version 3.1.1. There are a couple of subtle but important differences between version 2.x.y and 3.x.y. If you&amp;rsquo;re using version 2.x.y, then I strongly recommend you to upgrade to version 3.x.y.
Motivation If you&amp;rsquo;re reading this blog post, then you&amp;rsquo;re likely to be aware of LightGBM. The latter is a best of breed gradient boosting library.</description></item><item><title>A few intermediate pandas tricks</title><link>https://maxhalford.github.io/blog/pandas-tricks/</link><pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/pandas-tricks/</guid><description>I want to use this post to share some pandas snippets that I find useful. I use them from time to time, in particular when I&amp;rsquo;m doing time series competitions on platforms such as Kaggle. Like any data scientist, I perform similar data processing steps on different datasets. Usually, I put repetitive patterns in xam, which is my personal data science toolbox. However, I think that the following snippets are too small and too specific for being added into a library.</description></item><item><title>The correct way to evaluate online machine learning models</title><link>https://maxhalford.github.io/blog/online-learning-evaluation/</link><pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-learning-evaluation/</guid><description>Motivation Most supervised machine learning algorithms work in the batch setting, whereby they are fitted on a training set offline, and are used to predict the outcomes of new samples. The only way for batch machine learning algorithms to learn from new samples is to train them from scratch with both the old samples and the new ones. Meanwhile, some learning algorithms are online, and can predict as well as update themselves when new samples are available.</description></item><item><title>Server-sent events in Flask without extra dependencies</title><link>https://maxhalford.github.io/blog/flask-sse-no-deps/</link><pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/flask-sse-no-deps/</guid><description>Server-sent events (SSE) is a mechanism for sending updates from a server to a client. The fundamental difference with WebSockets is that the communication only goes in one direction. In other words, the client cannot send information to the server. For many usecases this is all you might need. Indeed, if you just want to receive notifications/updates/messages, then using a WebSocket is overkill. Once you&amp;rsquo;ve implemented the SSE functionality on your server, then all you need on a JavaScript client is an EventSource.</description></item><item><title>I got plagiarized and Google didn't help</title><link>https://maxhalford.github.io/blog/plagiarism-google-didnt-help/</link><pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/plagiarism-google-didnt-help/</guid><description>One of my most popular articles is the one on target encoding. It gets a fair amount of mentions on Kaggle discussions and I see it pop up from time to time in other contexts. It also brings brought me around 2500 unique monthly viewers. That&amp;rsquo;s quite a chunk of people for an unambitious blogger like me. Up to a few months ago, my article was on the first page of Google when you typed in searches such as &amp;ldquo;target encoding python&amp;rdquo; and &amp;ldquo;bayesian target encoding&amp;rdquo;.</description></item><item><title>Speeding up scikit-learn for single predictions</title><link>https://maxhalford.github.io/blog/speeding-up-sklearn-single-predictions/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/speeding-up-sklearn-single-predictions/</guid><description>It is now common practice to train machine learning models offline before putting them behind an API endpoint to serve predictions. Specifically, we want an API route which can make a prediction for a single row/instance/sample/data point/individual (call it what you want). Nowadays, we have great tools to do this that care of the nitty-gritty details, such as Cortex, MLFlow, Kubeflow, and Clipper. There are also paid services that hold your hand a bit more, such as DataRobot, H2O, and Cubonacci.</description></item><item><title>Bayesian linear regression for practitioners</title><link>https://maxhalford.github.io/blog/bayesian-linear-regression/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/bayesian-linear-regression/</guid><description>Motivation Suppose you have an infinite stream of feature vectors $x_i$ and targets $y_i$. In this case, $i$ denotes the order in which the data arrives. If you&amp;rsquo;re doing supervised learning, then your goal is to estimate $y_i$ before it is revealed to you. In order to do so, you have a model which is composed of parameters denoted $\theta_i$. For instance, $\theta_i$ represents the feature weights when using linear regression.</description></item><item><title>Under-sampling a dataset with desired ratios</title><link>https://maxhalford.github.io/blog/undersampling-ratios/</link><pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/undersampling-ratios/</guid><description>Introduction I&amp;rsquo;ve just spent a few hours looking at under-sampling and how it can help a classifier learn from an imbalanced dataset. The idea is quite simple: randomly sample the majority class and leave the minority class untouched. There are more sophisticated ways to do this &amp;ndash; for instance by creating synthetic observations from the minority class √† la SMOTE &amp;ndash; but I won&amp;rsquo;t be discussing that here.
I checked out the imblearn library and noticed they have an implementation of random under-sampling aptly named RandomUnderSampler.</description></item><item><title>Finding fuzzy duplicates with pandas</title><link>https://maxhalford.github.io/blog/transitive-duplicates/</link><pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/transitive-duplicates/</guid><description>Duplicate detection is the task of finding two or more instances in a dataset that are in fact identical. As an example, take the following toy dataset:
First name Last name Email 0 Erlich Bachman eb@piedpiper.com 1 Erlich Bachmann eb@piedpiper.com 2 Erlik Bachman eb@piedpiper.co 3 Erlich Bachmann eb@piedpiper.com Each of these instances (rows, if you prefer) corresponds to the same &amp;ldquo;thing&amp;rdquo; &amp;ndash; note that I&amp;rsquo;m not using the word &amp;ldquo;entity&amp;rdquo; because entity resolution is a different, and yet related, concept.</description></item><item><title>A smooth approach to putting machine learning into production</title><link>https://maxhalford.github.io/blog/machine-learning-production/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/machine-learning-production/</guid><description>Putting machine learning into production is hard. Usually I&amp;rsquo;m doubtful of such statements, but in this case I&amp;rsquo;ve never met anyone for whom everything has gone smoothly. Most data scientists might agree that there is a huge gap between their local environment and a live environment. In fact, &amp;ldquo;productionalizing&amp;rdquo; machine learning is such a complex topic that entire companies have risen to address the issue. I&amp;rsquo;m not just talking about running a gigantic grid search and finding the best model, I&amp;rsquo;m talking about putting a machine learning model live so that it actually has a positive impact on your business/project.</description></item><item><title>Skyline queries in Python</title><link>https://maxhalford.github.io/blog/skyline-queries/</link><pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/skyline-queries/</guid><description>Imagine that you&amp;rsquo;re looking to buy a home. If you have an analytical mind then you might want to tackle this with a quantitative. Let&amp;rsquo;s suppose that you have a list of potential homes, and each home has some attributes that can help you compare them. As an example, we&amp;rsquo;ll consider three attributes:
The price of the house, which you want to minimize The size of the house, which you want to maximize The city where the house if located, which you don&amp;rsquo;t really care about Some houses will be objectively better than others because they will be cheaper and bigger.</description></item><item><title>SQL subquery enumeration</title><link>https://maxhalford.github.io/blog/sql-subquery-enumeration/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sql-subquery-enumeration/</guid><description>I recently stumbled on a rather fun problem during my PhD. I wanted to generate all possible subqueries from a given SQL query. In this case an example is easily worth a 1000 thousand words. Take the following SQL query:
SELECT * FROM customers AS c, purchases AS p, shops AS s WHERE p.customer_id = c.id AND p.shop_id = s.id AND c.nationality = &amp;#39;Swedish&amp;#39; AND c.hair = &amp;#39;Blond&amp;#39; AND s.city = &amp;#39;Stockholm&amp;#39; Here all the possible subqueries that can be generated from the above query.</description></item><item><title>Morellet crosses with JavaScript</title><link>https://maxhalford.github.io/blog/morellet/</link><pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/morellet/</guid><description>The days I&amp;rsquo;m working on a deep learning project. I hate it but I promised myself to give it a real try. My scripts are taking a long time so I decided to do some procedural art while I waited. This time I&amp;rsquo;m going to reproduce the following crosses made by Fran√ßois Morellet. I saw them the last I went to the Mus√©e Pompidou with some friends from university. I don&amp;rsquo;t have any smartphone anymore so one my friends was kind enough to take a few pictures for me, including this one.</description></item><item><title>Streaming groupbys in pandas for big datasets</title><link>https://maxhalford.github.io/blog/pandas-streaming-groupby/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/pandas-streaming-groupby/</guid><description>If you&amp;rsquo;ve done a bit of Kaggling, then you&amp;rsquo;ve probably been typing a fair share of df.groupby(some_col). That is, if you&amp;rsquo;re using Python. If you&amp;rsquo;re handling tabular data, then a lot of your features will revolve around computing aggregate statistics. This is very true for the ongoing PLAsTiCC Astronomical Classification challenge. The goal of the competition is to classify objects in the sky into one of 14 groups. The bulk of the available data is a set of so-called light curve.</description></item><item><title>Target encoding done the right way</title><link>https://maxhalford.github.io/blog/target-encoding/</link><pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/target-encoding/</guid><description>When you&amp;rsquo;re doing supervised learning, you often have to deal with categorical variables. That is, variables which don&amp;rsquo;t have a natural numerical representation. The problem is that most machine learning algorithms require the input data to be numerical. At some point or another a data science pipeline will require converting categorical variables to numerical variables.
There are many ways to do so:
Label encoding where you choose an arbitrary number for each category One-hot encoding where you create one binary column per category Vector representation a.</description></item><item><title>Stella triangles with JavaScript</title><link>https://maxhalford.github.io/blog/stella-triangles/</link><pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/stella-triangles/</guid><description>Around the same time last year I visited the San Francisco Museum of Modern Art. Frank Stella&amp;rsquo;s compositions really caught my eye. When I saw them I started thinking about how I could write a computer program to imitate his work. In this post I&amp;rsquo;m going to attempt to reproduce his so-called V Series.
Nice and simple right? Indeed in a lot of his work Frank Stella uses straight lines without much randomness.</description></item><item><title>Unknown pleasures with JavaScript</title><link>https://maxhalford.github.io/blog/unknown-pleasures/</link><pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/unknown-pleasures/</guid><description>No this blog post is not about how nice JavaScript can be, instead it&amp;rsquo;s just another one of my attempts at reproducing modern art with procedural generation and the HTML5 &amp;lt;canvas&amp;gt; element. This time I randomly generated images resembling the cover of the album by Joy Division called &amp;ldquo;Unknown Pleasures&amp;rdquo;.
According to Wikipedia, this somewhat iconic album cover is based on radio waves. I saw a poster of it in a bar not long ago and decided to reproduce the next time I had some time to kill.</description></item><item><title>Subsampling a training set to match a test set - Part 1</title><link>https://maxhalford.github.io/blog/subsampling-1/</link><pubDate>Mon, 19 Jun 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/subsampling-1/</guid><description>Some friends and I recently qualified for the final of the 2017 edition of the Data Science Game competition. The first part was a Kaggle competition with data provided by Deezer. The problem was a binary classification task where one had to predict if a user was going to listen to a song that was proposed to him. Like many teams we extracted clever features and trained an XGBoost classifier, classic.</description></item><item><title>Halftoning with Go - Part 2</title><link>https://maxhalford.github.io/blog/halftoning-2/</link><pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/halftoning-2/</guid><description>The next stop on my travel through the world of halftoning will be the implementation of Weighted Voronoi Stippling as described in Adrian Secord&amp;rsquo;s 2002 paper. This method is more involved than the ones I detailed in my previous blog post, however the results are quite interesting. Again, I did the implementation in Go.
Notice the black dot in the middle of the white square? Overview I found a fair amount of resources about the method, most of them being implementations of Adrian Secord&amp;rsquo;s paper.</description></item><item><title>Grid paintings √† la Mondrian with JavaScript</title><link>https://maxhalford.github.io/blog/mondrian/</link><pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/mondrian/</guid><description>I was at a laundrette today and had just finished my book so I had some time to kill. Naturally I devised an algorithm for generating drawings that would resemble the grid-like paintings that Piet Mondrian made famous. With the benefit of hindsight I guess I could indulge in saner activities while waiting for my laundry to dry!
I went through different ideas but in the end I settled on a recursive approach.</description></item><item><title>A short introduction and conclusion to the OpenBikes 2016 Challenge</title><link>https://maxhalford.github.io/blog/openbikes-challenge/</link><pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/openbikes-challenge/</guid><description>During my undergraduate internship in 2015 I started a side project called OpenBikes. The idea was to visualize and analyze bike sharing over multiple cities. Axel Bellec joined me and in 2016 we won a national open data competition. Since then we haven&amp;rsquo;t pursued anything major, instead we use OpenBikes to try out technologies and to apply concepts we learn at university and online.
Before the 2016 summer holidays one of my professors, Aur√©lien Garivier mentioned that he was considering using our data for a Kaggle-like competition between some statistics curriculums in France.</description></item><item><title>Halftoning with Go - Part 1</title><link>https://maxhalford.github.io/blog/halftoning-1/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/halftoning-1/</guid><description>Recently I stumbled upon this webpage which shows how to use a TSP solver as a halftoning technique. I began to read about related concepts like dithering and stippling. I don&amp;rsquo;t have any background in photography but I can appreciate the visual appeal of these techniques. As I understand it these techniques were first invented to save ink for printing. However nowadays printing has become cheaper and the modern use of these technique is mostly aesthetic, at least for images.</description></item><item><title>Recursive polygons with JavaScript</title><link>https://maxhalford.github.io/blog/recursive-polygons/</link><pubDate>Fri, 25 Mar 2016 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/recursive-polygons/</guid><description>I like modern art, I enjoy looking at the stuff that was made at the beginning of the 20th century and thinking how it is still shaping today&amp;rsquo;s style. I&amp;rsquo;m not an expert, it&amp;rsquo;s just a hobby of mine. I especially like the Centre Pompidou in Paris, it&amp;rsquo;s got loads of fascinating stuff. While I was going through the galleries it struck me that some of the paintings were very geometrical.</description></item><item><title>The Na√Øve Bayes classifier</title><link>https://maxhalford.github.io/blog/naive-bayes/</link><pubDate>Thu, 10 Sep 2015 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/naive-bayes/</guid><description>The objective of a classifier is to decide to which class (also called label) to assign an observation based on observed data. In supervised learning, this is done by taking into account previous classifications. In other words if we know that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the training set.</description></item><item><title>An introduction to genetic algorithms</title><link>https://maxhalford.github.io/blog/genetic-algorithms-introduction/</link><pubDate>Sun, 02 Aug 2015 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/genetic-algorithms-introduction/</guid><description>The goal of genetic algorithms (GAs) is to solve problems whose solutions are not easily found (ie. NP problems, nonlinear optimization, etc.). For example, finding the shortest path from A to B in a directed graph is easily done with Djikstra&amp;rsquo;s algorithm, it can be solved in polynomial time. However the time to find the smallest path that joins all points on a non-directed graph, also known as the Travelling Salesman Problem (TSP) increases exponentially as the number of points increases.</description></item><item><title>Setting up a droplet to host a Flask app</title><link>https://maxhalford.github.io/blog/flask-droplet/</link><pubDate>Tue, 14 Jul 2015 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/flask-droplet/</guid><description>After having worked for some weeks on the OpenBikes website, it was time to put it online. Digital Ocean seemed to provide a good service and so I decided to give it a spin. Their documentation is quite good but it doesn&amp;rsquo;t cover exactly everything for setting up Flask. In this post I simply want to record every single step I took.
OpenBikes is a project with a Flask backend and a few upstart jobs.</description></item><item><title>Visualizing bike stations live data</title><link>https://maxhalford.github.io/blog/bike-stations/</link><pubDate>Wed, 03 Jun 2015 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/bike-stations/</guid><description>Recently some friends and I decided to launch openbikes.co, a website for visualizing (and later on analyzing) urban bike traffic. We have a lot of ideas that we will progressively implement. Anyway, the point is that all of it started with me fiddling about with the JCDecaux API and the leaflet.js library and I would like to share it with you. Shall we?
Presentation In this post I want to show you the tools and the code to get a fully functional website for visualizing live data.</description></item><item><title>An introduction to symbolic regression</title><link>https://maxhalford.github.io/slides/symbolic-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/slides/symbolic-regression/</guid><description>An introduction to symbolic regression Max Halford - PhD student IRIT/IMT Toulouse Data Science Meetup - December 2017 .center[ .left-column[![tds_logo](/assets/img/presentations/tds_logo.jpeg)] .right-column[![xgp_logo](/assets/img/presentations/xgp_logo.png)] ] --- layout: true # Symbolic regression --- ## Quick overview - The goal is to evolve "programs" with selection, mutation, and crossover - Selection keeps programs that perform well - Mutation changes a piece of the program - Crossover combines two programs --- --- ## Example programs --- ## Kaggle Titanic top 1% üö¢ 2 years ago [scirpus](https://www.</description></item><item><title>Bio</title><link>https://maxhalford.github.io/bio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/bio/</guid><description>Hello üëã
I&amp;rsquo;m a data analyst/engineer/scientist working at Alan. I&amp;rsquo;m half British üá¨üáß and half Belgian üáßüá™. I went to university in Toulouse, France üá´üá∑. My academic üéì background is a mix of maths üßÆ, economics üí∏, and computer science üñ•Ô∏è. I got hooked into data science in 2014 after watching Moneyball ‚öæ and reading The Signal and the Noise üìñ. My PhD topic had to do with database query optimisation and machine learning ü§ñ.</description></item><item><title>Links</title><link>https://maxhalford.github.io/links/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/links/</guid><description>Papers PhD - Statistical learning for selectivity estimation in relational databases (manuscript, slides) Selectivity correction with online machine learning - BDA 2020 Selectivity Estimation with Attribute Value Dependencies using Linked Bayesian Networks - TLDKS 2020 An Approach Based on Bayesian Networks for Query Selectivity Estimation - DASFAA, 2019 Entropic Variable Projection for Explainability and Intepretability - 2018 Master 2 year internship at HelloFresh (report, slides) Master 1 year internship at Privateaser (report, slides) Undergraduate internship at INSA Toulouse (report, slides) Detailed solutions to the first 30 Project Euler problems Talks The challenges of online machine learning in production - Ita√∫ Unibanco Meetup 2021 Quelle est l‚Äôempreinte √©cologique du Big Data?</description></item></channel></rss>