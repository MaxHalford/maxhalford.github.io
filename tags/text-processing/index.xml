<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>text-processing on Max Halford</title><link>https://maxhalford.github.io/tags/text-processing/</link><description>Recent content in text-processing on Max Halford</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor>maxhalford25@gmail.com (Max Halford)</managingEditor><webMaster>maxhalford25@gmail.com (Max Halford)</webMaster><lastBuildDate>Sun, 20 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://maxhalford.github.io/tags/text-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>Parsing garment descriptions with GPT-3</title><link>https://maxhalford.github.io/blog/garment-parsing-gpt3/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/garment-parsing-gpt3/</guid><description>The task You&amp;rsquo;ll have heard of GPT-3 if you haven&amp;rsquo;t been hiding under a rock. I&amp;rsquo;ve recently been impressed by Nat Friedman teaching GPT-3 to use a browser, and SeekWell generating SQL queries from free-text. I think the most exciting usecases are yet to come. But GPT-3 has a good chance of changing the way we approach mundane tasks at work.
I wrote an article a couple of months ago about a boring task I have to do at work.</description></item><item><title>NLP at Carbonfact: how would you do it?</title><link>https://maxhalford.github.io/blog/carbonfact-nlp-open-problem/</link><pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/carbonfact-nlp-open-problem/</guid><description>The task I work at a company called Carbonfact. Our core value proposal is computing the carbon footprint of clothing items, expressed in carbon dioxide equivalent &amp;ndash; $kgCO_2e$ in short. For instance, we started by measuring the footprint of shoes &amp;ndash; no pun intended. We do these measurements with life cycle analysis (LCA) software we built ourselves. We use these analyses to fuel higher-level tasks for our clients, such as carbon accounting and sustainable procurement.</description></item><item><title>Fuzzy regex matching in Python</title><link>https://maxhalford.github.io/blog/fuzzy-regex-matching-in-python/</link><pubDate>Mon, 04 Apr 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/fuzzy-regex-matching-in-python/</guid><description>Fuzzy string matching in a nutshell Say we&amp;rsquo;re looking for a pattern in a blob of text. If you know the text has no typos, then determining whether it contains a pattern is trivial. In Python you can use the in function. You can also write a regex pattern with the re module from the standard library. But what about if the text contains typos? For instance, this might be the case with user inputs on a website, or with OCR outputs.</description></item><item><title>OCR spelling correction is hard</title><link>https://maxhalford.github.io/blog/ocr-spelling-correction-is-hard/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/ocr-spelling-correction-is-hard/</guid><description>I recently saw SymSpell pop up on Hackernews. It claims to be a million times faster than Peter Norvig&amp;rsquo;s spelling corrector. I think it&amp;rsquo;s great that there&amp;rsquo;s a fast open source solution for spelling correction. But in my experience, the most challenging aspect of spelling correction is not necessarily speed.
When I worked at Alan, I mostly wrote logic to extract structured information from medical documents. After some months working on the topic, I have to admit I hadn&amp;rsquo;t cracked the problem.</description></item><item><title>Homoglyphs: different characters that look identical</title><link>https://maxhalford.github.io/blog/homoglyphs/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/homoglyphs/</guid><description>A wild homoglyph appears For instance, can you tell if there&amp;rsquo;s a difference between H and Η? How about N and Ν? These characters may seem identical, but they are actually different. You can try this out for yourself in Python:
&amp;gt;&amp;gt;&amp;gt; &amp;#39;H&amp;#39; == &amp;#39;Η&amp;#39; False &amp;gt;&amp;gt;&amp;gt; &amp;#39;N&amp;#39; == &amp;#39;Ν&amp;#39; False Indeed, these all represent different Unicode characters:
&amp;gt;&amp;gt;&amp;gt; ord(&amp;#39;H&amp;#39;), ord(&amp;#39;Η&amp;#39;) (72, 919) &amp;gt;&amp;gt;&amp;gt; ord(&amp;#39;N&amp;#39;), ord(&amp;#39;Ν&amp;#39;) (78, 925) Η in fact represents the capital Eta letter, while Ν is a capital Nu.</description></item><item><title>Automated document processing at Alan</title><link>https://maxhalford.github.io/blog/medium-document-processing/</link><pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/medium-document-processing/</guid><description/></item><item><title>Text classification by data compression</title><link>https://maxhalford.github.io/blog/text-classification-by-compression/</link><pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/text-classification-by-compression/</guid><description>Edit: I posted this on Hackernews and got some valuable feedback. Many brought up the fact that you should be able to reuse the internal state of the compressor instead of recompressing the training data each time a prediction is made. There&amp;rsquo;s also some insightful references to data compression theory and its ties to statistical learning.
Last night I felt like reading Artificial Intelligence: A Modern Approach. I stumbled on something fun in the natural language processing chapter.</description></item><item><title>Reducing the memory footprint of a scikit-learn text classifier</title><link>https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/</link><pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/</guid><description>Context This week at Alan I&amp;rsquo;ve been working on parsing French medical prescriptions. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image.</description></item><item><title>Converting Amazon Textract tables to pandas DataFrames</title><link>https://maxhalford.github.io/blog/textract-table-to-pandas/</link><pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/textract-table-to-pandas/</guid><description>I&amp;rsquo;m currently doing a lot of document processing at work. One of my tasks is to extract tables from PDF files. I evaluated Amazon Textract&amp;rsquo;s table extraction capability as part of this task. It&amp;rsquo;s very well documented, as is the rest of Textract. I was slightly disappointed by the examples, but nothing serious.
I wanted to write this short blog post to share a piece of code I use to convert tables extracted through Amazon Textract to pandas.</description></item><item><title>Unsupervised text classification with word embeddings</title><link>https://maxhalford.github.io/blog/unsupervised-text-classification/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/unsupervised-text-classification/</guid><description>Addendum: since writing this article, I have discovered that the method I describe is a form of zero-shot learning. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.
I recently watched a lecture by Adam Tauman Kalai on stereotype bias in text data. The lecture is very good, but something that had nothing to do with the lecture&amp;rsquo;s main topic caught my attention.</description></item></channel></rss>