<!doctype html><html lang=en><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-63302552-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-63302552-1')</script><script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script><meta charset=utf-8><meta name=generator content="Hugo 0.82.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/bayesian-linear-regression/"><link rel=canonical href=https://maxhalford.github.io/blog/bayesian-linear-regression/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Bayesian linear regression for practitioners","headline":"Bayesian linear regression for practitioners","description":"Motivation Suppose you have an infinite stream of feature vectors $x_i$ and targets $y_i$. In this case, $i$ denotes the order in which the data arrives. If you\u0026rsquo;re doing supervised learning, then your goal is to estimate $y_i$ before it is revealed to you. In order to do so, you have a model which is composed of parameters denoted $\\theta_i$. For instance, $\\theta_i$ represents the feature weights when using linear regression.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2020","datePublished":"2020-02-26 00:00:00 \u002b0000 UTC","dateModified":"2020-02-26 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/bayesian-linear-regression\/","keywords":[]}</script><title>Bayesian linear regression for practitioners - Max Halford</title><meta property="og:title" content="Bayesian linear regression for practitioners - Max Halford"><meta property="og:type" content="article"><meta name=description content="Motivation Suppose you have an infinite stream of feature vectors $x_i$ and targets $y_i$. In this case, $i$ denotes the order in which the data arrives. If you&rsquo;re doing supervised learning, then your goal is to estimate $y_i$ before it is revealed to you. In order to do so, you have a model which is composed of parameters denoted $\theta_i$. For instance, $\theta_i$ represents the feature weights when using linear regression."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link href="https://fonts.googleapis.com/css?family=PT+Serif|PT+Sans|Permanent+Marker" rel=stylesheet><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0,tags:'ams'},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll('mjx-container').forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=post-header><header><div class="signatures site-title"><a href=/>Max Halford</a></div></header><div class="row end-xs"><div><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></div><div class=header-line></div></div><header class=post-header><h1 class=post-title>Bayesian linear regression for practitioners</h1><div class="row post-desc"><div class=col-xs-12><time class=post-date datetime="2020-02-26 00:00:00 UTC">2020-02-26 Â· 34 minute read</time></div></div></header><div class="post-content markdown-body"><h2 id=toc>Table of contents</h2><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a></li><li><a href=#bayesian-inference-with-colors>Bayesian inference with colors</a></li><li><a href=#online-belief-updating>Online belief updating</a></li><li><a href=#the-case-of-linear-regression>The case of linear regression</a></li><li><a href=#progressive-validation>Progressive validation</a></li><li><a href=#some-visualisation>Some visualisation</a></li><li><a href=#prediction-intervals>Prediction intervals</a></li><li><a href=#mini-batching>Mini-batching</a></li><li><a href=#handling-concept-drift>Handling concept drift</a></li><li><a href=#zero-mean-isotropic-gaussian-prior>Zero mean isotropic Gaussian prior</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav><h2 id=motivation>Motivation</h2><p>Suppose you have an infinite stream of feature vectors $x_i$ and targets $y_i$. In this case, $i$ denotes the order in which the data arrives. If you&rsquo;re doing supervised learning, then your goal is to estimate $y_i$ <em>before</em> it is revealed to you. In order to do so, you have a model which is composed of parameters denoted $\theta_i$. For instance, $\theta_i$ represents the feature weights when using linear regression. After a while, $y_i$ will be revealed, which will allow you to update $\theta_i$ and thus obtain $\theta_{i+1}$. To perform the update, you may apply whichever learning rule you wish &ndash; for instance most people use <a href=https://www.wikiwand.com/en/Stochastic_gradient_descent#/Extensions_and_variants>some flavor of stochastic gradient descent</a>. The process I just described is called <a href=https://www.wikiwand.com/en/Online_machine_learning>online supervised machine learning</a>. The difference between online machine learning and the more traditional batch machine learning is that an online model is dynamic and learns on the fly. Online learning solves a lot of pain points in real-world environments, mostly because it doesn&rsquo;t require retraining models from scratch every time new data arrives.</p><p>Most of the popular online machine learning models are built around some form of stochastic gradient descent. Moreover, they are typically used to make point predictions via maximum likelihood estimation. A different &ndash; and generally more desirable &ndash; approach is to estimate $y_i$ by using a distribution. In other words, instead of predicting the most likely output, we would rather like to get an idea of the spread of values that $y_i$ might take. Having a distribution instead of a single value gives an idea of the uncertainty of the model, which is obviously a nice thing to have. Such is one of the many benefits of using <em>Bayesian inference</em>.</p><h2 id=bayesian-inference-with-colors>Bayesian inference with colors</h2><p>I&rsquo;ve stumbled on many blogs, posts, textbooks, slides, that discussed Bayesian inference. In my opinion, it&rsquo;s a hard topic, and it has many rabbit holes that go on for ever. Chapter 3 of <a href=https://www.wikiwand.com/en/Christopher_Bishop>Christopher Bishop</a>&rsquo;s <a href=https://cds.cern.ch/record/998831/files/9780387310732_TOC.pdf>classical book</a> is a very good reference, but it leaves me unsatisfied with regards to practical aspects &ndash; the book is not available for free, but <a href=https://cedar.buffalo.edu/~srihari/CSE574/Chap3/3.4-BayesianRegression.pdf>these slides</a> contain all the material from chapter 3 on Bayesian linear regression. This is mostly because I am too lazy to take the time required to understand everything written in Bishop&rsquo;s book. Here&rsquo;s a bunch of blog posts that helped me get a clearer understanding:</p><ul><li><a href=https://dtransposed.github.io/blog/Bayesian-Linear-Regression.html><em>Sequential Bayesian Learning - Linear Regression</em></a> by dtransposed</li><li><a href=https://koaning.io/posts/bayesian-propto-streaming/><em>Bayesian/Streaming Algorithms</em></a> by Vincent Warmerdam</li><li><a href=https://zjost.github.io/bayesian-linear-regression/><em>Bayesian Linear Regression Tutorial</em></a> by zjost</li><li><a href=https://dfdazac.github.io/03-bayes_lr_ex.html><em>Sequential Bayesian linear regression</em></a> by Daniel Daza</li></ul><p>In this post I would like to present a (my) pragmatic view of Bayesian inference, focused on online machine learning and practical aspects. Before getting into the code, I would like to give a generic overview of the topic. However, if you&rsquo;re in a hurry and want to dig into the code straight away, feel free to <a href=#the-case-of-linear-regression>move on forward</a>. I&rsquo;ve purposefully chosen a mathematical notation that is particularly well suited to online machine learning. I&rsquo;ve taken a bit of freedom with regards to the notation; if you&rsquo;re part of the <a href=http://mirror.uncyc.org/wiki/Mathematical_Inquisition>mathematical inquisition</a> then take a chill pill. Note that there will probably be a fair bit of overlap with the blog posts I listed above, but I don&rsquo;t think that matters too much.</p><div style=border:3px;border-style:solid;border-color:#228b22;padding:1em;padding-bottom:0;margin-bottom:15px>I like to think of a Bayesian model as a set of blocks. For forecasting purposes, the block we're interested in is called the <span style=color:#228b22>predictive distribution</span>. The predictive distribution is the distribution of the target $y_i$ given a set of features $x_i$. We'll write it down as so:
<span style=color:#228b22>$$p(y_i | x_i)$$</span><p>This is the distribution we want to obtain. It&rsquo;s the Holy Grail for many practitioners who want to take into account predictive uncertainty. Later on we&rsquo;ll see how the predictive distribution is obtained by assembling the rest of the blocks.</p></div><div style=border:3px;border-style:solid;border-color:#cd5c5c;padding:1em;padding-bottom:0;margin-bottom:15px>The next block is the <span style=color:#cd5c5c>likelihood</span>, which is the probability distribution of an observation $y_i$ conditioned on the current model parameters $\theta_i$ and a set of features $x_i$. In other words, given the current state of the model, the likelihood tells you how realistic it is to observe the pair $(x_i, y_i)$. We'll write it down as follows:
<span style=color:#cd5c5c>$$p(y_i | x_i, \theta_i)$$</span><p>The thing to understand is that the likelihood is usually imposed by the problem you&rsquo;re dealing with. In textbooks, the likelihood is often chosen to be a Gaussian or a binomial, mostly because these distributions occur naturally in textbook problems. However, the likelihood can be any parametric distribution. The key idea is that the likelihood is something defined by the problem at hand, and usually isn&rsquo;t something you have to much freedom with.</p></div><div style=border:3px;border-style:solid;border-color:#4169e1;padding:1em;padding-bottom:0;margin-bottom:15px>Next, we have the <span style=color:#4169e1>prior distribution</span> of the model parameters. From an online learning perspective, I like to think of this as the current distribution of the model parameters. This will get clearer later on, I promise! We'll simply denote the prior distribution as so:
<span style=color:#4169e1>$$p(\theta_i)$$</span><p>Choosing a prior is important, because for our case it can add regularization to our model. The trick is that if we choose a prior distribution that is so-called <i>conjugate</i> for the <span style=color:#cd5c5c>likelihood</span>, then we get access to analytical formulas for updating the model parameters. If, however, the prior and the likelihood are not compatible with each other, then we have to resort to using approximate methods such as <a href=ttps://twiecki.io/blog/2015/11/10/mcmc-sampling>MCMC</a> and <a href=https://www.wikiwand.com/en/Variational_Bayesian_methods>variational inference</a>. As cool and trendy as they may be, these tools are mostly designed for situations where all the data is available at once. In other words they are not applicable in a streaming context, whereas analytical formulas are.</p></div><div style=border:3px;border-style:solid;border-color:#9370db;padding:1em;padding-bottom:0;margin-bottom:15px>Finally, the <span style=color:#9370db>posterior distribution</span>
represents the distribution of the model parameters $\theta_{i+1}$ once we've received a new pair $(x_{i}, y_{i})$. For online learning purposes, here is how we're going to write it down:
<span style=color:#9370db>$$p(\theta_{i+1} | \theta_{i}, x_{i}, y_{i})$$</span><p>As you might have guessed or already know, the <span style=color:#9370db>posterior distribution</span>
is obtained by combining the <span style=color:#cd5c5c>likelihood</span>
of $(x_{i}, y_{i})$ and the <span style=color:#4169e1>prior distribution</span>
of the current model parameters $\theta_{i}$. As a mnemonic, <span style=color:#cd5c5c>red</span>
$+$ <span style=color:#4169e1>blue</span>
$=$ <span style=color:#9370db>purple</span>
.</p></div><p>Now that we have all our blocks, we need to put them together. It&rsquo;s quite straightforward once you understand how the blocks are related. <span style=color:#cd5c5c>You start off with the likelihood, which is a probability distribution you have to choose</span>. For example if you&rsquo;re looking to predict counts then you would use a Poisson distribution. I&rsquo;m refraining from giving a more detailed example simply because we will be going over one later on. What matters for the while is to develop an intuition, and for that I want to keep the notation as general as possible. <span style=color:#4169e1>Once you have settled on a likelihood, you need to choose a prior distribution for the model parameters</span>. Because our focus is on online learning, we want to have access to quick analytical formulas, and not MCMC voodoo. In order to so, we need to pick a prior distribution which is conjugate to the likelihood. Note that most distributions have at least one other distribution which is conjugate to them, as detailed <a href=https://www.wikiwand.com/en/Conjugate_prior#/Table_of_conjugate_distributions>here</a>. <span style=color:#9370db>Now that you have decided which likelihood to use and what prior to associate with it, you may derive the posterior distribution of the model parameters</span>. This operation is the cornerstone of Bayesian inference, and is done via <a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">Bayes' rule</a>:</p><p>$$\begin{equation}
\textcolor{mediumpurple}{p(\theta_{i+1} | \theta_i, x_i, y_i)} = \frac{\textcolor{indianred}{p(y_i | x_i, \theta_i)} \textcolor{royalblue}{p(\theta_i)}}{p(x_i, y_i)}
\end{equation}$$</p><p>Now you may be wondering what $p(x_i, y_i)$ is. It turns out it is the distribution of the data, and is something that we don&rsquo;t know! Indeed, if we knew the generating process of the data, then we wouldn&rsquo;t really have to be doing machine learning in the first place, right? There are however analytical formulas that use the rest of the information at our disposal &ndash; namely the <span style=color:#4169e1>prior</span>
and the <span style=color:#cd5c5c>likelihood</span>
&ndash; but they require the likelihood and the prior to be conjugate to each other. These formulas involve a sequence of mathematical steps which we will omit. All you have to know is that if the <span style=color:#4169e1>prior</span>
and the <span style=color:#cd5c5c>likelihood</span>
are conjugate to each other, then an analytical formula for computing the posterior is available, which allows us to perform online learning. To keep things general, we will simply write down:</p><p>$$\begin{equation}
\textcolor{mediumpurple}{p(\theta_{i+1} | \theta_i, x_i, y_i)} \propto \textcolor{indianred}{p(y_i | x_i, \theta_i)} \textcolor{royalblue}{p(\theta_i)}
\end{equation}$$</p><p>The previous statement simply expresses the fact that the posterior distribution of the model parameters is proportional to the product of the likelihood and the prior distribution. In other words, in can be obtained using an analytical formula that is specific to the chosen likelihood and prior distribution. If we&rsquo;re being pragmatic, then what we&rsquo;re really interested in is to obtain the <span style=color:#228b22>predictive distribution</span>, which is obtained by marginalizing over the model parameters $\theta_i$:</p><p>$$\begin{equation}
\textcolor{forestgreen}{p(y_i | x_i)} = \int \textcolor{indianred}{p(y_i | \textbf{w}, x_i)} \textcolor{royalblue}{p(\textbf{w})} d\textbf{w}
\end{equation}$$</p><p>Again, this isn&rsquo;t analytically tractable, except if the likelihood and the prior are conjugate to each other. The equation does make sense though, because essentially we&rsquo;re computing a weighted average of the potential $y_i$ values for each possible model parameter $\textbf{w}$.</p><p>$$\begin{equation}
\textcolor{forestgreen}{p(y_i | x_i)} \propto \textcolor{indianred}{p(y_i | x_i, \theta_i)} \textcolor{royalblue}{p(\theta_i)}
\end{equation}$$</p><p>Basically, the thing to remember is that the <span style=color:#228b22>predictive distribution</span> can be obtained by mixing the <span style=color:#cd5c5c>likelihood</span> and the <span style=color:#4169e1>current distribution of the weights</span>.</p><h2 id=online-belief-updating>Online belief updating</h2><p>The important result of the previous section is that we can get update the distribution of the parameters when a new pair $(x_i, y_i)$ arrives:</p><p>$$\begin{equation}
\textcolor{mediumpurple}{p(\theta_{i+1} | \theta_i, x_i, y_i)} \propto \textcolor{indianred}{p(x_i, y_i | \theta_i)} \textcolor{royalblue}{p(\theta_i)}
\end{equation}$$</p><p>Before any data comes in, the model parameters follow the initial distribution we picked, which is $p(\theta_0)$. At this point, if we&rsquo;re asked to predict $y_0$, then it&rsquo;s predictive distribution would be obtained as so:</p><p>$$\begin{equation}
\textcolor{forestgreen}{p(y_0 | x_0)} \propto \textcolor{indianred}{p(y_0 | x_0, \theta_0)} \textcolor{royalblue}{p(\theta_0)}
\end{equation}$$</p><p>Next, once the first observation $(x_0, y_0)$ arrives, we can update the distribution of the parameters:</p><p>$$\begin{equation}
\textcolor{mediumpurple}{p(\theta_1 | \theta_0, x_0, y_0)} \propto \textcolor{indianred}{p(x_0, y_0 | \theta_0)} \textcolor{royalblue}{p(\theta_0)}
\end{equation}$$</p><p>The predictive distribution, given a set of features $x_1$, is thus:</p><p>$$\begin{equation}
\textcolor{forestgreen}{p(y_1 | x_1)} \propto \textcolor{indianred}{p(y_1 | x_1, \theta_1)} \underbrace{\textcolor{mediumpurple}{p(\theta_1 | \theta_0, x_0, y_0)}}_{\textcolor{royalblue}{p(\theta_1)}}
\end{equation}$$</p><p>The previous equations expresses the fact that the prior of the weights for the current iteration is the posterior of the weights at the previous iteration. Once the second pair $(x_1, y_1)$ is available, the distribution of the model parameters is updated in the same way as before:</p><p>$$\begin{equation}
\textcolor{mediumpurple}{p(\theta_2 | \theta_1, x_1, y_1)} \propto \textcolor{indianred}{p(y_1 | x_1, \theta_1)} \underbrace{\textcolor{indianred}{p(y_0 | x_0, \theta_0)} \textcolor{royalblue}{p(\theta_0)}}_{\textcolor{royalblue}{p(\theta_1)}}
\end{equation}$$</p><p>When the pair $(x_2, y_2)$ arrives, the distribution of the weights will be obtained as so:</p><p>$$\begin{equation}
\textcolor{mediumpurple}{p(\theta_3 | \theta_2, x_2, y_2)} \propto \textcolor{indianred}{p(y_2 | x_2, \theta_2)} \underbrace{\textcolor{indianred}{p(y_1 | x_1, \theta_1)} \underbrace{\textcolor{indianred}{p(y_0 | x_0, \theta_0)} \textcolor{royalblue}{p(\theta_0)}}_{\textcolor{royalblue}{p(\theta_1)}} }_{\textcolor{royalblue}{p(\theta_2)}}
\end{equation}$$</p><p>Hopefully, by now you&rsquo;ve understood that there is recursive relationship that links each iteration: the posterior distribution at step $i$ becomes the prior distribution at step $i+1$. This simple fact is the reason why analytical Bayesian inference can naturally be used as an online machine learning algorithm. Indeed, we only need to store the current distribution of the weights to make everything work. When I started to understand this for the first time, I found it slightly magical.</p><p>On a sidenote, I want to mention that this presentation of Bayesian inference is very much &ldquo;old school&rdquo;. Most people who do Bayesian inference use MCMC and variational inference techniques. These tools are really cool, and I highly recommend checking out libraries such as <a href=https://github.com/stan-dev/stan>Stan</a>, <a href=https://docs.pymc.io/>PyMC3</a> &ndash; (and <a href=https://github.com/pymc-devs/pymc4>PyMC4</a> which will be it&rsquo;s successor), <a href=https://github.com/blei-lab/edward>Edward</a>, and <a href=https://github.com/pyro-ppl/pyro>Pyro</a>. The issue with these tools is that they require having all the training data available in memory, and thus are not able to learn in an online manner. There do however seem to be some variants that work online, such as <a href=https://www.wikiwand.com/en/Particle_filter>sequential Monte Carlo</a> and <a href=http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf>stochastic variational inference</a>. In my experience these algorithms work well for mini-batches, but not necessarily in a pure online setting where the observations are processed one-by-one. I will probably be discussing these methods in a future blog post.</p><h2 id=the-case-of-linear-regression>The case of linear regression</h2><p>Up until now we didn&rsquo;t give any useful example. We will now see how to perform linear regression by using Bayesian inference. In a linear regression, the model parameters $\theta_i$ are just weights $w_i$ that are linearly applied to a set of features $x_i$:</p><p>$$\begin{equation}
y_i = w_i x_i^\intercal + \epsilon_i
\end{equation}$$</p><p>Each prediction is the scalar product between $p$ features $x_i$ and $p$ weights $w_i$. The trick here is that we&rsquo;re going to assume that the noise $\epsilon_i$ follows a given distribution. In particular, we will be boring and use the Gaussian <a href=https://www.wikiwand.com/en/Ansatz>ansatz</a>, which implies that the likelihood function is a Gaussian distribution:</p><p>$$\begin{equation}
\color{indianred} p(y_i | x_i, w_i) = \mathcal{N}(w_i x_i^\intercal, \beta^{-1})
\end{equation}$$</p><p>Christopher Bishop calls $\beta$ the &ldquo;noise precision parameter&rdquo;. In statistics, the <a href=https://www.wikiwand.com/en/Precision_(statistics)>precision</a> is inversely related to the noise variance as so: $\beta = \frac{1}{\sigma^2}$. Basically, it translates our belief on how noisy the target distribution is. Both concepts coexist mostly because statisticians can&rsquo;t agree on a common Bible. There are ways to tune this parameter automatically from the data. However, for the purpose of simplicity, in this blog post we will &ldquo;treat it as a known constant&rdquo; &ndash; I&rsquo;m quoting Christopher Bishop. In any case, the appropriate prior distribution for the above likelihood function is the <a href=https://www.wikiwand.com/en/Multivariate_normal_distribution>multivariate Gaussian distribution</a>:</p><p>$$\begin{equation}
\color{royalblue} p(w_0) = \mathcal{N}(m_0, S_0)
\end{equation}$$</p><p>$m_0$ is the mean of the distribution while $S_0$ is it&rsquo;s covariance matrix. Initially, their initial values will be:</p><p>$$\begin{equation}
m_0 = (0, \dots , 0)
\end{equation}$$</p><p>$$\begin{equation}
S_0 = \begin{pmatrix} \alpha^{-1} & \dots & \dots \\ \dots & \alpha^{-1} & \dots \\ \dots & \dots & \alpha^{-1} \end{pmatrix}
\end{equation}$$</p><p>We can now determine the posterior distribution of the weights:</p><p>$$\begin{equation}
\color{mediumpurple} p(w_{i+1} | w_i, x_i, y_i) = \mathcal{N}(m_{i+1}, S_{i+1})
\end{equation}$$</p><p>$$\begin{equation}
S_{i+1} = (S_i^{-1} + \beta x_i^\intercal x_i)^{-1}
\end{equation}$$</p><p>$$\begin{equation}
m_{i+1} = S_{i+1}(S_i^{-1} m_i + \beta x_i y_i)
\end{equation}$$</p><p>Note that $x_i^\intercal x_i$ is the <a href=https://www.wikiwand.com/en/Outer_product>outer product</a> of $x_i$ with itself. I&rsquo;m using the convention where $x_i$ is a row and not a column. The outer product would be denoted as $x_i x_i^\intercal$ if $x_i$ were instead a column. By now you might be thinking that I&rsquo;ve produced these formulas from thin air, and you would be right. The steps for getting to these formulas are quite straightforward, assuming your calculus is not too rusty. However I won&rsquo;t be going into them in this blog post. If you want to go deeper into the maths, I recommend getting Christopher Bishop&rsquo;s and/or checking out <a href="https://www.youtube.com/watch?v=nrd4AnDLR3U&list=PLD0F06AA0D2E8FFBA&index=61">this video</a>. We can also obtain the predictive distribution:</p><p>$$\begin{equation}
\color{forestgreen} p(y_i) = \mathcal{N}(\mu_i, \sigma_i)
\end{equation}$$</p><p>$$\begin{equation}
\mu_i = w_i x_i^\intercal
\end{equation}$$</p><p>$$\begin{equation}
\sigma_i = \frac{1}{\beta} + x_i S_i x_i^\intercal
\end{equation}$$</p><p>If you&rsquo;re a programmer/hacker/practitioner, then the two previous sets of formulas are all you need to get rolling. Assuming someone has worked out the analytical solution for you, the formulas are quite straightforward to implement. That&rsquo;s the sweet and sour conundrum of analytical Bayesian inference: the math is relatively hard to work out, but once you&rsquo;re done it&rsquo;s devilishly simple to implement. I&rsquo;m going to use Python and define a class with two methods: <code>learn</code> and <code>fit</code>. The <code>learn</code> method is what most Pythonistas call <code>fit</code>. I use <code>learn</code> because I feel that it conveys more meaning. I determined that the most efficient way to proceed is to store the inverse of the covariance matrix instead of it&rsquo;s non-inverted version. All in all the code is quite simply, mostly thanks to <a href=https://numpy.org/>numpy</a> which takes care of all the linear algebra details. I made it so that the <code>predict</code> method returns an instance of <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html><code>scipy.stats.norm</code></a>. This instance is nothing more than a 1D probability distribution with useful methods such as <code>.mean()</code>, <code>.std()</code>, <code>.interval()</code>, and <code>.pdf()</code>. From what I gather this isn&rsquo;t a very efficient way proceed, but it sure is convenient.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=kn>from</span> <span class=nn>scipy</span> <span class=kn>import</span> <span class=n>stats</span>

<span class=k>class</span> <span class=nc>BayesLinReg</span><span class=p>:</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_features</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>beta</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>n_features</span> <span class=o>=</span> <span class=n>n_features</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>=</span> <span class=n>beta</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_features</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>identity</span><span class=p>(</span><span class=n>n_features</span><span class=p>)</span> <span class=o>/</span> <span class=n>alpha</span>

    <span class=k>def</span> <span class=nf>learn</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>

        <span class=c1># Update the inverse covariance matrix (Bishop eq. 3.51)</span>
        <span class=n>cov_inv</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>

        <span class=c1># Update the mean vector (Bishop eq. 3.50)</span>
        <span class=n>cov</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>cov_inv</span><span class=p>)</span>
        <span class=n>mean</span> <span class=o>=</span> <span class=n>cov</span> <span class=err>@</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=err>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>y</span> <span class=o>*</span> <span class=n>x</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=o>=</span> <span class=n>cov_inv</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>=</span> <span class=n>mean</span>

        <span class=k>return</span> <span class=bp>self</span>

    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>

        <span class=c1># Obtain the predictive mean (Bishop eq. 3.58)</span>
        <span class=n>y_pred_mean</span> <span class=o>=</span> <span class=n>x</span> <span class=err>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean</span>

        <span class=c1># Obtain the predictive variance (Bishop eq. 3.59)</span>
        <span class=n>w_cov</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span><span class=p>)</span>
        <span class=n>y_pred_var</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>+</span> <span class=n>x</span> <span class=err>@</span> <span class=n>w_cov</span> <span class=err>@</span> <span class=n>x</span><span class=o>.</span><span class=n>T</span>

        <span class=k>return</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=n>y_pred_mean</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=n>y_pred_var</span> <span class=o>**</span> <span class=o>.</span><span class=mi>5</span><span class=p>)</span>

    <span class=nd>@property</span>
    <span class=k>def</span> <span class=nf>weights_dist</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>cov</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>stats</span><span class=o>.</span><span class=n>multivariate_normal</span><span class=p>(</span><span class=n>mean</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>mean</span><span class=p>,</span> <span class=n>cov</span><span class=o>=</span><span class=n>cov</span><span class=p>)</span>
</code></pre></div><p>Now that we&rsquo;ve implemented Bayesian linear regression, let&rsquo;s use it!</p><h2 id=progressive-validation>Progressive validation</h2><p>In this blog post, I&rsquo;m mostly interested in the online learning capabilities of Bayesian linear regression. In an online learning scenario, we can use <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.3925&rep=rep1&type=pdf">progressive validation</a> to measure the performance of a model. The idea is simple: when an observation $(x_t, y_t)$ arrives, we make a prediction $\hat{y}_t = f(x_t)$, then and only then we update the model. We can then compare the sequence of $y_t$s and $\hat{y}_t$s to get an idea of how well the model did. This method is quite well-known &ndash; for instance it is mentioned in <a href=https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/41159.pdf>a paper</a> by researchers from Google (see subsection 5.1). The benefit of this validation scheme is that all the data acts as a training set as well as a test set, which allows us to skip performing $k$-fold cross-validation. It&rsquo;s also very easy to put in place:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
<span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>metrics</span>

<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_boston</span><span class=p>(</span><span class=n>return_X_y</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>BayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>alpha</span><span class=o>=.</span><span class=mi>3</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>))</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)):</span>
    <span class=n>y_pred</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span>

<span class=k>print</span><span class=p>(</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
</code></pre></div><p>This produces a mean absolute error of around <code>3.784</code>. To get an idea of how good or bad this is, we&rsquo;ll train an instance of scikit-learn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html><code>SGDRegressor</code></a> in the same manner and use it&rsquo;s performance as a reference.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>exceptions</span>
<span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>linear_model</span>
<span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>preprocessing</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>SGDRegressor</span><span class=p>(</span><span class=n>eta0</span><span class=o>=.</span><span class=mi>15</span><span class=p>)</span>  <span class=c1># here eta0 is the learning rate</span>

<span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>))</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>scale</span><span class=p>(</span><span class=n>X</span><span class=p>),</span> <span class=n>y</span><span class=p>)):</span>
    <span class=k>try</span><span class=p>:</span>
        <span class=n>y_pred</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>([</span><span class=n>xi</span><span class=p>])[</span><span class=mi>0</span><span class=p>]</span>
    <span class=k>except</span> <span class=n>exceptions</span><span class=o>.</span><span class=n>NotFittedError</span><span class=p>:</span>
        <span class=n>y_pred</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.</span>
    <span class=n>model</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>([</span><span class=n>xi</span><span class=p>],</span> <span class=p>[</span><span class=n>yi</span><span class=p>])</span>

<span class=k>print</span><span class=p>(</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
</code></pre></div><p>There are a couple of differences with the previous snippet: the data is scaled with <a href=https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html><code>preprocessing.scale</code></a> because stochastic gradient descent works better that way; furthermore we need to catch <a href=https://scikit-learn.org/stable/modules/generated/sklearn.exceptions.NotFittedError.html><code>exceptions.NotFittedError</code></a> in order for the first prediction to not fail and default to 0. This produces a mean absolute error of around <code>4.172</code>, which is worse than the Bayesian linear regression. In other words our implementation of Bayesian linear regression seems to be working quite well. Naturally we could tinker with the parameters of the <code>SGDRegressor</code> &ndash; trust me, I have! &ndash; but from what I&rsquo;ve tried on other datasets they seem to have a somewhat similar performance.</p><h2 id=some-visualisation>Some visualisation</h2><p>In a Bayesian linear regression, the weights follow a distribution that quantifies their uncertainty. In the case where there are two features &ndash; and therefore two weights in a linear regression &ndash; this distribution can be represented with a <a href=https://www.itl.nist.gov/div898/handbook/eda/section3/contour.htm>contour plot</a>. As for the predictive distribution, which quantifies the uncertainty of the model regarding the spread of possible feature values, we can visualize it with a shaded area, as is sometimes done in <a href=https://www.wikiwand.com/en/Control_chart>control charts</a>. The following piece of code contains all the steps for producing a visualization of both distributions. The data is generated by taking samples from a linear regression of fixed parameters with some Gaussian noise added to the output.</p><details><summary>Click to see the code</summary><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>mpl_toolkits.axes_grid1</span> <span class=kn>import</span> <span class=n>ImageGrid</span>

<span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=c1># Pick some true parameters that the model has to find</span>
<span class=n>weights</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=o>-.</span><span class=mi>3</span><span class=p>,</span> <span class=o>.</span><span class=mi>5</span><span class=p>])</span>

<span class=k>def</span> <span class=nf>sample</span><span class=p>(</span><span class=n>n</span><span class=p>):</span>
    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n</span><span class=p>):</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)])</span>
        <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=o>.</span><span class=mi>2</span><span class=p>)</span>
        <span class=k>yield</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>BayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>25</span><span class=p>)</span>

<span class=c1># The following 3 variables are just here for plotting purposes</span>
<span class=n>N</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>w</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span>
<span class=n>W</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dstack</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=n>w</span><span class=p>))</span>

<span class=n>n_samples</span> <span class=o>=</span> <span class=mi>5</span>
<span class=n>fig</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>7</span> <span class=o>*</span> <span class=n>n_samples</span><span class=p>,</span> <span class=mi>21</span><span class=p>))</span>
<span class=n>grid</span> <span class=o>=</span> <span class=n>ImageGrid</span><span class=p>(</span>
    <span class=n>fig</span><span class=p>,</span> <span class=mi>111</span><span class=p>,</span>  <span class=c1># similar to subplot(111)</span>
    <span class=n>nrows_ncols</span><span class=o>=</span><span class=p>(</span><span class=n>n_samples</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span>  <span class=c1># creates a n_samplesx3 grid of axes</span>
    <span class=n>axes_pad</span><span class=o>=.</span><span class=mi>5</span>  <span class=c1># pad between axes in inch.</span>
<span class=p>)</span>

<span class=c1># We&#39;ll store the features and targets for plotting purposes</span>
<span class=n>xs</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>ys</span> <span class=o>=</span> <span class=p>[]</span>

<span class=k>def</span> <span class=nf>prettify_ax</span><span class=p>(</span><span class=n>ax</span><span class=p>):</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>set_xlim</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>set_ylim</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;$w_1$&#39;</span><span class=p>)</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;$w_2$&#39;</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>ax</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>sample</span><span class=p>(</span><span class=n>n_samples</span><span class=p>)):</span>

    <span class=n>pred_dist</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span>

    <span class=c1># Prior weight distribution</span>
    <span class=n>ax</span> <span class=o>=</span> <span class=n>prettify_ax</span><span class=p>(</span><span class=n>grid</span><span class=p>[</span><span class=mi>3</span> <span class=o>*</span> <span class=n>i</span><span class=p>])</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;Prior weight distribution #{i + 1}&#39;</span><span class=p>)</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>contourf</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=n>w</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>weights_dist</span><span class=o>.</span><span class=n>pdf</span><span class=p>(</span><span class=n>W</span><span class=p>),</span> <span class=n>N</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>)</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=o>*</span><span class=n>weights</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>)</span>  <span class=c1># true weights the model has to find</span>

    <span class=c1># Update model</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span>

    <span class=c1># Prior weight distribution</span>
    <span class=n>ax</span> <span class=o>=</span> <span class=n>prettify_ax</span><span class=p>(</span><span class=n>grid</span><span class=p>[</span><span class=mi>3</span> <span class=o>*</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>])</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;Posterior weight distribution #{i + 1}&#39;</span><span class=p>)</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>contourf</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=n>w</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>weights_dist</span><span class=o>.</span><span class=n>pdf</span><span class=p>(</span><span class=n>W</span><span class=p>),</span> <span class=n>N</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>)</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=o>*</span><span class=n>weights</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>)</span>  <span class=c1># true weights the model has to find</span>

    <span class=c1># Posterior target distribution</span>
    <span class=n>xs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span>
    <span class=n>ys</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>yi</span><span class=p>)</span>
    <span class=n>posteriors</span> <span class=o>=</span> <span class=p>[</span><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=n>wi</span><span class=p>]))</span> <span class=k>for</span> <span class=n>wi</span> <span class=ow>in</span> <span class=n>w</span><span class=p>]</span>
    <span class=n>ax</span> <span class=o>=</span> <span class=n>prettify_ax</span><span class=p>(</span><span class=n>grid</span><span class=p>[</span><span class=mi>3</span> <span class=o>*</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>2</span><span class=p>])</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;Posterior target distribution #{i + 1}&#39;</span><span class=p>)</span>
    <span class=c1># Plot the old points and the new points</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>([</span><span class=n>xi</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=k>for</span> <span class=n>xi</span> <span class=ow>in</span> <span class=n>xs</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]],</span> <span class=n>ys</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>xs</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>],</span> <span class=n>ys</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;*&#39;</span><span class=p>)</span>
    <span class=c1># Plot the predictive mean along with the predictive interval</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=p>[</span><span class=n>p</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>posteriors</span><span class=p>],</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>)</span>
    <span class=n>cis</span> <span class=o>=</span> <span class=p>[</span><span class=n>p</span><span class=o>.</span><span class=n>interval</span><span class=p>(</span><span class=o>.</span><span class=mi>95</span><span class=p>)</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>posteriors</span><span class=p>]</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>fill_between</span><span class=p>(</span>
        <span class=n>x</span><span class=o>=</span><span class=n>w</span><span class=p>,</span>
        <span class=n>y1</span><span class=o>=</span><span class=p>[</span><span class=n>ci</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=k>for</span> <span class=n>ci</span> <span class=ow>in</span> <span class=n>cis</span><span class=p>],</span>
        <span class=n>y2</span><span class=o>=</span><span class=p>[</span><span class=n>ci</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=k>for</span> <span class=n>ci</span> <span class=ow>in</span> <span class=n>cis</span><span class=p>],</span>
        <span class=n>alpha</span><span class=o>=.</span><span class=mi>1</span>
    <span class=p>)</span>
    <span class=c1># Plot the true target distribution</span>
    <span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>xi</span><span class=p>])</span> <span class=k>for</span> <span class=n>xi</span> <span class=ow>in</span> <span class=n>w</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>)</span>
</code></pre></div></details><p><img src=/img/blog/bayesian-linear-regression/viz.png alt=viz></p><p>The first column represents the prior distribution of the weights. In other words, it shows what the model think the parameters should be, before seeing the next sample. The second column shows the distribution of the weights after the model has processed the next sample. In other words it represents the posterior distribution. You&rsquo;ll notice that the prior distribution at each row is equal to the posterior distribution of the previous row. This stems from the fact that the posterior after having seen a sample becomes the prior for the next sample. Finally, the third column shows how this impacts the uncertainty of the posterior predictive distribution. Intuitively, as more samples arrive, the uncertainty lowers and the shaded area &ndash; which is a 95% predictive interval &ndash; becomes slimmer. Likewise, the fact that the ellipse representing the weight distribution shrinks indicates that the model is growing in confidence. As we will see later on, this isn&rsquo;t always a good thing &ndash; hint: <a href=https://www.wikiwand.com/en/Concept_drift>concept drift</a>.</p><h2 id=prediction-intervals>Prediction intervals</h2><p>A nice property about Bayesian models is that they allow to quantify the uncertainty of predictions. In practical terms, the <code>predict</code> method of our implementation outputs a statistical distribution &ndash; to be precise, an instance of <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html><code>scipy.stats.norm</code></a>. Therefore, we have access to an array of tools for free. For instance, we can use the <code>interval</code> method to obtain an interval in which it is likely that the prediction belongs. This interval, called the prediction interval, is a topic which confuses a lot of practitioners &ndash; including yours truly. I recommend reading <a href=https://www.wikiwand.com/en/Prediction_interval>this Wikipedia article</a> to clarify your mind on the subject.</p><p>The thing to understand is that we&rsquo;re using a parametric model, therefore the correctness of our prediction intervals is based on the assumption that the model choices we have made are valid. For instance, we are assuming that the likelihood follows a Gaussian distribution. Other models, such as gradient boosting, are non-parametric, and produce prediction intervals that are <a href=https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html>(almost) always reliable</a>. Nonetheless, we can perform a visual check to see how reliable these prediction intervals actually are. To do so, we can check to see if the next target value is contained in the prediction interval. We can then calculate a running average of the amount of times where this occurs and display it along time. If we pick a confidence level of, say, 0.95, then we&rsquo;re expecting to see around 95% of the predictions contained in the prediction interval. In the following snippet we&rsquo;ll use the same <code>sample</code> method we used in the previous section.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>BayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>25</span><span class=p>)</span>
<span class=n>pct_in_ci</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>pct_in_ci_hist</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>n</span> <span class=o>=</span> <span class=mi>5</span><span class=n>_000</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>sample</span><span class=p>(</span><span class=n>n</span><span class=p>)):</span>

    <span class=n>ci</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span><span class=o>.</span><span class=n>interval</span><span class=p>(</span><span class=o>.</span><span class=mi>95</span><span class=p>)</span>
    <span class=n>in_ci</span> <span class=o>=</span> <span class=n>ci</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>yi</span> <span class=o>&lt;</span> <span class=n>ci</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
    <span class=n>pct_in_ci</span> <span class=o>+=</span> <span class=p>(</span><span class=n>in_ci</span> <span class=o>-</span> <span class=n>pct_in_ci</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>  <span class=c1># online update of an average</span>
    <span class=n>pct_in_ci_hist</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>pct_in_ci</span><span class=p>)</span>

    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span>

<span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>9</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>n</span><span class=p>),</span> <span class=n>pct_in_ci_hist</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=n>y</span><span class=o>=.</span><span class=mi>95</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Quality of the prediction interval along time&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;# of observed samples&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;</span><span class=si>% o</span><span class=s1>f predictions in 95% prediction interval&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylim</span><span class=p>(</span><span class=o>.</span><span class=mi>9</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
</code></pre></div><p><img src=/img/blog/bayesian-linear-regression/ci.png alt=ci></p><p>This seems to be working quite well; then again the generated data follows a Gaussian distribution so this was expected. What happens if try the same thing on a real-world dataset? As a test I&rsquo;ve done exactly this on the <a href=https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset>California housing dataset</a>, which is a moderately large dataset.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>fetch_california_housing</span><span class=p>(</span><span class=n>return_X_y</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>BayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
<span class=n>pct_in_ci</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>pct_in_ci_hist</span> <span class=o>=</span> <span class=p>[]</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)):</span>

    <span class=n>ci</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span><span class=o>.</span><span class=n>interval</span><span class=p>(</span><span class=o>.</span><span class=mi>95</span><span class=p>)</span>
    <span class=n>in_ci</span> <span class=o>=</span> <span class=n>ci</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>&lt;</span> <span class=n>yi</span> <span class=o>&lt;</span> <span class=n>ci</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
    <span class=n>pct_in_ci</span> <span class=o>+=</span> <span class=p>(</span><span class=n>in_ci</span> <span class=o>-</span> <span class=n>pct_in_ci</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>  <span class=c1># online update of an average</span>
    <span class=n>pct_in_ci_hist</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>pct_in_ci</span><span class=p>)</span>

    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span>

<span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>9</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)),</span> <span class=n>pct_in_ci_hist</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=n>y</span><span class=o>=.</span><span class=mi>95</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Quality of the prediction interval along time&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;# of observed samples&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;</span><span class=si>% o</span><span class=s1>f predictions in 95% prediction interval&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
</code></pre></div><p><img src=/img/blog/bayesian-linear-regression/ci_california.png alt=ci></p><p>That definitely doesn&rsquo;t look very good! That&rsquo;s the problem with parametric models: they make assumptions about your data. If you&rsquo;re really about prediction intervals that work regardless of your dataset &ndash; who wouldn&rsquo;t want that? &ndash; then I would look into <a href=https://www.wikiwand.com/en/Prediction_interval#/Non-parametric_methods>non-parametric prediction intervals</a> and <a href=https://www.wikiwand.com/en/Quantile_regression>quantile regression</a> &ndash; see for instance <a href=https://github.com/Microsoft/LightGBM/issues/1036>this GitHub issue</a> for LightGBM.</p><h2 id=mini-batching>Mini-batching</h2><p>The main issue with our implementation of Bayesian linear regression is that it is horribly <em>slow</em>. The culprit is not hard the identify: it&rsquo;s the matrix inversion we have to do at each step in order to update the inverse covariance matrix &ndash; actually in our specific implementation, a big chunk of time is spent creating instances of <code>scipy.stats.norm</code>, as explained in <a href=https://github.com/scipy/scipy/issues/9394>this GitHub issue</a>, but that&rsquo;s specific to this blog post. Indeed, the <a href=https://www.wikiwand.com/en/Computational_complexity_of_mathematical_operations#/Matrix_algebra>complexity of a matrix inversion</a> is $\mathcal{O}(n^3)$ &ndash; actually, there seems to be an algorithm with complexity $\mathcal{O}(n^{2.373})$, but don&rsquo;t ask me how it works. You have to understand that in most cases, people who use online machine learning use it to crunch huge datasets, therefore they very often resort to using algorithms that run in only $\mathcal{O}(n)$ time. For instance many practitioners <a href=http://fastml.com/the-secret-of-the-big-guys/>use plain and simple stochastic gradient descent</a> &ndash; albeit with a few bells and whistles. More sophisticated online optimizers have been proposed &ndash; such as the <a href=https://link.springer.com/content/pdf/10.1007/s10994-007-5016-8.pdf>online Newton step</a>, which uses the Hessian in addition to the gradient and runs in $\mathcal{O}(n^2)$ time &ndash; but are frowned upon because the name of the game is speed. Therefore, the complexity of our Bayesian linear regression, which has a lower bound complexity of $\mathcal{O}(n^3)$, is going to be a limiting factor for scaling to large datasets.</p><p>Later on, we&rsquo;ll see how we can circumvent this issue by making different assumptions, but first I want to discuss mini-batching. I&rsquo;m not going to go into the maths but it turns out that we can process multiple examples at the same time, whilst obtaining the same final model parameters. In other words, if process A and then B or A and B, the resulting model parameters will be exactly the same. This makes a lot of sense: you know the same amount of information whether I show you data piece by piece or all at the same time. The added benefit is that this significantly speeds up the learning step because it reduces the amount of matrix inversions which need to be performed.</p><p>We need to bring a few adjustments to our implementation in order to allow it to take in more than one observation at a time. We&rsquo;ll make sure that the new implementation works for both a single pair $(x_i, y_i)$ as well as for a batch of pairs. This mostly boils down to <code>numpy</code> details that are not worth going into. I didn&rsquo;t make these adjustments in the initial implementation in order to maximize readability. I am going to inherit from <code>BayesLinReg</code> in order not to have to copy/paste the <code>__init__</code> method.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>BatchBayesLinReg</span><span class=p>(</span><span class=n>BayesLinReg</span><span class=p>):</span>

    <span class=k>def</span> <span class=nf>learn</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>

        <span class=c1># If x and y are singletons, then we coerce them to a batch of length 1</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>atleast_2d</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>atleast_1d</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>

        <span class=c1># Update the inverse covariance matrix (Bishop eq. 3.51)</span>
        <span class=n>cov_inv</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>x</span><span class=o>.</span><span class=n>T</span> <span class=err>@</span> <span class=n>x</span>

        <span class=c1># Update the mean vector (Bishop eq. 3.50)</span>
        <span class=n>cov</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>cov_inv</span><span class=p>)</span>
        <span class=n>mean</span> <span class=o>=</span> <span class=n>cov</span> <span class=err>@</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=err>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>y</span> <span class=err>@</span> <span class=n>x</span><span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=o>=</span> <span class=n>cov_inv</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>=</span> <span class=n>mean</span>

        <span class=k>return</span> <span class=bp>self</span>

    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>

        <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>atleast_2d</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

        <span class=c1># Obtain the predictive mean (Bishop eq. 3.58)</span>
        <span class=n>y_pred_mean</span> <span class=o>=</span> <span class=n>x</span> <span class=err>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean</span>

        <span class=c1># Obtain the predictive variance (Bishop eq. 3.59)</span>
        <span class=n>w_cov</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span><span class=p>)</span>
        <span class=n>y_pred_var</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>+</span> <span class=p>(</span><span class=n>x</span> <span class=err>@</span> <span class=n>w_cov</span> <span class=o>*</span> <span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

        <span class=c1># Drop a dimension from the mean and variance in case x and y were singletons</span>
        <span class=c1># There might be a more elegant way to proceed but this works!</span>
        <span class=n>y_pred_mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>y_pred_mean</span><span class=p>)</span>
        <span class=n>y_pred_var</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>y_pred_var</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=n>y_pred_mean</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=n>y_pred_var</span> <span class=o>**</span> <span class=o>.</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div><p>I&rsquo;ll let you spot the differences if you are so inclined. In order to compare this with the original implementation, we&rsquo;re going to split the California housing dataset in two &ndash; a training set and a test set.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>model_selection</span>

<span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>fetch_california_housing</span><span class=p>(</span><span class=n>return_X_y</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>
<span class=n>X_train</span><span class=p>,</span> <span class=n>X_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>model_selection</span><span class=o>.</span><span class=n>train_test_split</span><span class=p>(</span>
    <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span>
    <span class=n>test_size</span><span class=o>=.</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>shuffle</span><span class=o>=</span><span class=bp>True</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
</code></pre></div><p>First, let&rsquo;s train the initial implementation on the training and compute it&rsquo;s performance on the test set. I&rsquo;m running this code in a Jupyter notebook so I have access to the <a href=https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-time><code>%%time</code></a> magic command to measure the execution time of the snippet.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=o>%%</span><span class=n>time</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>BayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>alpha</span><span class=o>=.</span><span class=mi>3</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=k>for</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>):</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>

<span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>X_test</span><span class=p>))</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)):</span>
    <span class=n>y_pred</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>

<span class=k>print</span><span class=p>(</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
</code></pre></div><p>This produces a mean absolute error of around <code>0.57</code>, and takes approximatively <code>4.66s</code> to run. Now let&rsquo;s run our newly implemented <code>BatchBayesLinReg</code> and feed it all the training set at once.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=o>%%</span><span class=n>time</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>BatchBayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>alpha</span><span class=o>=.</span><span class=mi>3</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>

<span class=k>print</span><span class=p>(</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
</code></pre></div><p>As expected, this outputs a mean absolute error of around <code>0.57</code>, which is identical to the online version. However, the execution time is now only a mere <code>6.42ms</code>, which is a whopping 725 times faster! Therefore the only advantage of the streaming variant is that it allows you to not have to retrain the model when new data is available. The twist is that we can use the best of both worlds: we can &ldquo;warm-start&rdquo; our model by training on a big batch of data, and afterwards feed it individual samples. Bayesian models are really flexible that way. The &ldquo;only&rdquo; downside of the batch approach is that it requires having all the data available at once. In some contexts this isn&rsquo;t feasible nor desirable. As a compromise, we can train the model with mini-batches by chunking the training set into batches of, say, 16 observations:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=o>%%</span><span class=n>time</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>BatchBayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>alpha</span><span class=o>=.</span><span class=mi>3</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=n>batch_size</span> <span class=o>=</span> <span class=mi>16</span>
<span class=n>n_batches</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span> <span class=o>//</span> <span class=n>batch_size</span>

<span class=n>batches</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span>
    <span class=n>np</span><span class=o>.</span><span class=n>array_split</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>n_batches</span><span class=p>),</span>
    <span class=n>np</span><span class=o>.</span><span class=n>array_split</span><span class=p>(</span><span class=n>y_train</span><span class=p>,</span> <span class=n>n_batches</span><span class=p>)</span>
<span class=p>)</span>

<span class=k>for</span> <span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=ow>in</span> <span class=n>batches</span><span class=p>:</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>

<span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>

<span class=k>print</span><span class=p>(</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
</code></pre></div><p>This produces the same mean absolute error, and takes <code>36.9ms</code> to execute, which is 126 times faster than the online variant.</p><h2 id=handling-concept-drift>Handling concept drift</h2><p><a href=https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/>Concept drift</a> occurs when something in the data generating process changes. A common assumption of machine learning models is that the data the model will encounter after it&rsquo;s training phase &ndash; call it the test set, if you like &ndash; has the same statistical properties as the training set. If this assumption is violated, then the model is bound to underperform. In practice, when you deploy a machine learning model, the model&rsquo;s performance will usually degrade along time because &ldquo;something&rdquo; in the data is changing. If you want to read more about concept drift, I recommend reading the <a href=https://www.wikiwand.com/en/Concept_drift>Wikipedia article on the topic</a>, it has some nice real-world examples in it.</p><p>Most practictioners use batch machine learning; they therefore deal with concept drift by retraining the model as often as possible. As you can imagine, this isn&rsquo;t very efficient and consumes a lot of resources. Meanwhile, a good online machine model can and should be able to handle concept drift. This guarantees that it&rsquo;s performance remains stable along time. Let&rsquo;s see if our <code>BayesLinReg</code> implementation is able to cope with concept drift. As a benchmark, I&rsquo;ve decided to simulate a concept drift. There are <a href=https://www.wikiwand.com/en/Concept_drift#/Datasets>datasets</a> out there that contain concept drift, but I wanted to have fine-grained control on the data generating process. I decided on something relatively simple: I picked two sets of linear regression parameters, and slowly transitioned from one to the other. I&rsquo;m sure there&rsquo;s a nice way to write this mathematically, but I believe that the code speaks for itself:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>sample</span><span class=p>(</span><span class=n>first_period</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>transition_period</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>second_period</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>

    <span class=c1># Pick two pairs of weights which the model is going to have to find</span>
    <span class=n>start_weights</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=o>-.</span><span class=mi>3</span><span class=p>,</span>  <span class=o>.</span><span class=mi>5</span><span class=p>])</span>
    <span class=n>end_weights</span>   <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>  <span class=mi>1</span><span class=p>,</span> <span class=o>-.</span><span class=mi>7</span><span class=p>])</span>

    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>first_period</span> <span class=o>+</span> <span class=n>transition_period</span> <span class=o>+</span> <span class=n>second_period</span><span class=p>):</span>

        <span class=c1># Sample a vector of features</span>
        <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)])</span>

        <span class=c1># Decide which set of weights to use</span>
        <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>first_period</span><span class=p>:</span>
            <span class=n>weights</span> <span class=o>=</span> <span class=n>start_weights</span>

        <span class=k>elif</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>first_period</span> <span class=o>+</span> <span class=n>transition_period</span><span class=p>:</span>
            <span class=n>ratio</span> <span class=o>=</span> <span class=p>(</span><span class=n>i</span> <span class=o>-</span> <span class=n>first_period</span><span class=p>)</span> <span class=o>/</span> <span class=n>transition_period</span>
            <span class=n>weights</span> <span class=o>=</span> <span class=n>ratio</span> <span class=o>*</span> <span class=n>start_weights</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>ratio</span><span class=p>)</span> <span class=o>*</span> <span class=n>end_weights</span>

        <span class=k>else</span><span class=p>:</span>
            <span class=n>weights</span> <span class=o>=</span> <span class=n>end_weights</span>

        <span class=c1># Yield the features and the target (with some noise)</span>
        <span class=k>yield</span> <span class=n>x</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=o>.</span><span class=mi>2</span><span class=p>)</span>
</code></pre></div><p>For the first <code>first_period</code> steps, the <code>sample</code> function generates samples using the first set of weights. For the last <code>second_period</code> steps, it uses the second set of weights. In between, it mixes both sets of weights together depending on the number of the iteration. Let&rsquo;s see if this is able to trick our model.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>BayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>25</span><span class=p>)</span>

<span class=n>y_true</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=p>[]</span>

<span class=n>first_period</span> <span class=o>=</span> <span class=mi>100</span>
<span class=n>transition_period</span> <span class=o>=</span> <span class=mi>50</span>
<span class=n>second_period</span> <span class=o>=</span> <span class=mi>100</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>sample</span><span class=p>(</span><span class=n>first_period</span><span class=p>,</span> <span class=n>transition_period</span><span class=p>,</span> <span class=n>second_period</span><span class=p>)):</span>
    <span class=n>y_true</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>yi</span><span class=p>)</span>
    <span class=n>y_pred</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span>

<span class=c1># Convert to numpy arrays for comfort</span>
<span class=n>y_true</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y_true</span><span class=p>)</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y_pred</span><span class=p>)</span>

<span class=n>score</span> <span class=o>=</span> <span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>9</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span>
    <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y_true</span><span class=p>)),</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>),</span>
    <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;|&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Individual absolute errors&#39;</span>
<span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span>
    <span class=n>np</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y_true</span><span class=p>))</span> <span class=o>+</span> <span class=mi>1</span><span class=p>),</span>
    <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Running average of the error&#39;</span>
<span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>axvspan</span><span class=p>(</span>
    <span class=n>first_period</span><span class=p>,</span> <span class=n>first_period</span> <span class=o>+</span> <span class=n>transition_period</span><span class=p>,</span>
    <span class=n>alpha</span><span class=o>=.</span><span class=mi>1</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;green&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Transition period&#39;</span>
<span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;Absolute errors along time (final score: {score:.5f})&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;# of observed samples&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Absolute error&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
</code></pre></div><p>This produces the following chart:</p><p><img src=/img/blog/bayesian-linear-regression/drift_vanilla.png alt=drift_vanilla></p><p>Each individual blue tick represents the absolute error between the ground truth and the model&rsquo;s estimate. The red line is the running average of said errors. The green shaded area represents the transition period between both sets of weights the model has to uncover. Clearly, once the data generating starts changing &ndash; i.e., the beginning of the green shaded area &ndash; the model&rsquo;s performance starts to deteriorate. The Bayesian linear regression therefore isn&rsquo;t coping very well with concept drift. Can we do something about it? I&rsquo;ve never seen this answered in textbooks. The only place I found something mentionned about it was in <a href=(https://koaning.io/posts/bayesian-propto-streaming/)>Vincent Warmerdam&rsquo;s blog post</a> &ndash; in the <em>Extra Modelling Options</em> section. He hints at using a smoothing factor to essentially give more importance to recent observations. As he says, this feels a bit like a hack, mostly because there doesn&rsquo;t seem to be much theoretical justification to back this &ldquo;trick&rdquo;. Nonetheless, I tinkered with Vincent&rsquo;s idea and it seems to work rather well. Here goes:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>RobustBayesLinReg</span><span class=p>(</span><span class=n>BayesLinReg</span><span class=p>):</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>smoothing</span><span class=p>,</span> <span class=n>n_features</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>beta</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>n_features</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>beta</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>smoothing</span> <span class=o>=</span> <span class=n>smoothing</span>

    <span class=k>def</span> <span class=nf>learn</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>

        <span class=c1># Update the inverse covariance matrix, with smoothing</span>
        <span class=n>cov_inv</span> <span class=o>=</span> <span class=p>(</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>smoothing</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=o>+</span>
            <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>smoothing</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span>
        <span class=p>)</span>

        <span class=c1># Update the mean vector, with smoothing</span>
        <span class=n>cov</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>cov_inv</span><span class=p>)</span>
        <span class=n>mean</span> <span class=o>=</span> <span class=n>cov</span> <span class=err>@</span> <span class=p>(</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>smoothing</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=err>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>+</span>
            <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>smoothing</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>y</span> <span class=o>*</span> <span class=n>x</span>
        <span class=p>)</span>

        <span class=bp>self</span><span class=o>.</span><span class=n>cov_inv</span> <span class=o>=</span> <span class=n>cov_inv</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>=</span> <span class=n>mean</span>

        <span class=k>return</span> <span class=bp>self</span>
</code></pre></div><p>I&rsquo;ll let you compare this with the initial implementation. In terms of update equations, by denoting with $\gamma$ the smoothing factor, this is what it looks like:</p><p>$$S_{i+1} = (\gamma S_i^{-1} + (1 - \gamma)\beta x_i^\intercal x_i)^{-1}$$</p><p>$$m_{i+1} = S_{i+1}(\gamma S_i^{-1} m_i + (1 - \gamma) \beta x_i y_i)$$</p><p>Note that in the new implementation nothing changes for the <code>predict</code> method; we&rsquo;ve therefore inherited from <code>BayesLinReg</code> to avoid an unnecessary copy/paste. The smoothing works essentially like an exponentially weighted average &ndash; as a reference, see the <code>alpha</code> parametrisation of the <a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ewm.html><code>ewm</code> method in <code>pandas</code></a>. Let&rsquo;s check out the performance of this model on the same scenario when using a smoothing factor of <code>0.8</code> &ndash; which is actually the only value I tried because it worked.</p><details><summary>Click to see the code</summary><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>RobustBayesLinReg</span><span class=p>(</span><span class=n>smoothing</span><span class=o>=.</span><span class=mi>8</span><span class=p>,</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>25</span><span class=p>)</span>

<span class=n>y_true</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=p>[]</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>sample</span><span class=p>(</span><span class=n>first_period</span><span class=p>,</span> <span class=n>transition_period</span><span class=p>,</span> <span class=n>second_period</span><span class=p>)):</span>
    <span class=n>y_true</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>yi</span><span class=p>)</span>
    <span class=n>y_pred</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span>

<span class=c1># Convert to numpy arrays for comfort</span>
<span class=n>y_true</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y_true</span><span class=p>)</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y_pred</span><span class=p>)</span>

<span class=n>score</span> <span class=o>=</span> <span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>

<span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>9</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
<span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span>
    <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y_true</span><span class=p>)),</span> <span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>),</span>
    <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;|&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Individual absolute errors&#39;</span>
<span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span>
    <span class=n>np</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y_true</span><span class=p>))</span> <span class=o>+</span> <span class=mi>1</span><span class=p>),</span>
    <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Running average of the error&#39;</span>
<span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>axvspan</span><span class=p>(</span>
    <span class=n>first_period</span><span class=p>,</span> <span class=n>first_period</span> <span class=o>+</span> <span class=n>transition_period</span><span class=p>,</span>
    <span class=n>alpha</span><span class=o>=.</span><span class=mi>1</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;green&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Transition period&#39;</span>
<span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;Absolute errors along time (final score: {score:.5f})&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;# of observed samples&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Absolute error&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
</code></pre></div></details><p><img src=/img/blog/bayesian-linear-regression/drift_robust.png alt=drift_robust></p><p>It seems to be working very well! Some would say this is worth writing a research paper. I&rsquo;m sure that there&rsquo;s already one that exists, but I haven&rsquo;t found it yet.</p><h2 id=zero-mean-isotropic-gaussian-prior>Zero mean isotropic Gaussian prior</h2><p><img src=/img/blog/bayesian-linear-regression/trump.jpg alt=trump></p><p>This section title is a bit of a mouthful &ndash; at least for me &ndash; but it&rsquo;s actually simpler than it sounds. In his book, Christopher Bishop proposes a simpler prior distribution for the weights which significantly reduces the amount of computation to perform. Under this prior, which is still a $p$-dimensional Gaussian, the weights are centered around 0 &ndash; hence the &ldquo;zero mean&rdquo; &ndash; whilst they each have the same variance &ndash; which corresponds to the &ldquo;<a href=https://www.wikiwand.com/en/Isotropy>isotropic</a>&rdquo; part. Mathematically, the prior is defined as so:</p><p>$$\begin{equation}
\color{royalblue} p(w_0) = \mathcal{N}(0, \alpha^{-1}I)
\end{equation}$$</p><p>This leads to the following posterior distribution for the weights:</p><p>$$\begin{equation}
\color{mediumpurple} p(w_{i+1} | w_i, x_i, y_i) = \mathcal{N}(m_{i+1}, S_{i+1})
\end{equation}$$</p><p>$$\begin{equation}
S_{i+1} = (\alpha I + \beta x_i^\intercal x_i)^{-1}
\end{equation}$$</p><p>$$\begin{equation}
m_{i+1} = \beta S_{i+1} x_i^\intercal y_i
\end{equation}$$</p><p>At a first glance, these formulas look just as heavy as before because they still necessitate a matrix inversion. The trick is that the equation for obtaining $S_{i+1}$ has a particular structure that we can exploit. Indeed, it turns out that there is a wonderful formula to evaluate this equation without having to explicitely apply the inverse operation &ndash; I love it when this happens. It&rsquo;s called the <a href=https://www.wikiwand.com/en/Sherman%E2%80%93Morrison_formula>Shermanâ€“Morrison formula</a>. Here it is with Wikipedia&rsquo;s notation:</p><p>$$\begin{equation}
(A + uv^\intercal)^{-1} = A^{-1} - \frac{A^{-1} uv^\intercal A^{-1}}{1 + v^\intercal A^{-1}u}
\end{equation}$$</p><p>In our case, we have:</p><p>$$\begin{equation}
A \leftarrow \alpha I
\end{equation}$$</p><p>$$\begin{equation}
u \leftarrow x
\end{equation}$$</p><p>$$\begin{equation}
v \leftarrow x
\end{equation}$$</p><p>This formula is efficient because we only have to compute $A^{-1}$ once. The following snippet contains the implementation of Bayesian linear regression with a zero mean isotropic Gaussian prior and the Sherman-Morrisson formula:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>sherman_morrison</span><span class=p>(</span><span class=n>A_inv</span><span class=p>,</span> <span class=n>u</span><span class=p>,</span> <span class=n>v</span><span class=p>):</span>
    <span class=n>num</span> <span class=o>=</span> <span class=n>A_inv</span> <span class=err>@</span> <span class=n>np</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>u</span><span class=p>,</span> <span class=n>v</span><span class=p>)</span> <span class=err>@</span> <span class=n>A_inv</span>
    <span class=n>den</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>+</span> <span class=n>v</span> <span class=err>@</span> <span class=n>A_inv</span> <span class=err>@</span> <span class=n>u</span>
    <span class=k>return</span> <span class=n>A_inv</span> <span class=o>-</span> <span class=n>num</span> <span class=o>/</span> <span class=n>den</span>


<span class=k>class</span> <span class=nc>SimpleBayesLinReg</span><span class=p>:</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_features</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>beta</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>n_features</span> <span class=o>=</span> <span class=n>n_features</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>=</span> <span class=n>beta</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_features</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>A_inv</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>alpha</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>identity</span><span class=p>(</span><span class=n>n_features</span><span class=p>))</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>A_inv</span>

    <span class=k>def</span> <span class=nf>learn</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>

        <span class=c1># Update the inverse covariance matrix (Bishop eq. 3.54)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=o>=</span> <span class=n>sherman_morrison</span><span class=p>(</span><span class=n>A_inv</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>A_inv</span><span class=p>,</span> <span class=n>u</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=n>v</span><span class=o>=</span><span class=n>x</span><span class=p>)</span>

        <span class=c1># Update the mean vector (Bishop eq. 3.53)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=err>@</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span>

        <span class=k>return</span> <span class=bp>self</span>

    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>

        <span class=c1># Obtain the predictive mean (Bishop eq. 3.58)</span>
        <span class=n>y_pred_mean</span> <span class=o>=</span> <span class=n>x</span> <span class=err>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean</span>

        <span class=c1># Obtain the predictive variance (Bishop eq. 3.59)</span>
        <span class=n>y_pred_var</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>+</span> <span class=n>x</span> <span class=err>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>cov</span> <span class=err>@</span> <span class=n>x</span><span class=o>.</span><span class=n>T</span>

        <span class=k>return</span> <span class=n>stats</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=n>y_pred_mean</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=n>y_pred_var</span> <span class=o>**</span> <span class=o>.</span><span class=mi>5</span><span class=p>)</span>
</code></pre></div><p>Let&rsquo;s apply progressive validation with the Boston housing dataset.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_boston</span><span class=p>(</span><span class=n>return_X_y</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>SimpleBayesLinReg</span><span class=p>(</span><span class=n>n_features</span><span class=o>=</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>alpha</span><span class=o>=.</span><span class=mi>3</span><span class=p>,</span> <span class=n>beta</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

<span class=n>y_pred</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>))</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)):</span>
    <span class=n>y_pred</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
    <span class=n>model</span><span class=o>.</span><span class=n>learn</span><span class=p>(</span><span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span>

<span class=k>print</span><span class=p>(</span><span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>))</span>
</code></pre></div><p>This produces a mean absolute error of around <code>4.6</code>. For reference the initial implementation scored <code>3.78</code>, so this new model isn&rsquo;t as good.</p><h2 id=conclusion>Conclusion</h2><p>That wraps it up for now. There are definitely some more directions I would like to take, namely:</p><ul><li>Supporting binary and multi-class classification: this would require determining other update formulas, probably based on a binomial distribution for the likelihood function.</li><li>Learning <code>alpha</code> and <code>beta</code>: in his book, Christopher Bishop points at some ways to estimate both hyperparameters from the data. However, I need to drink a lot more coffee in order to understand how he proceeds.</li><li>Handling sparse feature vectors: it would be nice to handle a large number features by allowing sparse computation (and not just manually filling the gaps of <code>x</code> with 0s!). This would lead the way, for instance, to Bayesian factorisation machines &ndash; although that <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.364.8054">already seems to be something</a>.</li><li>Non-zero mean isotropic Gaussian prior: the prior introduced in the last section doesn&rsquo;t perform as well as the initial implementation; I wonder if it&rsquo;s possible to specify a model somewhere in between and therefore obtain a better whilst still avoiding to have to compute a matrix inversion at every iteration.</li><li>Implementing a &ldquo;master class&rdquo; where all the variants and tricks I have presented are implemented and can be switched on or off.</li></ul><p>I will eventually dig into these topics. My initial plan was to include them in this post, but I&rsquo;ve been waiting to publish this post for far too long &ndash; plus it&rsquo;s already quite dense. If this post has peaked your interest and you would like to collaborate or have some questions, then feel free to get in touch with me at <a href=mailto:maxhalford25@gmail.com>maxhalford25@gmail.com</a>. Also, I&rsquo;ve put all the code used in this blog post and put it in <a href=https://gist.github.com/MaxHalford/c2cb7f2b064156fd780150650db9dec0>this gist</a>. The next step for me is going to be to implement Bayesian linear regression in <a href=https://github.com/creme-ml/creme>creme</a>, which is an online machine learning library I maintain.</p></div><script type=text/javascript>var s=document.createElement('script');s.setAttribute('src','https://utteranc.es/client.js'),s.setAttribute('repo','MaxHalford/maxhalford.github.io'),s.setAttribute('issue-term','pathname'),s.setAttribute('crossorigin','anonymous'),s.setAttribute('async',null),s.setAttribute('theme','github-light'),document.body.appendChild(s)</script><div class=footer><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to top</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.0/elevator.min.js></script><script>var elementButton=document.querySelector('.elevator'),elevator=new Elevator({element:elementButton,mainAudio:'/music/elevator.mp3',endAudio:'/music/ding.mp3'})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=site-footer><div class=site-footer-item><a href=https://github.com/MaxHalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M8 0C3.58.0.0 3.582.0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385.0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953.0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117.0.0.67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147.0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48.0 1.07-.01 1.93-.01 2.19.0.21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg></span></a></div><div class=site-footer-item><a href=https://linkedin.com/in/maxhalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235.0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4.0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762.0-1.376-.617-1.376-1.377.0-.758.614-1.375 1.376-1.375.76.0 1.376.617 1.376 1.375.0.76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816.0H1.18C.528.0.0.516.0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652.0 1.185-.516 1.185-1.153V1.153C16 .516 15.467.0 14.815.0z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://twitter.com/halford_max><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812.0-3.282 1.47-3.282 3.28.0.26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21.0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26.0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04.0 9.34-5 9.34-9.33.0-.14.0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://kaggle.com/maxhalford><span class=inline-svg><svg role="img" viewBox="0 0 26 26" xmlns="http://www.w3.org/2000/svg"><title>Kaggle icon</title><path fill="currentcolor" d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187.0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236.0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234.0.351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144.0.236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.358"/></svg></span></a></div><div class=site-footer-item><a href="https://scholar.google.com/citations?user=erRNNi0AAAAJ&hl=en"><span class=inline-svg><svg viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952.0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584.0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288.0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962.0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zm762.098 382.384c-64.358 64.424-141.86 96.57-232.572 96.57H329.144c-90.712.0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572V182.859c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712.0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159V762.033c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162.0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53.0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476.0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86.0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908.0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426.0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432.0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234.0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048.0 6.642.19 12.492.672 18.974H292.574l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382.0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994.0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376.0 103.05 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382.0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.05 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z"/></svg></span></a></div><div class=site-footer-item><a href=/files/resume_max_halford.pdf><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 392.533 392.533" style="enable-background:new 0 0 392.533 392.533"><g><g><path fill="currentcolor" d="M292.396 324.849H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 329.697 298.473 324.849 292.396 324.849z"/></g></g><g><g><path fill="currentcolor" d="M292.396 277.01H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 281.859 298.473 277.01 292.396 277.01z"/></g></g><g><g><path fill="currentcolor" d="M196.137 45.834c-25.859.0-46.998 21.075-46.998 46.998.0 25.859 21.139 46.933 46.998 46.933s46.998-21.075 46.998-46.998-21.139-46.933-46.998-46.933zm0 72.017c-13.77.0-25.083-11.313-25.083-25.083s11.248-25.083 25.083-25.083 25.083 11.313 25.083 25.083c0 13.769-11.313 25.083-25.083 25.083z"/></g></g><g><g><path fill="currentcolor" d="M258.521 163.362c-39.887-15.515-84.752-15.515-124.638.0-13.059 5.107-21.786 18.101-21.786 32.388v44.347c-.065 6.012 4.849 10.925 10.861 10.925h146.424c6.012.0 10.925-4.848 10.925-10.925V195.75C280.307 181.463 271.58 168.469 258.521 163.362zm0 65.874H133.883v-33.422c0-5.301 3.168-10.214 7.887-12.024 34.844-13.511 74.02-13.511 108.865.0 4.719 1.875 7.887 6.659 7.887 12.024v33.422z"/></g></g><g><g><path fill="currentcolor" d="M313.083.0H131.491c-8.404.0-16.291 3.232-22.238 9.18L57.018 61.414c-5.947 5.948-9.18 13.834-9.18 22.238v277.333c0 17.39 14.158 31.547 31.547 31.547h233.762c17.39.0 31.547-14.158 31.547-31.547V31.547C344.501 14.158 330.343.0 313.083.0zM112.032 37.236v27.022H85.01l27.022-27.022zm210.683 79.58h-40.598c-6.012.0-10.925 4.849-10.925 10.925.0 6.012 4.848 10.925 10.925 10.925h40.598v19.394h-14.869c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h14.869v181.139c0 5.366-4.331 9.697-9.632 9.697H79.192c-5.301.0-9.632-4.331-9.632-9.632V86.044h53.398c6.012.0 10.925-4.848 10.925-10.925V21.721h179.2c5.301.0 9.632 4.331 9.632 9.632v85.463z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></span></a></div><div class=site-footer-item><a href=https://play.spotify.com/user/1166811350><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 168 168"><path fill="currentcolor" d="m83.996.277C37.747.277.253 37.77.253 84.019c0 46.251 37.494 83.741 83.743 83.741 46.254.0 83.744-37.49 83.744-83.741.0-46.246-37.49-83.738-83.745-83.738l.001-.004zm38.404 120.78c-1.5 2.46-4.72 3.24-7.18 1.73-19.662-12.01-44.414-14.73-73.564-8.07-2.809.64-5.609-1.12-6.249-3.93-.643-2.81 1.11-5.61 3.926-6.25 31.9-7.291 59.263-4.15 81.337 9.34 2.46 1.51 3.24 4.72 1.73 7.18zm10.25-22.805c-1.89 3.075-5.91 4.045-8.98 2.155-22.51-13.839-56.823-17.846-83.448-9.764-3.453 1.043-7.1-.903-8.148-4.35-1.04-3.453.907-7.093 4.354-8.143 30.413-9.228 68.222-4.758 94.072 11.127 3.07 1.89 4.04 5.91 2.15 8.976v-.001zm.88-23.744c-26.99-16.031-71.52-17.505-97.289-9.684-4.138 1.255-8.514-1.081-9.768-5.219-1.254-4.14 1.08-8.513 5.221-9.771 29.581-8.98 78.756-7.245 109.83 11.202 3.73 2.209 4.95 7.016 2.74 10.733-2.2 3.722-7.02 4.949-10.73 2.739z"/></svg></span></a></div><div class=site-footer-item><a href=mailto:maxhalford25@gmail.com><span class=inline-svg><svg viewBox="0 0 15 20" xmlns="http://www.w3.org/2000/svg"><title>mail</title><path fill="currentcolor" d="M0 4v8c0 .55.45 1 1 1h12c.55.0 1-.45 1-1V4c0-.55-.45-1-1-1H1c-.55.0-1 .45-1 1zm13 0L7 9 1 4h12zM1 5.5l4 3-4 3v-6zM2 12l3.5-3L7 10.5 8.5 9l3.5 3H2zm11-.5-4-3 4-3v6z" fill="#000" fill-rule="evenodd"/></svg></span></a></div><div class=site-footer-item><a href=/index.xml><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M12.8 16C12.8 8.978 7.022 3.2.0 3.2V0c8.777.0 16 7.223 16 16h-3.2zM2.194 11.61c1.21.0 2.195.985 2.195 2.196.0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017.0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818.0 10.606 4.79 10.606 10.607z"/></svg></span></a></div></div></div></div></article><script></script></body></html>