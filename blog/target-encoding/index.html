<!doctype html><html lang=en><head><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogImMwMjJhMjg1LWJmY2YtNDc0OC1hYzczLTJhMDQ1Njk3NTI0YyIsICJpZCI6ICIzNjc3NjQ3Ny04MTE2LTRmYWQtYjcwMy1iZmM3YjMwZGJjMjMifQ.A0vHm-VWbXG6uBFZiwuspN_AyfSYNrdZE3IgwgWSt4g></script><meta charset=utf-8><meta name=generator content="Hugo 0.110.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/target-encoding/"><link rel=canonical href=https://maxhalford.github.io/blog/target-encoding/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Target encoding done the right way","headline":"Target encoding done the right way","description":"When you\u0026rsquo;re doing supervised learning, you often have to deal with categorical variables. That is, variables which don\u0026rsquo;t have a natural numerical representation. The problem is that most machine learning algorithms require the input data to be numerical. At some point or another a data science pipeline will require converting categorical variables to numerical variables.\nThere are many ways to do so:\nLabel encoding where you choose an arbitrary number for each category One-hot encoding where you create one binary column per category Vector representation a.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2018","datePublished":"2018-10-13 00:00:00 \u002b0000 UTC","dateModified":"2018-10-13 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/target-encoding\/","keywords":["machine-learning"]}</script><title>Target encoding done the right way â€¢ Max Halford</title><meta property="og:title" content="Target encoding done the right way â€¢ Max Halford"><meta property="og:type" content="article"><meta name=description content="When you&rsquo;re doing supervised learning, you often have to deal with categorical variables. That is, variables which don&rsquo;t have a natural numerical representation. The problem is that most machine learning algorithms require the input data to be numerical. At some point or another a data science pipeline will require converting categorical variables to numerical variables.
There are many ways to do so:
Label encoding where you choose an arbitrary number for each category One-hot encoding where you create one binary column per category Vector representation a."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford ðŸ¦”</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>Target encoding done the right way</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2018-10-13 00:00:00 UTC">2018-10-13</time>
<span class=posts-line-tag>machine-learning</span></div></div></header><div class="post-content markdown-body"><p>When you&rsquo;re doing supervised learning, you often have to deal with categorical variables. That is, variables which don&rsquo;t have a natural numerical representation. The problem is that most machine learning algorithms require the input data to be numerical. At some point or another a data science pipeline will require converting categorical variables to numerical variables.</p><p>There are many ways to do so:</p><ul><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html>Label encoding</a> where you choose an arbitrary number for each category</li><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html>One-hot encoding</a> where you create one binary column per category</li><li><a href=https://www.tensorflow.org/tutorials/representation/word2vec>Vector representation</a> a.k.a. word2vec where you find a low dimensional subspace that fits your data</li><li><a href=https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst#categorical-feature-support>Optimal binning</a> where you rely on tree-learners such as LightGBM or CatBoost</li><li><a href=http://www.saedsayad.com/encoding.htm>Target encoding</a> where you average the target value by category</li></ul><p>Each and every one of these method has its own pros and cons. The best approach typically depends on your data and your requirements. If a variable has a lot of categories, then a one-hot encoding scheme will produce many columns, which can cause memory issues. In my experience, relying on LightGBM/CatBoost is the best out-of-the-box method. Label encoding is useless and you should never use it. However if your categorical variable happens to be ordinal then you can and should represent it with increasing numbers (for example &ldquo;cold&rdquo; becomes 0, &ldquo;mild&rdquo; becomes 1, and &ldquo;hot&rdquo; becomes 2). <a href=https://www.wikiwand.com/en/Word2vec>Word2vec</a> and others such methods are cool and good but they require some fine-tuning and don&rsquo;t always work out of the box.</p><p>Target encoding is a fast way to get the most out of your categorical variables with little effort. The idea is quite simple. Say you have a categorical variable $x$ and a target $y$ &ndash; $y$ can be binary or continuous, it doesn&rsquo;t matter. For each distinct element in $x$ you&rsquo;re going to compute the average of the corresponding values in $y$. Then you&rsquo;re going to replace each $x_i$ with the according mean. This is rather easy to do in Python and the <code>pandas</code> library.</p><p>First let&rsquo;s create some dummy data.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;x_0&#39;</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;a&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>5</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;b&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;x_1&#39;</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;c&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>9</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;d&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;y&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span></code></pre></div><table><thead><tr><th>$x_0$</th><th>$x_1$</th><th>$y$</th></tr></thead><tbody><tr><td>$a$</td><td>$c$</td><td>1</td></tr><tr><td>$a$</td><td>$c$</td><td>1</td></tr><tr><td>$a$</td><td>$c$</td><td>1</td></tr><tr><td>$a$</td><td>$c$</td><td>1</td></tr><tr><td>$a$</td><td>$c$</td><td>0</td></tr><tr><td>$b$</td><td>$c$</td><td>1</td></tr><tr><td>$b$</td><td>$c$</td><td>0</td></tr><tr><td>$b$</td><td>$c$</td><td>0</td></tr><tr><td>$b$</td><td>$c$</td><td>0</td></tr><tr><td>$b$</td><td>$d$</td><td>0</td></tr></tbody></table><p>We can start by computing the means of the $x_0$ column.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>means</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;x_0&#39;</span><span class=p>)[</span><span class=s1>&#39;y&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span></code></pre></div><p>This results in the following dictionary.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;a&#39;</span><span class=p>:</span> <span class=mf>0.8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;b&#39;</span><span class=p>:</span> <span class=mf>0.2</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We can then replace each value in $x_0$ with the matching mean.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_0&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_0&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>means</span><span class=p>)</span>
</span></span></code></pre></div><p>We now have the following data frame.</p><table><thead><tr><th>$x_0$</th><th>$x_1$</th><th>$y$</th></tr></thead><tbody><tr><td>0.8</td><td>$c$</td><td>1</td></tr><tr><td>0.8</td><td>$c$</td><td>1</td></tr><tr><td>0.8</td><td>$c$</td><td>1</td></tr><tr><td>0.8</td><td>$c$</td><td>1</td></tr><tr><td>0.8</td><td>$c$</td><td>0</td></tr><tr><td>0.2</td><td>$c$</td><td>1</td></tr><tr><td>0.2</td><td>$c$</td><td>0</td></tr><tr><td>0.2</td><td>$c$</td><td>0</td></tr><tr><td>0.2</td><td>$c$</td><td>0</td></tr><tr><td>0.2</td><td>$d$</td><td>0</td></tr></tbody></table><p>We can do the same for $x_1$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_1&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_1&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;x_1&#39;</span><span class=p>)[</span><span class=s1>&#39;y&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
</span></span></code></pre></div><table><thead><tr><th>$x_0$</th><th>$x_1$</th><th>$y$</th></tr></thead><tbody><tr><td>0.8</td><td>0.555</td><td>1</td></tr><tr><td>0.8</td><td>0.555</td><td>1</td></tr><tr><td>0.8</td><td>0.555</td><td>1</td></tr><tr><td>0.8</td><td>0.555</td><td>1</td></tr><tr><td>0.8</td><td>0.555</td><td>0</td></tr><tr><td>0.2</td><td>0.555</td><td>1</td></tr><tr><td>0.2</td><td>0.555</td><td>0</td></tr><tr><td>0.2</td><td>0.555</td><td>0</td></tr><tr><td>0.2</td><td>0.555</td><td>0</td></tr><tr><td>0.2</td><td>0</td><td>0</td></tr></tbody></table><p>Target encoding is good because it picks up values that can explain the target. In this silly example value $a$ of variable $x_0$ has an average target value of $0.8$. This can greatly help the machine learning classifications algorithms used downstream.</p><p>The problem of target encoding has a name: <strong>over-fitting</strong>. Indeed, relying on an average value isn&rsquo;t always a good idea when the number of values used in the average is low. You&rsquo;ve got to keep in mind that the dataset you&rsquo;re training on is a sample of a larger set. This means that whatever artifacts you may find in the training set might not hold true when applied to another dataset (i.e. the test set).</p><p>In the above xample, the value $d$ of variable $x_1$ is replaced with a 0 because it only appears once and the corresponding value of $y$ is a 0. Therefore, we&rsquo;re over-fitting because we don&rsquo;t have enough values to be <em>sure</em> that 0 is in fact the mean value of $y$ when $x_1$ is equal to $d$. In other words, relying on the average of only a few values is too reckless.</p><p>There are various ways to handle this. A popular way is to use cross-validation and compute the means in each out-of-fold dataset. This is <a href=http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html>what H20 does</a> and what many Kagglers do too. Another approach which I much prefer is to use <a href=https://www.wikiwand.com/en/Additive_smoothing>additive smoothing</a>. This is supposedly <a href=https://www.wikiwand.com/en/Bayes_estimator#/Practical_example_of_Bayes_estimators>what IMDB uses to rate its movies</a>.</p><p>The intuition is as follows. Imagine a new movie is posted on IMDB and it receives three ratings. Taking into account the three ratings gives the movie an average of 9.5. This is surprising because most movies tend to hover around 7, and the very good ones rarely go above 8. The point is that the average can&rsquo;t be trusted because there are too few values. The trick is to &ldquo;smooth&rdquo; the average by <strong>including the average rating over all movies</strong>. In other words, if there aren&rsquo;t many ratings we should rely on the global average rating, whereas if there enough ratings then we can safely rely on the local average.</p><p>Mathematically this is equivalent to:</p><p>$$\begin{equation}
\mu = \frac{n \times \bar{x} + m \times w}{n + m}
\end{equation}$$</p><p>where</p><ul><li>$\mu$ is the mean we&rsquo;re trying to compute (the one that&rsquo;s going to replace our categorical values)</li><li>$n$ is the number of values you have</li><li>$\bar{x}$ is your estimated mean</li><li>$m$ is the &ldquo;weight&rdquo; you want to assign to the overall mean</li><li>$w$ is the overall mean</li></ul><p>In this notation, $m$ is the only parameter you have to set. The idea is that the higher $m$ is, the more you&rsquo;re going to rely on the overall mean $w$. If $m$ is equal to 0 then you&rsquo;re simply going to compute the empirical mean, which is:</p><p>$$\begin{equation}
\mu = \frac{n \times \bar{x} + 0 \times w}{n + 0} = \frac{n \times \bar{x}}{n} = \bar{x}
\end{equation}$$</p><p>In other words you&rsquo;re not doing any smoothing whatsoever.</p><p>Again this is quite easy to do in Python. First we&rsquo;re going to write a method that computes a smooth mean. It&rsquo;s going to take as input a <code>pandas.DataFrame</code>, a categorical column name, the name of the target column, and a weight $m$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>calc_smooth_mean</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>by</span><span class=p>,</span> <span class=n>on</span><span class=p>,</span> <span class=n>m</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Compute the global mean</span>
</span></span><span class=line><span class=cl>    <span class=n>mean</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>on</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Compute the number of values and the mean of each group</span>
</span></span><span class=line><span class=cl>    <span class=n>agg</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=n>by</span><span class=p>)[</span><span class=n>on</span><span class=p>]</span><span class=o>.</span><span class=n>agg</span><span class=p>([</span><span class=s1>&#39;count&#39;</span><span class=p>,</span> <span class=s1>&#39;mean&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>counts</span> <span class=o>=</span> <span class=n>agg</span><span class=p>[</span><span class=s1>&#39;count&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>means</span> <span class=o>=</span> <span class=n>agg</span><span class=p>[</span><span class=s1>&#39;mean&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Compute the &#34;smoothed&#34; means</span>
</span></span><span class=line><span class=cl>    <span class=n>smooth</span> <span class=o>=</span> <span class=p>(</span><span class=n>counts</span> <span class=o>*</span> <span class=n>means</span> <span class=o>+</span> <span class=n>m</span> <span class=o>*</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>counts</span> <span class=o>+</span> <span class=n>m</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Replace each value by the according smoothed mean</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>df</span><span class=p>[</span><span class=n>by</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>smooth</span><span class=p>)</span>
</span></span></code></pre></div><p>Let&rsquo;s see what this does in the previous example with a weight of, say, 10.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_0&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>calc_smooth_mean</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>by</span><span class=o>=</span><span class=s1>&#39;x_0&#39;</span><span class=p>,</span> <span class=n>on</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>m</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_1&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>calc_smooth_mean</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>by</span><span class=o>=</span><span class=s1>&#39;x_1&#39;</span><span class=p>,</span> <span class=n>on</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>m</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></div><table><thead><tr><th>$x_0$</th><th>$x_1$</th><th>$y$</th></tr></thead><tbody><tr><td>0.6</td><td>0.526316</td><td>1</td></tr><tr><td>0.6</td><td>0.526316</td><td>1</td></tr><tr><td>0.6</td><td>0.526316</td><td>1</td></tr><tr><td>0.6</td><td>0.526316</td><td>1</td></tr><tr><td>0.6</td><td>0.526316</td><td>0</td></tr><tr><td>0.4</td><td>0.526316</td><td>1</td></tr><tr><td>0.4</td><td>0.526316</td><td>0</td></tr><tr><td>0.4</td><td>0.526316</td><td>0</td></tr><tr><td>0.4</td><td>0.526316</td><td>0</td></tr><tr><td>0.4</td><td>0.454545</td><td>0</td></tr></tbody></table><p>It&rsquo;s quite noticeable that each computed value is much closer to the overall mean of 0.5. This is because a weight of 10 is rather large for a dataset of only 10 values. The value $d$ of variable $x_1$ has been replaced with 0.454545 instead of the 0 we got earlier. The equation for obtaining it was:</p><p>$$\begin{equation}
d = \frac{1 \times 0 + 10 \times 0.5}{1 + 10} = \frac{0 + 5}{11} \simeq 0.454545
\end{equation}$$</p><p>Meanwhile the new value for replacing the value $a$ of variable $x_0$ was:</p><p>$$\begin{equation}
a = \frac{5 \times 0.8 + 10 \times 0.5}{5 + 10} = \frac{4 + 5}{15} = 0.6
\end{equation}$$</p><p>Computing smooth means can be done extremely quickly. What&rsquo;s more, you only have to choose a single parameter, which is $m$. I find that setting to something like 300 works well in most cases. It&rsquo;s quite intuitive really: you&rsquo;re saying that you require that there must be at least 300 values for the sample mean to overtake the global mean. There are other ways to do target encoding that are somewhat popular on Kaggle, such as <a href=https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf>this one</a>. However, the latter method produces encoded variables which are very correlated with the output of additive smoothing, at the cost of requiring two parameters.</p><p>I&rsquo;m well aware that there are many well-written posts about target encoding elsewhere. I just want to do my bit and help spread the word: don&rsquo;t use vanilla target encoding! Go Bayesian and use priors. For those who are interested in using a scikit-learn compatible implementation, I did that just <a href=https://github.com/MaxHalford/xam/blob/master/docs/feature-extraction.md#smooth-target-encoding>here</a>.</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px;margin-bottom:30px><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div><iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=32 width=114 style=border:0;border-radius:6px></iframe></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script>
<script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href=/blog/naive-bayes/>The NaÃ¯ve Bayes classifier</a></li><li><a href=/blog/subsampling-1/>Subsampling a training set to match a test set - Part 1</a></li><li><a href=/blog/genetic-algorithms-introduction/>An introduction to genetic algorithms</a></li></ul></div></div></div></article><script></script></body></html>