<!doctype html><html lang=en><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-63302552-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-63302552-1')</script><script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script><meta charset=utf-8><meta name=generator content="Hugo 0.82.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/"><link rel=canonical href=https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Reducing the memory footprint of a scikit-learn text classifier","headline":"Reducing the memory footprint of a scikit-learn text classifier","description":"Context This week at Alan I\u0026rsquo;ve been working on parsing French medical prescriptions. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2021","datePublished":"2021-04-11 00:00:00 \u002b0000 UTC","dateModified":"2021-04-11 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/sklearn-text-classifier-memory-footprint-reduction\/","keywords":[]}</script><title>Reducing the memory footprint of a scikit-learn text classifier - Max Halford</title><meta property="og:title" content="Reducing the memory footprint of a scikit-learn text classifier - Max Halford"><meta property="og:type" content="article"><meta name=description content="Context This week at Alan I&rsquo;ve been working on parsing French medical prescriptions. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0,tags:'ams'},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll('mjx-container').forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=post-header><header><div class="signatures site-title"><a href=/>Max Halford</a></div></header><div class="row end-xs"><div><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></div><div class=header-line></div></div><header class=post-header><h1 class=post-title>Reducing the memory footprint of a scikit-learn text classifier</h1><div class="row post-desc"><div class=col-xs-12><time class=post-date datetime="2021-04-11 00:00:00 UTC">2021-04-11 Â· 7 minute read</time></div></div></header><div class="post-content markdown-body"><h2 id=toc>Table of contents</h2><nav id=TableOfContents><ul><li><a href=#context>Context</a></li><li><a href=#measuring-the-models-size>Measuring the model&rsquo;s size</a></li><li><a href=#reducing-the-models-size>Reducing the model&rsquo;s size</a></li></ul></nav><h2 id=context>Context</h2><p>This week at Alan I&rsquo;ve been working on parsing <a href=https://www.wikiwand.com/fr/Ordonnance_(m%C3%A9decine)>French medical prescriptions</a>. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image. We can thus use the text transcription to classify the prescription.</p><p>I played around with writing some regex rules and reached a macro F1 score of 95%. Not bad, but not perfect. I wanted to reach 100% because it seemed to me like a simple classification task. That is, when I looked at some sample prescriptions, the type of the prescription was really obvious. I weighed the pros and cons of writing more complex regex patterns versus building a machine learning text classifier. I opted for the latter. I wrote a simple <a href=https://scikit-learn.org/stable/modules/compose.html>scikit-learn pipeline</a> and tested it as so:</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CountVectorizer</span>
<span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
<span class=kn>from</span> <span class=nn>sklearn.pipeline</span> <span class=kn>import</span> <span class=n>make_pipeline</span>
<span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>Normalizer</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>make_pipeline</span><span class=p>(</span>
    <span class=n>CountVectorizer</span><span class=p>(),</span>
    <span class=n>Normalizer</span><span class=p>(),</span>
    <span class=n>LogisticRegression</span><span class=p>()</span>
<span class=p>)</span>

<span class=n>train</span> <span class=o>=</span> <span class=n>prescriptions</span><span class=p>[:</span><span class=nb>len</span><span class=p>(</span><span class=n>prescriptions</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span><span class=p>]</span>
<span class=n>test</span> <span class=o>=</span> <span class=n>prescriptions</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>prescriptions</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span><span class=p>:]</span>

<span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>],</span> <span class=n>train</span><span class=p>[</span><span class=s1>&#39;type&#39;</span><span class=p>])</span>

<span class=n>y_pred</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span>
    <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>test</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]),</span>
    <span class=n>columns</span><span class=o>=</span><span class=n>model</span><span class=o>.</span><span class=n>classes_</span><span class=p>,</span>
    <span class=n>index</span><span class=o>=</span><span class=n>test</span><span class=o>.</span><span class=n>index</span>
<span class=p>)</span>
<span class=n>y_pred</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div><table><thead><tr><th style=text-align:right>prescription_id</th><th style=text-align:right>contact_lenses</th><th style=text-align:right>glasses</th><th style=text-align:right>pharmacy</th></tr></thead><tbody><tr><td style=text-align:right>495866</td><td style=text-align:right>0.984725</td><td style=text-align:right>0.0115185</td><td style=text-align:right>0.00375634</td></tr><tr><td style=text-align:right>495838</td><td style=text-align:right>0.041953</td><td style=text-align:right>0.187964</td><td style=text-align:right>0.770083</td></tr><tr><td style=text-align:right>495838</td><td style=text-align:right>0.041953</td><td style=text-align:right>0.187964</td><td style=text-align:right>0.770083</td></tr><tr><td style=text-align:right>495825</td><td style=text-align:right>0.0221103</td><td style=text-align:right>0.966687</td><td style=text-align:right>0.011203</td></tr><tr><td style=text-align:right>495812</td><td style=text-align:right>0.964409</td><td style=text-align:right>0.0213663</td><td style=text-align:right>0.0142247</td></tr></tbody></table><p>Note: the <code>prescriptions</code> variable is a <code>pandas.DataFrame</code> containing the prescription text transcriptions along with the type (glasses, lenses, or pharmacy).</p><p>I decided to trust the classifier if the probability it assigned to a class was above <code>0.7</code>. If the classifier is unsure, the prescription type can&rsquo;t be determined, and it will be rejected or sent for manual processing. This rule provided the following classification report:</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>classification_report</span>

<span class=n>confident</span> <span class=o>=</span> <span class=n>y_pred</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;columns&#39;</span><span class=p>)</span> <span class=o>&gt;</span> <span class=o>.</span><span class=mi>7</span>
<span class=k>print</span><span class=p>(</span><span class=n>classification_report</span><span class=p>(</span>
    <span class=n>test</span><span class=p>[</span><span class=n>confident</span><span class=p>][</span><span class=s1>&#39;type&#39;</span><span class=p>],</span>
    <span class=n>y_pred</span><span class=p>[</span><span class=n>confident</span><span class=p>]</span><span class=o>.</span><span class=n>idxmax</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;columns&#39;</span><span class=p>),</span>
    <span class=n>digits</span><span class=o>=</span><span class=mi>3</span>
<span class=p>))</span>
</code></pre></div><pre><code>                precision    recall  f1-score   support

contact_lenses      0.981     0.964     0.972       950
       glasses      0.991     0.986     0.989      2104
      pharmacy      0.987     0.999     0.993      2319

      accuracy                          0.988      5373
     macro avg      0.986     0.983     0.985      5373
  weighted avg      0.988     0.988     0.987      5373
</code></pre><p>The number of cases where the model is confident enough &ndash; the recall in other words &ndash; can be computed as so:</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;Recall is {confident.sum() / len(confident):.2%}&#39;</span><span class=p>)</span>
</code></pre></div><pre><code>Recall is 90.24%
</code></pre><p>Good enough! I checked some false positive cases and all of them were mislabeled. Indeed, the training data I&rsquo;m using are manual annotations made by our operators. Once in a while they make a mistake, which is totally acceptable and simply something we have to deal with. However, learning with noisy labels is another story that I don&rsquo;t discuss in this blog post. I do however recommend <a href=https://arxiv.org/pdf/1911.00068.pdf>this research article</a> if you&rsquo;re interested.</p><p>I&rsquo;m confident enough to put this model into production. At Alan we like to keep things simple. Our website is a <a href=https://flask.palletsprojects.com/en/1.1.x/>Flask</a> app running on <a href=https://www.heroku.com/>Heroku</a>. We want to load this scikit-learn model in memory when the app boots up, and simply call it when a prescription has to be classified. If this pattern works for us, we might use it for other document classification tasks.</p><p>The model I trained weighs around 2.7 MB. That&rsquo;s over 8000 times <a href=https://www.bbc.com/future/article/20190704-apollo-in-50-numbers-the-technology>the amount of RAM</a> in the first Apollo shuttle that went to the Moon. Obviously I&rsquo;m saying this with my tongue in my cheek. And yet, I believe it&rsquo;s important to be frugal and not waste the memory of our precious <a href=https://www.heroku.com/dynos>Heroku dynos</a>. Especially if I want to convince the engineering team to use more machine learning models.</p><p>In a text classifier, the importance of each word follows a long-tail distribution. Most words are not important at all, which is the driving insight for <a href=https://www.wikiwand.com/en/Knowledge_distillation>knowledge distillation</a>. As we will see, this is very true for the above model. I decided to push the envelope and lighten the memory footprint of the model, while preserving its accuracy. Anyway, enough with the (long) introduction, here goes.</p><h2 id=measuring-the-models-size>Measuring the model&rsquo;s size</h2><p>From experience, I have a good idea of the model&rsquo;s memory footprint layout. However, it&rsquo;s always good to confirm this by measuring the memory footprint of each part of the model. I wrote a little utility to do this for any scikit-learn pipeline:</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=kn>from</span> <span class=nn>humanize</span> <span class=kn>import</span> <span class=n>naturalsize</span>
<span class=kn>from</span> <span class=nn>pickle</span> <span class=kn>import</span> <span class=n>dumps</span>

<span class=k>def</span> <span class=nf>obj_size</span><span class=p>(</span><span class=n>obj</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>naturalsize</span><span class=p>(</span><span class=n>sys</span><span class=o>.</span><span class=n>getsizeof</span><span class=p>(</span><span class=n>dumps</span><span class=p>(</span><span class=n>obj</span><span class=p>)))</span>

<span class=k>def</span> <span class=nf>estimator_size</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
    <span class=k>return</span> <span class=p>{</span>
        <span class=n>attr</span><span class=p>:</span> <span class=n>obj_size</span><span class=p>(</span><span class=nb>getattr</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>attr</span><span class=p>))</span>
        <span class=k>for</span> <span class=n>attr</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=vm>__dict__</span> <span class=k>if</span> <span class=n>attr</span><span class=o>.</span><span class=n>endswith</span><span class=p>(</span><span class=s1>&#39;_&#39;</span><span class=p>)</span>
    <span class=p>}</span>

<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>step</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>steps</span><span class=p>:</span>
    <span class=k>print</span><span class=p>(</span><span class=n>name</span><span class=p>)</span>
    <span class=k>for</span> <span class=n>attr</span><span class=p>,</span> <span class=n>size</span> <span class=ow>in</span> <span class=n>estimator_size</span><span class=p>(</span><span class=n>step</span><span class=p>)</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
        <span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;</span><span class=se>\t</span><span class=s1>{attr.ljust(20)} {size}&#39;</span><span class=p>)</span>
    <span class=k>print</span><span class=p>()</span>

<span class=k>print</span><span class=p>(</span><span class=s1>&#39;Total&#39;</span><span class=o>.</span><span class=n>ljust</span><span class=p>(</span><span class=mi>20</span><span class=p>),</span> <span class=s1>&#39; &#39;</span> <span class=o>*</span> <span class=mi>7</span><span class=p>,</span> <span class=n>obj_size</span><span class=p>(</span><span class=n>model</span><span class=p>))</span>
</code></pre></div><pre><code>countvectorizer
	fixed_vocabulary_    37 Bytes
	stop_words_          38 Bytes
	vocabulary_          980.8 kB

normalizer
	n_features_in_       50 Bytes

logisticregression
	n_features_in_       50 Bytes
	classes_             219 Bytes
	n_iter_              184 Bytes
	coef_                1.8 MB
	intercept_           204 Bytes

Total                        2.7 MB
</code></pre><p>The above table shows the memory footprint of each attribute for each step of the pipeline. I&rsquo;m only looking at the attributes whose name ends with a <code>_</code> because that&rsquo;s the scikit-learn convention for marking attributes that have been created during the <code>fit</code> call. In other words, I&rsquo;m ignoring hyperparameters that are provided during initialization, because their memory footprint is insignificant.</p><p>As we can see, most of the memory footprint is taken up by the logistic regression <code>coef_</code> attribute. The latter is a matrix which stores the $n \times k$ weights for each of the $n$ words and the $k$ classes. The second culprit is the count vectorizer&rsquo;s <code>vocabulary_</code> attribute, which assigns an index to each word. This allows determining the weight vector of each word because <code>coef_</code> is integer-indexed instead of being word-indexed. We can now confidently move on and reduce the memory footprint of these two attributes.</p><h2 id=reducing-the-models-size>Reducing the model&rsquo;s size</h2><p>In a logistic regression, the importance of a feature is proportional to the magnitude of its weight vector. Therefore, unimportant features have a weight vector whose magnitude is close to 0. Intuitively, their contribution to the final sum is very small. In our case the features are the words in the text. By determining the unimportant words, we may reduce the model&rsquo;s memory by limiting the considered vocabulary.</p><p>First, let&rsquo;s measure the importance of each word. We can compute the feature-wise $L^2$ norm to measure the magnitude of each word&rsquo;s weight vector.</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>np</span>

<span class=n>log_reg</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>steps</span><span class=p>[</span><span class=mi>2</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
<span class=n>W</span> <span class=o>=</span> <span class=n>log_reg</span><span class=o>.</span><span class=n>coef_</span>
<span class=n>magnitudes</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>W</span><span class=p>,</span> <span class=nb>ord</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</code></pre></div><p>Now let&rsquo;s list the most important words by sorting their associated weight vector magnitudes.</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=n>vectorizer</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>steps</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
<span class=n>idx_to_word</span> <span class=o>=</span> <span class=p>{</span>
    <span class=n>idx</span><span class=p>:</span> <span class=n>word</span>
    <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>vectorizer</span><span class=o>.</span><span class=n>vocabulary_</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
<span class=p>}</span>

<span class=n>top</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>magnitudes</span><span class=p>)[::</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>

<span class=n>meaningful_vocab</span> <span class=o>=</span> <span class=p>[</span>
    <span class=n>idx_to_word</span><span class=p>[</span><span class=n>k</span><span class=p>]</span>
    <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>top</span><span class=p>[:</span><span class=mi>100</span><span class=p>]</span>
<span class=p>]</span>
</code></pre></div><p>Here are the 10 most important words:</p><pre><code>lentilles
monture
verres
lunettes
ophtalmologiste
50
oeil
ophtalmologie
25
gauche
</code></pre><p>Looks good to me! I&rsquo;m not too sure what the 25 and 50 are doing there, but I guess they&rsquo;re indicative of drug dosage (e.g. &ldquo;50 mg of paracetamol&rdquo;), and are thus distinctive indicators of pharmaceutical prescriptions.</p><p>We can prune the model now that we know which words matter. I found that the easiest way to proceed is to specify a list of words to the <code>CountVectorizer</code>. Indeed, the latter has a <code>vocabulary</code> parameter which overrides the lists of words that are used in the model. By default, all the words in the training set are used.</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=n>pruned_model</span> <span class=o>=</span> <span class=n>make_pipeline</span><span class=p>(</span>
    <span class=n>CountVectorizer</span><span class=p>(</span>
        <span class=n>vocabulary</span><span class=o>=</span><span class=n>meaningful_vocab</span>
    <span class=p>),</span>
    <span class=n>Normalizer</span><span class=p>(),</span>
    <span class=n>LogisticRegression</span><span class=p>()</span>
<span class=p>)</span>

<span class=n>pruned_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>],</span> <span class=n>train</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>])</span>
</code></pre></div><p>Let&rsquo;s first check the pruned model&rsquo;s performance.</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=n>y_pred_pruned</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span>
    <span class=n>pruned_model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>test</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]),</span>
    <span class=n>columns</span><span class=o>=</span><span class=n>pruned_model</span><span class=o>.</span><span class=n>classes_</span><span class=p>,</span>
    <span class=n>index</span><span class=o>=</span><span class=n>test</span><span class=o>.</span><span class=n>index</span>
<span class=p>)</span>

<span class=n>confident</span> <span class=o>=</span> <span class=n>y_pred_pruned</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;columns&#39;</span><span class=p>)</span> <span class=o>&gt;</span> <span class=o>.</span><span class=mi>7</span>
<span class=k>print</span><span class=p>(</span><span class=n>classification_report</span><span class=p>(</span>
    <span class=n>test</span><span class=p>[</span><span class=n>confident</span><span class=p>][</span><span class=s1>&#39;type&#39;</span><span class=p>],</span>
    <span class=n>y_pred_pruned</span><span class=p>[</span><span class=n>confident</span><span class=p>]</span><span class=o>.</span><span class=n>idxmax</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;columns&#39;</span><span class=p>),</span>
    <span class=n>digits</span><span class=o>=</span><span class=mi>3</span>
<span class=p>))</span>
</code></pre></div><pre><code>                precision    recall  f1-score   support

contact_lenses      0.976     0.963     0.970      1068
       glasses      0.988     0.985     0.986      2170
      pharmacy      0.991     0.999     0.995      2324

      accuracy                          0.987      5562
     macro avg      0.985     0.982     0.984      5562
  weighted avg      0.987     0.987     0.987      5562
</code></pre><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;Recall is {confident.sum() / len(confident):.2%}&#39;</span><span class=p>)</span>
</code></pre></div><pre><code>Recall is 93.42%
</code></pre><p>The performance is (slightly) better! The intention of this pruning process was not to improve the model, but it did. That&rsquo;s the magic of regularization for you. Indeed, the pruning process we&rsquo;ve applied boils down to <a href=https://www.wikiwand.com/en/Regularization_(mathematics)#/Regularizers_for_sparsity>sparsity regularisation</a>.</p><p>Let&rsquo;s now see the memory footprint of the pruned model.</p><div class=highlight><pre class=chroma><code class=language-py data-lang=py><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>step</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>steps</span><span class=p>:</span>
    <span class=k>print</span><span class=p>(</span><span class=n>name</span><span class=p>)</span>
    <span class=k>for</span> <span class=n>attr</span><span class=p>,</span> <span class=n>size</span> <span class=ow>in</span> <span class=n>estimator_size</span><span class=p>(</span><span class=n>step</span><span class=p>)</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
        <span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s1>&#39;</span><span class=se>\t</span><span class=s1>{attr.ljust(20)} {size}&#39;</span><span class=p>)</span>
    <span class=k>print</span><span class=p>()</span>

<span class=k>print</span><span class=p>(</span><span class=s1>&#39;Total&#39;</span><span class=o>.</span><span class=n>ljust</span><span class=p>(</span><span class=mi>20</span><span class=p>),</span> <span class=s1>&#39; &#39;</span> <span class=o>*</span> <span class=mi>7</span><span class=p>,</span> <span class=n>obj_size</span><span class=p>(</span><span class=n>model</span><span class=p>))</span>
</code></pre></div><pre><code>countvectorizer
	fixed_vocabulary_    37 Bytes
	vocabulary_          1.1 kB

normalizer
	n_features_in_       38 Bytes

logisticregression
	n_features_in_       38 Bytes
	classes_             219 Bytes
	n_iter_              184 Bytes
	coef_                2.6 kB
	intercept_           204 Bytes

Total                        5.0 kB
</code></pre><p>We&rsquo;ve reduced the model&rsquo;s memory consumption by a whopping factor of ~545. We&rsquo;ve done this without impacting the memory&rsquo;s accuracy. It&rsquo;s a free win! Note that you could push the envelope even further by reducing the precision of the coefficient matrix to 32 bits instead of 64 bits, but the savings would be marginal.</p></div><script type=text/javascript>var s=document.createElement('script');s.setAttribute('src','https://utteranc.es/client.js'),s.setAttribute('repo','MaxHalford/maxhalford.github.io'),s.setAttribute('issue-term','pathname'),s.setAttribute('crossorigin','anonymous'),s.setAttribute('async',null),s.setAttribute('theme','github-light'),document.body.appendChild(s)</script><div class=footer><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.0/elevator.min.js></script><script>var elementButton=document.querySelector('.elevator'),elevator=new Elevator({element:elementButton,mainAudio:'/music/elevator.mp3',endAudio:'/music/ding.mp3'})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=site-footer><div class=site-footer-item><a href=https://github.com/MaxHalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M8 0C3.58.0.0 3.582.0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385.0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953.0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117.0.0.67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147.0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48.0 1.07-.01 1.93-.01 2.19.0.21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg></span></a></div><div class=site-footer-item><a href=https://linkedin.com/in/maxhalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235.0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4.0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762.0-1.376-.617-1.376-1.377.0-.758.614-1.375 1.376-1.375.76.0 1.376.617 1.376 1.375.0.76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816.0H1.18C.528.0.0.516.0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652.0 1.185-.516 1.185-1.153V1.153C16 .516 15.467.0 14.815.0z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://twitter.com/halford_max><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812.0-3.282 1.47-3.282 3.28.0.26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21.0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26.0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04.0 9.34-5 9.34-9.33.0-.14.0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://kaggle.com/maxhalford><span class=inline-svg><svg role="img" viewBox="0 0 26 26" xmlns="http://www.w3.org/2000/svg"><title>Kaggle icon</title><path fill="currentcolor" d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187.0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236.0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234.0.351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144.0.236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.358"/></svg></span></a></div><div class=site-footer-item><a href="https://scholar.google.com/citations?user=erRNNi0AAAAJ&hl=en"><span class=inline-svg><svg viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952.0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584.0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288.0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962.0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zm762.098 382.384c-64.358 64.424-141.86 96.57-232.572 96.57H329.144c-90.712.0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572V182.859c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712.0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159V762.033c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162.0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53.0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476.0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86.0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908.0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426.0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432.0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234.0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048.0 6.642.19 12.492.672 18.974H292.574l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382.0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994.0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376.0 103.05 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382.0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.05 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z"/></svg></span></a></div><div class=site-footer-item><a href=/files/resume_max_halford.pdf><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 392.533 392.533" style="enable-background:new 0 0 392.533 392.533"><g><g><path fill="currentcolor" d="M292.396 324.849H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 329.697 298.473 324.849 292.396 324.849z"/></g></g><g><g><path fill="currentcolor" d="M292.396 277.01H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 281.859 298.473 277.01 292.396 277.01z"/></g></g><g><g><path fill="currentcolor" d="M196.137 45.834c-25.859.0-46.998 21.075-46.998 46.998.0 25.859 21.139 46.933 46.998 46.933s46.998-21.075 46.998-46.998-21.139-46.933-46.998-46.933zm0 72.017c-13.77.0-25.083-11.313-25.083-25.083s11.248-25.083 25.083-25.083 25.083 11.313 25.083 25.083c0 13.769-11.313 25.083-25.083 25.083z"/></g></g><g><g><path fill="currentcolor" d="M258.521 163.362c-39.887-15.515-84.752-15.515-124.638.0-13.059 5.107-21.786 18.101-21.786 32.388v44.347c-.065 6.012 4.849 10.925 10.861 10.925h146.424c6.012.0 10.925-4.848 10.925-10.925V195.75C280.307 181.463 271.58 168.469 258.521 163.362zm0 65.874H133.883v-33.422c0-5.301 3.168-10.214 7.887-12.024 34.844-13.511 74.02-13.511 108.865.0 4.719 1.875 7.887 6.659 7.887 12.024v33.422z"/></g></g><g><g><path fill="currentcolor" d="M313.083.0H131.491c-8.404.0-16.291 3.232-22.238 9.18L57.018 61.414c-5.947 5.948-9.18 13.834-9.18 22.238v277.333c0 17.39 14.158 31.547 31.547 31.547h233.762c17.39.0 31.547-14.158 31.547-31.547V31.547C344.501 14.158 330.343.0 313.083.0zM112.032 37.236v27.022H85.01l27.022-27.022zm210.683 79.58h-40.598c-6.012.0-10.925 4.849-10.925 10.925.0 6.012 4.848 10.925 10.925 10.925h40.598v19.394h-14.869c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h14.869v181.139c0 5.366-4.331 9.697-9.632 9.697H79.192c-5.301.0-9.632-4.331-9.632-9.632V86.044h53.398c6.012.0 10.925-4.848 10.925-10.925V21.721h179.2c5.301.0 9.632 4.331 9.632 9.632v85.463z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></span></a></div><div class=site-footer-item><a href=https://play.spotify.com/user/1166811350><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 168 168"><path fill="currentcolor" d="m83.996.277C37.747.277.253 37.77.253 84.019c0 46.251 37.494 83.741 83.743 83.741 46.254.0 83.744-37.49 83.744-83.741.0-46.246-37.49-83.738-83.745-83.738l.001-.004zm38.404 120.78c-1.5 2.46-4.72 3.24-7.18 1.73-19.662-12.01-44.414-14.73-73.564-8.07-2.809.64-5.609-1.12-6.249-3.93-.643-2.81 1.11-5.61 3.926-6.25 31.9-7.291 59.263-4.15 81.337 9.34 2.46 1.51 3.24 4.72 1.73 7.18zm10.25-22.805c-1.89 3.075-5.91 4.045-8.98 2.155-22.51-13.839-56.823-17.846-83.448-9.764-3.453 1.043-7.1-.903-8.148-4.35-1.04-3.453.907-7.093 4.354-8.143 30.413-9.228 68.222-4.758 94.072 11.127 3.07 1.89 4.04 5.91 2.15 8.976v-.001zm.88-23.744c-26.99-16.031-71.52-17.505-97.289-9.684-4.138 1.255-8.514-1.081-9.768-5.219-1.254-4.14 1.08-8.513 5.221-9.771 29.581-8.98 78.756-7.245 109.83 11.202 3.73 2.209 4.95 7.016 2.74 10.733-2.2 3.722-7.02 4.949-10.73 2.739z"/></svg></span></a></div><div class=site-footer-item><a href=mailto:maxhalford25@gmail.com><span class=inline-svg><svg viewBox="0 0 15 20" xmlns="http://www.w3.org/2000/svg"><title>mail</title><path fill="currentcolor" d="M0 4v8c0 .55.45 1 1 1h12c.55.0 1-.45 1-1V4c0-.55-.45-1-1-1H1c-.55.0-1 .45-1 1zm13 0L7 9 1 4h12zM1 5.5l4 3-4 3v-6zM2 12l3.5-3L7 10.5 8.5 9l3.5 3H2zm11-.5-4-3 4-3v6z" fill="#000" fill-rule="evenodd"/></svg></span></a></div><div class=site-footer-item><a href=/index.xml><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M12.8 16C12.8 8.978 7.022 3.2.0 3.2V0c8.777.0 16 7.223 16 16h-3.2zM2.194 11.61c1.21.0 2.195.985 2.195 2.196.0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017.0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818.0 10.606 4.79 10.606 10.607z"/></svg></span></a></div></div></div></div></article><script></script></body></html>