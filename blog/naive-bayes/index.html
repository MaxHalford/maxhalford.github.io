<!doctype html><html lang=en><head><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogImMwMjJhMjg1LWJmY2YtNDc0OC1hYzczLTJhMDQ1Njk3NTI0YyIsICJpZCI6ICIzNjc3NjQ3Ny04MTE2LTRmYWQtYjcwMy1iZmM3YjMwZGJjMjMifQ.A0vHm-VWbXG6uBFZiwuspN_AyfSYNrdZE3IgwgWSt4g></script><meta charset=utf-8><meta name=generator content="Hugo 0.124.0"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/naive-bayes/"><link rel=canonical href=https://maxhalford.github.io/blog/naive-bayes/><meta property="og:title" content="The Na√Øve Bayes classifier"><meta property="og:description" content="The objective of a classifier is to decide to which class (also called label) to assign an observation based on observed data. In supervised learning, this is done by taking into account previous classifications. In other words if we know that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the training set."><meta property="og:type" content="article"><meta property="og:url" content="https://maxhalford.github.io/blog/naive-bayes/"><meta property="og:image" content="https://maxhalford.github.io/img/belle-ile.jpg"><meta property="article:section" content="blog"><meta property="article:published_time" content="2015-09-10T00:00:00+00:00"><meta property="article:modified_time" content="2015-09-10T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://maxhalford.github.io/img/belle-ile.jpg"><meta name=twitter:title content="The Na√Øve Bayes classifier"><meta name=twitter:description content="The objective of a classifier is to decide to which class (also called label) to assign an observation based on observed data. In supervised learning, this is done by taking into account previous classifications. In other words if we know that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the training set."><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶Ü</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"The Na√Øve Bayes classifier","headline":"The Na√Øve Bayes classifier","description":"The objective of a classifier is to decide to which class (also called label) to assign an observation based on observed data. In supervised learning, this is done by taking into account previous classifications. In other words if we know that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the training set.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2015","datePublished":"2015-09-10 00:00:00 \u002b0000 UTC","dateModified":"2015-09-10 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/naive-bayes\/","keywords":["machine-learning"]}</script><title>The Na√Øve Bayes classifier ‚Ä¢ Max Halford</title>
<meta property="og:title" content="The Na√Øve Bayes classifier ‚Ä¢ Max Halford"><meta property="og:type" content="article"><meta name=description content="The objective of a classifier is to decide to which class (also called label) to assign an observation based on observed data. In supervised learning, this is done by taking into account previous classifications. In other words if we know that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the training set."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford ü¶Ü</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>The Na√Øve Bayes classifier</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2015-09-10 00:00:00 UTC">2015-09-10
</time><span class=posts-line-tag>machine-learning</span></div></div></header><div class="post-content markdown-body"><p>The objective of a classifier is to decide to which <em>class</em> (also called <em>label</em>) to assign an observation based on observed data. In <em>supervised learning</em>, this is done by taking into account previous classifications. In other words if we <em>know</em> that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the <em>training set</em>.</p><p>In mathematical terms say $X$ is a list of attributes for each one of a group of $n$ observations. In the training set, each observation $X_i$ has a label $Y_i$ associated to it. The question is which $Y$ to associate to a new $X_i$?</p><p>The Na√Øve Bayes method is a probabilistic approach. It assigns a probability to each label $Y_i$ based on the training set. The best guess is the label with the highest probability.</p><h2 id=the-mathematics>The mathematics</h2><p>The general idea is to know $P_F(C)$ where $C$ is a discrete class/label and $F$ is a list of <em>features</em>. For example if the class was an author and the features were words in a text the conditional probability $P_F(C)$ could be read as</p><blockquote><p>&ldquo;What is the probability that a text is written by author $C$ given that words $F$ are in the text?&rdquo;</p></blockquote><p>Good question. This is where the &ldquo;Bayes&rdquo; in the title comes from <a href="https://www.wikiwand.com/en/Bayes'_theorem">Bayes&rsquo; theorem</a> gives</p><p>$$P_F(C_j) = \frac{P(C_j) \times P_{C_j}(F)}{P(F)}$$</p><ul><li>$P(C_j)$ is called the <em>prior</em>. It is the probability we are looking for <em>before</em> the data is being considered. In philosophy the prior is the state of the knowledge on a subject before an experience is performed.</li><li>$P_{C_j}(F)$ is the frequency at which $F$ appeared for $C_j$. This is the observed data in the training set.</li><li>$P(F)$ represents the support $F$ provides for $C$.</li></ul><p>In a sense Bayes&rsquo; theorem is a &ldquo;magic&rdquo; formula which gives the probability of having A (for example an author) based on the fact that we already B (for example some words). It does because we already know at what frequency B appears when A does. In other words, if the word &ldquo;Quiditch&rdquo; appears in a sentence, you have a good chance that the sentence belongs to a Harry Potter book because <em>you already know</em> that it frequently appears in Harry Potter books.</p><p>If we calculate $P_F(C_j)$ for every $j$ then we can sort them based on their value and choose one of them. This is called a <em>decision rule</em>. Not surprisingly the most common decision rule is to take the highest probability, this is called <em>maximum a posteriori</em> (MAP).</p><p>Now for calculating $P_F(C_j)$:</p><ul><li>$P(C_j)$ is simply the frequency of the $C_j$ class.</li><li>$P(F)$ doesn&rsquo;t depend on $C$ and so it doesn&rsquo;t have to be calculated. Indeed $P(F)$ is the same for every $C$ and won&rsquo;t affect the order of all the $P_F(C_j)$.</li><li>$P_{C_j}(F)$ is the only (slighly) tricky probability to compute.</li></ul><p>$P_{C_j}(F)$ is the probability of having a list of features in a text. For the sake of simplicity we can suppose that the occurrences of features are independent from each other (this is why the classifier is considered &ldquo;Na√Øve&rdquo;). Of course this is false, indeed text often use semantic fields. However it has been shown that the classifier does well even if the assumption seems foolish. Because the events are independent the mathematics become very convenient. Indeed if there are $p$ features in a text the probability that a document belongs to a class can now be written down as</p><p>$$P_{C_j}(F) = \prod_{k=1}^p P_{C_j}(f_k) = \prod_{k=1}^p \frac{P(C_j \cap f_k)}{P(C_j)}$$</p><p>It&rsquo;s important to understand that this isn&rsquo;t true if the features occurrences are not considered independent. For example, the events $A$ &ldquo;It&rsquo;s raining&rdquo; and $B$ &ldquo;It&rsquo;s cold&rdquo; are not independent. In a probabilistic sense, the event $P(A \cap B)$ (&ldquo;It&rsquo;s cold and rainy&rdquo;) is <em>not</em> equal to $P(A) \times P(B)$. This is because most of the time one implies the other. In our case it&rsquo;s mathematically convenient to consider the independence of events, that&rsquo;s all.</p><h2 id=python-implementation>Python implementation</h2><p>If you want to run the code that will come up yourself, you can simply copy/paste it all into your text editor, it will work out of the box. However you will have to download the pandas module in order to read a CSV dataframe.</p><p>I like using classes, you really can make most of <a href=https://www.wikiwand.com/en/Object-oriented_programming>object-oriented programming</a> for creating tidy packages. In this case there are two classes.</p><ul><li>A <code>Counter</code> class for parsing the texts and estimating the probabilities.</li><li>A <code>NaiveBayes</code> class for predicting the class of new texts which inherits from <code>Counter</code>.</li></ul><p>The main advantage of having a <code>Counter</code> class is that it can be reused for different classifiers. In machine learning in general it&rsquo;s a good idea to split feature extraction and prediction. This is the first thing to code.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Counter</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dataframe</span><span class=p>,</span> <span class=n>features</span><span class=p>,</span> <span class=n>textCol</span><span class=p>,</span> <span class=n>classCol</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Extract the features of a dataframe&#39;s column and classify
</span></span></span><span class=line><span class=cl><span class=s1>        them according to another dataframe&#39;s column.
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=c1># Keep the same feature extraction function</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>featureFunction</span> <span class=o>=</span> <span class=n>features</span>
</span></span><span class=line><span class=cl>        <span class=c1># Counts of features in a particular class</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>features</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=c1># Counts of class occurences</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=c1># Classify the dataframe</span>
</span></span><span class=line><span class=cl>        <span class=n>dataframe</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>classify</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>args</span><span class=o>=</span><span class=p>(</span><span class=n>textCol</span><span class=p>,</span> <span class=n>classCol</span><span class=p>,))</span>
</span></span></code></pre></div><p>The <code>Counter</code> class takes 4 arguments. The first is a dataframe which should contain a column for the texts (3rd argument) and a column for the classes (4th argument). The <code>features</code> argument is a function that extracts the desired features from a text; it can be anything as long as it returns an iterable (list, set, array). For example one could extract <a href=https://www.wikiwand.com/en/Word_stem>stemmed</a> and/or lower cased words. We will get to that later on. In order to compute the probabilities described above we have to count the features and class occurences, the best way to do this is to use dictionaries. The last line parses all the dataframe with the following procedure.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>classify</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>textCol</span><span class=p>,</span> <span class=n>classCol</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Classify a dataframe row based on a text column and a class
</span></span></span><span class=line><span class=cl><span class=s1>        column and a feature function.
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=c1># Extract the category of the row</span>
</span></span><span class=line><span class=cl>        <span class=n>classification</span> <span class=o>=</span> <span class=n>row</span><span class=p>[</span><span class=n>classCol</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=c1># Initialize the category count if it doesn&#39;t exist yet</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=o>.</span><span class=n>setdefault</span><span class=p>(</span><span class=n>classification</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Increment the count for the category</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=p>[</span><span class=n>classification</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=c1># Transform the text column into an iterable of features</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>feature</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>featureFunction</span><span class=p>(</span><span class=n>row</span><span class=p>[</span><span class=n>textCol</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>            <span class=c1># Initialize the feature count if doesn&#39;t exist yet</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=o>.</span><span class=n>setdefault</span><span class=p>(</span><span class=n>feature</span><span class=p>,</span> <span class=p>{})</span>
</span></span><span class=line><span class=cl>            <span class=c1># Initialize the feature&#39;s category count</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>setdefault</span><span class=p>(</span><span class=n>classification</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># Increment the feature&#39;s category count</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=n>feature</span><span class=p>][</span><span class=n>classification</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span></code></pre></div><p>The procedure takes a row as argument and analyses the text and class column. If the class has already appeared then it&rsquo;s count is incremented, else it begins at 1. The same goes for every feature extracted from the text. The <code>features</code> dictionary has two levels of depth, one for the feature and one for the class. With this implementation it&rsquo;s very easy to get the occurences of a feature in a text of a specific category: <code>features[f][c]</code>.</p><p>Now that the texts have been parsed the features we need a function that returns $P_{C_j}(f_k)$ in order to compute $P_{C_j}(F)$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>probability</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>classification</span><span class=p>,</span> <span class=n>feature</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Calculate the probability that a feature belongs to a
</span></span></span><span class=line><span class=cl><span class=s1>        certain class, ie. P(classification | feature).
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=c1># Count of classification occurence of a feature</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>feature</span> <span class=ow>not</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>classification</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=n>feature</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=n>count</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=n>feature</span><span class=p>][</span><span class=n>classification</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>count</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=c1># Total count of classification occurences</span>
</span></span><span class=line><span class=cl>        <span class=n>total</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=p>[</span><span class=n>classification</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=c1># Compute ratio (will be lower then 1 and higher then 0)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>count</span> <span class=o>/</span> <span class=n>total</span>
</span></span></code></pre></div><p>A problem that I haven&rsquo;t mentioned is how to deal with features that haven&rsquo;t been encountered/classified in previous texts when doing a prediction. Intuitively an unencountered feature should bias our the decision rule towards a specific class. Because $P_{C_j}(F)$ is the result of a product, multiplying it by $1$ won&rsquo;t affect it ($1$ is the <em>identity element</em> for the multiplication of real numbers).</p><p>The same kind of problem arises as to what probability to assign if a feature has been encountered but not for a certain class. In some sense this should be something <em>low</em>. But it shouldn&rsquo;t be 0, because then $P_{C_j}(F)$ would be equal to 0 and that is too harsh. However it&rsquo;s also reasonable to return $1$ after the <code>else</code>, which is equivalent to ignoring the feature (but not ignoring the fact that it was never associated with that class!).</p><p>Either the feature has already been associated with a class and we can calculate the <code>count / total</code> ratio, which is the number of times a feature has been associated with a class over the number of occurences of a class. In other words this is the frequency of the association between a feature and a class ($\frac{P(C_j \cap f_k)}{P(C_j)})$. In the other case the ratio is <code>1 / total</code>, which is very pessimistic.</p><p>Now we can code the <code>NaiveBayes</code> class, the one that will be used for prediction.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>NaiveBayes</span><span class=p>(</span><span class=n>Counter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predictSentence</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>sentence</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39; Classify one sentence based on it&#39;s features. &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>likelihoods</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=c1># For each possible classification</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>classification</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=c1># Compute P(classification)</span>
</span></span><span class=line><span class=cl>            <span class=n>likelihoods</span><span class=p>[</span><span class=n>classification</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span> <span class=o>+</span> \
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>classification</span><span class=p>]</span> <span class=o>/</span> <span class=o>+</span> <span class=nb>sum</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=o>.</span><span class=n>values</span><span class=p>())</span>
</span></span><span class=line><span class=cl>            <span class=c1># Multiply iy by the product of each P(F_i)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>feature</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>featureFunction</span><span class=p>(</span><span class=n>sentence</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>likelihoods</span><span class=p>[</span><span class=n>classification</span><span class=p>]</span> <span class=o>*=</span> <span class=bp>self</span><span class=o>.</span><span class=n>probability</span> <span class=o>+</span> \
</span></span><span class=line><span class=cl>                <span class=p>(</span><span class=n>classification</span><span class=p>,</span> <span class=n>feature</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>max</span><span class=p>(</span><span class=n>likelihoods</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=n>likelihoods</span><span class=o>.</span><span class=n>get</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>df</span><span class=p>,</span> <span class=n>textCol</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39; Classify a test dataframe and return a series object. &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>classifications</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>textCol</span><span class=p>]</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>predictSentence</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>classifications</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>words</span><span class=p>(</span><span class=n>sentence</span><span class=p>,</span> <span class=n>threshold</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    Obtain an iterable of words from a sentence. Lowercase the words.
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>splitter</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\\</span><span class=s1>W+&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Concatenate the lines into a big string</span>
</span></span><span class=line><span class=cl>    <span class=n>words</span> <span class=o>=</span> <span class=p>(</span><span class=n>word</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>splitter</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>sentence</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=c1># Lower case</span>
</span></span><span class=line><span class=cl>    <span class=n>words</span> <span class=o>=</span> <span class=p>(</span><span class=n>word</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>words</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Done!</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>words</span>
</span></span></code></pre></div><p>The first function classifies a sentence. The outer <code>for</code> loops over all the possible classes. Lines <code>9</code> and <code>10</code> compute $P_{C_j}$. The inner <code>for</code> loops iterates over all the features in order to compute $P_{C_j}(F)$. Finally the class that has the highest probability is returned. The second function applies the first function to a dataframe.</p><p>The only piece of the puzzle missing is a feature function. Basically, if we have a sentence, we want to be able to transform it into a vector of features (which can be words, prefixes, stems, &mldr;) in order to compute individual probabilities for each feature. A very simple approach is to split a sentence on it&rsquo;s white spaces and to put the words to lowercase.</p><p>Choosing a good feature function is crucial for the performance of a classifier, some people argue that it is even more important than the choice of the classifier.</p><p>The following piece of code puts all the pieces together.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;www.maxhalford.com/data/datasets/authors.csv&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>delimiter</span><span class=o>=</span><span class=s1>&#39;</span><span class=se>\t</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bayes</span> <span class=o>=</span> <span class=n>NaiveBayes</span><span class=p>(</span><span class=n>dataframe</span><span class=o>=</span><span class=n>df</span><span class=p>,</span> <span class=n>features</span><span class=o>=</span><span class=n>words</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                   <span class=n>textCol</span><span class=o>=</span><span class=s1>&#39;Sentence&#39;</span><span class=p>,</span> <span class=n>classCol</span><span class=o>=</span><span class=s1>&#39;Author&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>bayes</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=s1>&#39;Sentence&#39;</span><span class=p>))</span>
</span></span></code></pre></div></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px;margin-bottom:30px><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>
Back to the top</div></div><iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=32 width=114 style=border:0;border-radius:6px></iframe></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script><script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href=/blog/genetic-algorithms-introduction/>An introduction to genetic algorithms</a></li></ul></div></div></div></article></body></html>