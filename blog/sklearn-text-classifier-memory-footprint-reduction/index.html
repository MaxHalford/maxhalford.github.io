<!doctype html><html lang=en><head><script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script><meta charset=utf-8><meta name=generator content="Hugo 0.110.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/"><link rel=canonical href=https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Reducing the memory footprint of a scikit-learn text classifier","headline":"Reducing the memory footprint of a scikit-learn text classifier","description":"Context This week at Alan I\u0026rsquo;ve been working on parsing French medical prescriptions. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2021","datePublished":"2021-04-11 00:00:00 \u002b0000 UTC","dateModified":"2021-04-11 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/sklearn-text-classifier-memory-footprint-reduction\/","keywords":["machine-learning","text-processing"]}</script><title>Reducing the memory footprint of a scikit-learn text classifier â€¢ Max Halford</title><meta property="og:title" content="Reducing the memory footprint of a scikit-learn text classifier â€¢ Max Halford"><meta property="og:type" content="article"><meta name=description content="Context This week at Alan I&rsquo;ve been working on parsing French medical prescriptions. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford ðŸ¦”</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>Reducing the memory footprint of a scikit-learn text classifier</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2021-04-11 00:00:00 UTC">2021-04-11</time>
<span class=posts-line-tag>machine-learning</span>
<span class=posts-line-tag>text-processing</span></div></div></header><div class="post-content markdown-body"><h2 id=toc>Table of contents</h2><nav id=TableOfContents><ul><li><a href=#context>Context</a></li><li><a href=#measuring-the-models-size>Measuring the model&rsquo;s size</a></li><li><a href=#reducing-the-models-size>Reducing the model&rsquo;s size</a></li></ul></nav><h2 id=context>Context</h2><p>This week at Alan I&rsquo;ve been working on parsing <a href=https://www.wikiwand.com/fr/Ordonnance_(m%C3%A9decine)>French medical prescriptions</a>. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image. We can thus use the text transcription to classify the prescription.</p><p>I played around with writing some regex rules and reached a macro F1 score of 95%. Not bad, but not perfect. I wanted to reach 100% because it seemed to me like a simple classification task. That is, when I looked at some sample prescriptions, the type of the prescription was really obvious. I weighed the pros and cons of writing more complex regex patterns versus building a machine learning text classifier. I opted for the latter. I wrote a simple <a href=https://scikit-learn.org/stable/modules/compose.html>scikit-learn pipeline</a> and tested it as so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CountVectorizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.pipeline</span> <span class=kn>import</span> <span class=n>make_pipeline</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>Normalizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>make_pipeline</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>CountVectorizer</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>Normalizer</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>LogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train</span> <span class=o>=</span> <span class=n>prescriptions</span><span class=p>[:</span><span class=nb>len</span><span class=p>(</span><span class=n>prescriptions</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>test</span> <span class=o>=</span> <span class=n>prescriptions</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>prescriptions</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>],</span> <span class=n>train</span><span class=p>[</span><span class=s1>&#39;type&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y_pred</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>test</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>columns</span><span class=o>=</span><span class=n>model</span><span class=o>.</span><span class=n>classes_</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>index</span><span class=o>=</span><span class=n>test</span><span class=o>.</span><span class=n>index</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_pred</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</span></span></code></pre></div><table><thead><tr><th style=text-align:right>prescription_id</th><th style=text-align:right>contact_lenses</th><th style=text-align:right>glasses</th><th style=text-align:right>pharmacy</th></tr></thead><tbody><tr><td style=text-align:right>495866</td><td style=text-align:right>0.984725</td><td style=text-align:right>0.0115185</td><td style=text-align:right>0.00375634</td></tr><tr><td style=text-align:right>495838</td><td style=text-align:right>0.041953</td><td style=text-align:right>0.187964</td><td style=text-align:right>0.770083</td></tr><tr><td style=text-align:right>495838</td><td style=text-align:right>0.041953</td><td style=text-align:right>0.187964</td><td style=text-align:right>0.770083</td></tr><tr><td style=text-align:right>495825</td><td style=text-align:right>0.0221103</td><td style=text-align:right>0.966687</td><td style=text-align:right>0.011203</td></tr><tr><td style=text-align:right>495812</td><td style=text-align:right>0.964409</td><td style=text-align:right>0.0213663</td><td style=text-align:right>0.0142247</td></tr></tbody></table><p>Note: the <code>prescriptions</code> variable is a <code>pandas.DataFrame</code> containing the prescription text transcriptions along with the type (glasses, lenses, or pharmacy).</p><p>I decided to trust the classifier if the probability it assigned to a class was above <code>0.7</code>. If the classifier is unsure, the prescription type can&rsquo;t be determined, and it will be rejected or sent for manual processing. This rule provided the following classification report:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>classification_report</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>confident</span> <span class=o>=</span> <span class=n>y_pred</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;columns&#39;</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>.7</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>classification_report</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>test</span><span class=p>[</span><span class=n>confident</span><span class=p>][</span><span class=s1>&#39;type&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span><span class=p>[</span><span class=n>confident</span><span class=p>]</span><span class=o>.</span><span class=n>idxmax</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;columns&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>digits</span><span class=o>=</span><span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=p>))</span>
</span></span></code></pre></div><pre tabindex=0><code>                precision    recall  f1-score   support

contact_lenses      0.981     0.964     0.972       950
       glasses      0.991     0.986     0.989      2104
      pharmacy      0.987     0.999     0.993      2319

      accuracy                          0.988      5373
     macro avg      0.986     0.983     0.985      5373
  weighted avg      0.988     0.988     0.987      5373
</code></pre><p>The number of cases where the model is confident enough &ndash; the recall in other words &ndash; can be computed as so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Recall is </span><span class=si>{</span><span class=n>confident</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>confident</span><span class=p>)</span><span class=si>:</span><span class=s1>.2%</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>Recall is 90.24%
</code></pre><p>Good enough! I checked some false positive cases and all of them were mislabeled. Indeed, the training data I&rsquo;m using are manual annotations made by our operators. Once in a while they make a mistake, which is totally acceptable and simply something we have to deal with. However, learning with noisy labels is another story altogether, and I won&rsquo;t get into it in this blog post. I do however recommend <a href=https://arxiv.org/pdf/1911.00068.pdf>this research article</a> if you&rsquo;re interested.</p><p>I&rsquo;m confident enough to put this model into production. At Alan we like to keep things simple. Our website is a <a href=https://flask.palletsprojects.com/en/1.1.x/>Flask</a> app running on <a href=https://www.heroku.com/>Heroku</a>. We want to load this scikit-learn model in memory when the app boots up, and simply call it when a prescription has to be classified. If this pattern works for us, we might use it for other document classification tasks.</p><p>The model I trained weighs around 2.7 MB. That&rsquo;s over 8000 times <a href=https://www.bbc.com/future/article/20190704-apollo-in-50-numbers-the-technology>the amount of RAM</a> in the first Apollo shuttle that went to the Moon. Obviously I&rsquo;m saying this with my tongue in my cheek. And yet, I believe it&rsquo;s important to be frugal and not waste the memory of our precious <a href=https://www.heroku.com/dynos>Heroku dynos</a>. Especially if I want to convince the engineering team to use more machine learning models.</p><p>In a text classifier, the importance of each word follows a long-tail distribution. Most words are not important at all, which is the driving insight for <a href=https://www.wikiwand.com/en/Knowledge_distillation>knowledge distillation</a>. As we will see, this is very true for the above model. I decided to push the envelope and lighten the memory footprint of the model, while preserving its accuracy. Anyway, enough with the (long) introduction, here goes.</p><h2 id=measuring-the-models-size>Measuring the model&rsquo;s size</h2><p>From experience, I have a good idea of the model&rsquo;s memory footprint layout. However, it&rsquo;s always good to confirm this by measuring the memory footprint of each part of the model. I wrote a little utility to do this for any scikit-learn pipeline:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>humanize</span> <span class=kn>import</span> <span class=n>naturalsize</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pickle</span> <span class=kn>import</span> <span class=n>dumps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>obj_size</span><span class=p>(</span><span class=n>obj</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>naturalsize</span><span class=p>(</span><span class=n>sys</span><span class=o>.</span><span class=n>getsizeof</span><span class=p>(</span><span class=n>dumps</span><span class=p>(</span><span class=n>obj</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>estimator_size</span><span class=p>(</span><span class=n>model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>attr</span><span class=p>:</span> <span class=n>obj_size</span><span class=p>(</span><span class=nb>getattr</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>attr</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>attr</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=vm>__dict__</span> <span class=k>if</span> <span class=n>attr</span><span class=o>.</span><span class=n>endswith</span><span class=p>(</span><span class=s1>&#39;_&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>step</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>steps</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>attr</span><span class=p>,</span> <span class=n>size</span> <span class=ow>in</span> <span class=n>estimator_size</span><span class=p>(</span><span class=n>step</span><span class=p>)</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=se>\t</span><span class=si>{</span><span class=n>attr</span><span class=o>.</span><span class=n>ljust</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span><span class=si>}</span><span class=s1> </span><span class=si>{</span><span class=n>size</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Total&#39;</span><span class=o>.</span><span class=n>ljust</span><span class=p>(</span><span class=mi>20</span><span class=p>),</span> <span class=s1>&#39; &#39;</span> <span class=o>*</span> <span class=mi>7</span><span class=p>,</span> <span class=n>obj_size</span><span class=p>(</span><span class=n>model</span><span class=p>))</span>
</span></span></code></pre></div><pre tabindex=0><code>countvectorizer
	fixed_vocabulary_    37 Bytes
	stop_words_          38 Bytes
	vocabulary_          980.8 kB

normalizer
	n_features_in_       50 Bytes

logisticregression
	n_features_in_       50 Bytes
	classes_             219 Bytes
	n_iter_              184 Bytes
	coef_                1.8 MB
	intercept_           204 Bytes

Total                        2.7 MB
</code></pre><p>The above table shows the memory footprint of each attribute for each step of the pipeline. I&rsquo;m only looking at the attributes whose name ends with a <code>_</code> because that&rsquo;s the scikit-learn convention for marking attributes that have been created during the <code>fit</code> call. In other words, I&rsquo;m ignoring hyperparameters that are provided during initialization, because their memory footprint is insignificant.</p><p>As we can see, most of the memory footprint is taken up by the logistic regression <code>coef_</code> attribute. The latter is a matrix which stores the $n \times k$ weights for each of the $n$ words and the $k$ classes. The second culprit is the count vectorizer&rsquo;s <code>vocabulary_</code> attribute, which assigns an index to each word. This allows determining the weight vector of each word because <code>coef_</code> is integer-indexed instead of being word-indexed. We can now confidently move on and reduce the memory footprint of these two attributes.</p><h2 id=reducing-the-models-size>Reducing the model&rsquo;s size</h2><p>In a logistic regression, the importance of a feature is proportional to the magnitude of its weight vector. Therefore, unimportant features have a weight vector whose magnitude is close to 0. Intuitively, their contribution to the final sum is very small. In our case the features are the words in the text. By determining the unimportant words, we may reduce the model&rsquo;s memory by limiting the considered vocabulary.</p><p>First, let&rsquo;s measure the importance of each word. We can compute the feature-wise $L^2$ norm to measure the magnitude of each word&rsquo;s weight vector.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>log_reg</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>steps</span><span class=p>[</span><span class=mi>2</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=n>log_reg</span><span class=o>.</span><span class=n>coef_</span>
</span></span><span class=line><span class=cl><span class=n>magnitudes</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>W</span><span class=p>,</span> <span class=nb>ord</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></div><p>Now let&rsquo;s list the most important words by sorting their associated weight vector magnitudes.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>vectorizer</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>steps</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>idx_to_word</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>idx</span><span class=p>:</span> <span class=n>word</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>word</span><span class=p>,</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>vectorizer</span><span class=o>.</span><span class=n>vocabulary_</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>top</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>magnitudes</span><span class=p>)[::</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>meaningful_vocab</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>idx_to_word</span><span class=p>[</span><span class=n>k</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>top</span><span class=p>[:</span><span class=mi>100</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span></code></pre></div><p>Here are the 10 most important words:</p><pre tabindex=0><code>lentilles
monture
verres
lunettes
ophtalmologiste
50
oeil
ophtalmologie
25
gauche
</code></pre><p>Looks good to me! I&rsquo;m not too sure what the 25 and 50 are doing there, but I guess they&rsquo;re indicative of drug dosage (e.g. &ldquo;50 mg of paracetamol&rdquo;), and are thus distinctive indicators of pharmaceutical prescriptions.</p><p>We can prune the model now that we know which words matter. I found that the easiest way to proceed is to specify a list of words to the <code>CountVectorizer</code>. Indeed, the latter has a <code>vocabulary</code> parameter which overrides the lists of words that are used in the model. By default, all the words in the training set are used.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>pruned_model</span> <span class=o>=</span> <span class=n>make_pipeline</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>CountVectorizer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>vocabulary</span><span class=o>=</span><span class=n>meaningful_vocab</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>Normalizer</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>LogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pruned_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>],</span> <span class=n>train</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>])</span>
</span></span></code></pre></div><p>Let&rsquo;s first check the pruned model&rsquo;s performance.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>y_pred_pruned</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>pruned_model</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>test</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>columns</span><span class=o>=</span><span class=n>pruned_model</span><span class=o>.</span><span class=n>classes_</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>index</span><span class=o>=</span><span class=n>test</span><span class=o>.</span><span class=n>index</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>confident</span> <span class=o>=</span> <span class=n>y_pred_pruned</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;columns&#39;</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mf>.7</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>classification_report</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>test</span><span class=p>[</span><span class=n>confident</span><span class=p>][</span><span class=s1>&#39;type&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred_pruned</span><span class=p>[</span><span class=n>confident</span><span class=p>]</span><span class=o>.</span><span class=n>idxmax</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;columns&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>digits</span><span class=o>=</span><span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=p>))</span>
</span></span></code></pre></div><pre tabindex=0><code>                precision    recall  f1-score   support

contact_lenses      0.976     0.963     0.970      1068
       glasses      0.988     0.985     0.986      2170
      pharmacy      0.991     0.999     0.995      2324

      accuracy                          0.987      5562
     macro avg      0.985     0.982     0.984      5562
  weighted avg      0.987     0.987     0.987      5562
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Recall is </span><span class=si>{</span><span class=n>confident</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>confident</span><span class=p>)</span><span class=si>:</span><span class=s1>.2%</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>Recall is 93.42%
</code></pre><p>The performance is (slightly) better! The intention of this pruning process was not to improve the model, but it did. That&rsquo;s the magic of regularization for you. Indeed, the pruning process we&rsquo;ve applied boils down to <a href=https://www.wikiwand.com/en/Regularization_(mathematics)#/Regularizers_for_sparsity>sparsity regularisation</a>.</p><p>Let&rsquo;s now see the memory footprint of the pruned model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>step</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>steps</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>attr</span><span class=p>,</span> <span class=n>size</span> <span class=ow>in</span> <span class=n>estimator_size</span><span class=p>(</span><span class=n>step</span><span class=p>)</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=se>\t</span><span class=si>{</span><span class=n>attr</span><span class=o>.</span><span class=n>ljust</span><span class=p>(</span><span class=mi>20</span><span class=p>)</span><span class=si>}</span><span class=s1> </span><span class=si>{</span><span class=n>size</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Total&#39;</span><span class=o>.</span><span class=n>ljust</span><span class=p>(</span><span class=mi>20</span><span class=p>),</span> <span class=s1>&#39; &#39;</span> <span class=o>*</span> <span class=mi>7</span><span class=p>,</span> <span class=n>obj_size</span><span class=p>(</span><span class=n>model</span><span class=p>))</span>
</span></span></code></pre></div><pre tabindex=0><code>countvectorizer
	fixed_vocabulary_    37 Bytes
	vocabulary_          1.1 kB

normalizer
	n_features_in_       38 Bytes

logisticregression
	n_features_in_       38 Bytes
	classes_             219 Bytes
	n_iter_              184 Bytes
	coef_                2.6 kB
	intercept_           204 Bytes

Total                        5.0 kB
</code></pre><p>We&rsquo;ve reduced the model&rsquo;s memory consumption by a whopping factor of ~545. We&rsquo;ve done this without impacting the model&rsquo;s accuracy. It&rsquo;s a free win! Note that you could push the envelope even further by reducing the precision of the coefficient matrix to 32 bits instead of 64 bits, but the savings would be marginal.</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div class=footer><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script>
<script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto;margin-bottom:30px}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href=/blog/genetic-algorithms-introduction/>An introduction to genetic algorithms</a></li><li><a href=/blog/naive-bayes/>The NaÃ¯ve Bayes classifier</a></li><li><a href=/blog/machine-learning-production/>A smooth approach to putting machine learning into production</a></li></ul></div></div></div></article><script></script></body></html>