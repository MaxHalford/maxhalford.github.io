<!doctype html><html lang=en>
<head>
<script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.93.2">
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=author content="Max Halford">
<meta property="og:url" content="https://maxhalford.github.io/blog/phd-about/">
<link rel=canonical href=https://maxhalford.github.io/blog/phd-about/>
<link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>">
<script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"What my PhD was about","headline":"What my PhD was about","description":"I defended my PhD thesis on the 12th of October 2020, exactly 3 years and 11 days after having started it. The title of my PhD is Machine learning for query selectivity estimation in relational databases. I thought it would be worthwhile to summarise what I did. Note sure anyone will read this, but at least I\u0026rsquo;ll be able to remember what I did when I grow old and senile.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2021","datePublished":"2021-01-06 00:00:00 \u002b0000 UTC","dateModified":"2021-01-06 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/phd-about\/","keywords":[]}</script>
<title>What my PhD was about â€¢ Max Halford</title><meta property="og:title" content="What my PhD was about â€¢ Max Halford">
<meta property="og:type" content="article">
<meta name=description content="I defended my PhD thesis on the 12th of October 2020, exactly 3 years and 11 days after having started it. The title of my PhD is Machine learning for query selectivity estimation in relational databases. I thought it would be worthwhile to summarise what I did. Note sure anyone will read this, but at least I&rsquo;ll be able to remember what I did when I grow old and senile.">
<link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css>
<link rel=stylesheet href=/css/github-markdown.min.css>
<link rel=stylesheet href=/css/highlight/github.css>
<link rel=stylesheet href=/css/index.css>
<link rel=preconnect href=https://fonts.gstatic.com>
<link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head><body>
<article class=post id=article>
<div class="row center-xs" style=text-align:left>
<div class="col-xs-12 col-sm-10 col-md-7 col-lg-5">
<div class=post-header>
<header>
<div class="signatures site-title">
<a href=/>Max Halford ãƒ„</a>
</div></header><div class="row end-xs">
<div>
<a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a>
</div></div><div class=header-line></div></div><header class=post-header>
<h1 class=post-title>What my PhD was about</h1><div class="row post-desc">
<div class=col-xs-12>
<time class=post-date datetime="2021-01-06 00:00:00 UTC">
2021-01-06 Â· 4083 words
</time>
</div></div></header><div class="post-content markdown-body">
<h2 id=toc>Table of contents</h2><nav id=TableOfContents>
<ul>
<li><a href=#tldr>TLDR</a></li><li><a href=#more-seriously>More seriously</a></li><li><a href=#cost-modeling>Cost modeling</a></li><li><a href=#selectivity-estimation>Selectivity estimation</a></li><li><a href=#selectivity-estimation--density-estimation>Selectivity estimation â‰ˆ density estimation</a></li><li><a href=#conditional-independence-to-the-rescue>Conditional independence to the rescue</a></li><li><a href=#bayesian-networks>Bayesian networks</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav><p>I defended my PhD thesis on the 12th of October 2020, exactly 3 years and 11 days after having started it. The title of my PhD is <em>Machine learning for query selectivity estimation in relational databases</em>. I thought it would be worthwhile to summarise what I did. Note sure anyone will read this, but at least I&rsquo;ll be able to remember what I did when I grow old and senile.</p><h2 id=tldr>TLDR</h2><p>I contributed to making relational database queries go brrr.</p><h2 id=more-seriously>More seriously</h2><p><a href=https://www.wikiwand.com/en/Relational_database>Relational databases</a> are ubiquitous in most companies around the world. You might not know it, but there&rsquo;s always a high change that the app/website you&rsquo;re using is backed by a relational database. On the one hand, people use them store data. On the other hand, people use them to answer questions by querying the data. The focus of my PhD was on the latter.</p><p>In a relational database, data record are stored in tables. For instance, if we were recording item purchases in shops, then we might have something like so:</p><p>
<img src=/img/blog/phd-about/relation.svg width=90%>
</p><p>Typically, relations are <a href=https://www.wikiwand.com/en/Database_normalization>normalised</a>. Essentially, this means that the data isn&rsquo;t stored in a single relation. The data is instead scattered across multiple relations. References are used to associates rows with each other. In the above case, we can split the relation into three so-called &ldquo;dimension tables&rdquo; and one &ldquo;fact table&rdquo;.</p><p>
<img src=/img/blog/phd-about/relations.svg width=90%>
</p><p>Normalisation has a lot of benefits, one of them being that it avoids redundancies. However, as I will try to make evident throughout this post, analysing data that is scattered across relations is a pain in the neck. Indeed, the tools that statisticians have at their disposal are typically made to process data that belongs to a single relation/entity. Of course, the data from the subrelations, can be joined back together, but this is a costly operation that is avoided at all cost in certain situations.</p><p>As I mentionned, the focus of my PhD was on the querying aspect of databases. Users issue queries to the database via an interrogation language, which in 99% of cases is some flavor of <a href=https://www.wikiwand.com/fr/Structured_Query_Language>SQL</a>. For example, the following query can be used to count the number of meatball purchases that were made by blond Swedes in Ikea stores:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=k>COUNT</span><span class=p>(</span><span class=o>*</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>-- Relations
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>FROM</span><span class=w> </span><span class=n>customers</span><span class=p>,</span><span class=w> </span><span class=n>shops</span><span class=p>,</span><span class=w> </span><span class=n>items</span><span class=p>,</span><span class=w> </span><span class=n>purchases</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>-- Joins
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>WHERE</span><span class=w> </span><span class=n>purchases</span><span class=p>.</span><span class=n>customer</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>customers</span><span class=p>.</span><span class=n>id</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>AND</span><span class=w> </span><span class=n>purchases</span><span class=p>.</span><span class=n>shop</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>shops</span><span class=p>.</span><span class=n>id</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>AND</span><span class=w> </span><span class=n>purchases</span><span class=p>.</span><span class=n>item</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>items</span><span class=p>.</span><span class=n>id</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c1>-- Filters
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>AND</span><span class=w> </span><span class=n>customers</span><span class=p>.</span><span class=n>nationality</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;Swedish&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>AND</span><span class=w> </span><span class=n>customers</span><span class=p>.</span><span class=n>hair</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;Blond&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>AND</span><span class=w> </span><span class=n>items</span><span class=p>.</span><span class=n>name</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;Meatballs&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>AND</span><span class=w> </span><span class=n>shops</span><span class=p>.</span><span class=n>name</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>&#39;Ikea&#39;</span><span class=w>
</span></span></span></code></pre></div><p>Note that a lot of users also use <a href=https://www.wikiwand.com/en/Object%E2%90%93relational_mapping>ORM</a>s, which translate logic that is written in a given object-oriented programming language into SQL. Also note that by user I&rsquo;m not just referring to human brings that breath and go to the toilet; indeed a lot of database systems are typically interacted with via other computer systems that are controlled by more human-friendly interfaces. Anyway, what matters is that <strong>databases are queried via a declarative language, namely SQL</strong>. You tell the database <em>what</em> you want, but not <em>how</em> to retrieve it. The database will handle all the details for you and just spit our the result, which is wonderful when you think about it.</p><p>From an outside perspective, databases are simple: you query them and they produce a result. In fact, all the complexity of manipulating the data is delegated to the database. The database&rsquo;s goal is to take your SQL query and convert it into a sequence of steps that it will in order to answer the query. To be precise, the database is divided into different modules that handle different parts of this process. For instance, the <em>query compiler</em> is in charge of translating the SQL query into a sequence of instructions, whilst the <em>query executor</em> is responsible for running said instructions. Of course, all this has to happen as fast as possible in order to satisfy the user. Satisfation is typically defined via an <a href=https://www.wikiwand.com/en/Service-level_agreement>SLA</a> when the database is managed by a cloud provider.</p><p>The query compiler outputs a <em>query execution plan</em> (QEP for short). A QEP is a sequence of operations that have to be executed by the query executor in order to answer the associated SQL query. A QEP is a tree, where each node is an operator (such as a <code>JOIN</code> or a <code>WHERE</code>). Each leaf node represents a relation (such as <code>customers</code>, <code>shops</code>, etc.). The problem is that there a lot of QEPs that can answer a given query. In other words, there are many different ways to organise a sequence of operations in order to answer the same SQL query. The goal, as you might expect, is to find the fastest QEP. Indeed, the running times of the QEPs can (widely) vary. For instance, the three following QEPs all answer the same query, they just arrange the necessary operations in a different order:</p><p>
<img src=/img/blog/phd-about/plan_1.svg width=90%>
</p><p>
<img src=/img/blog/phd-about/plan_2.svg width=90%>
</p><p>
<img src=/img/blog/phd-about/plan_3.svg width=90%>
</p><p>What changed between each of the above QEPs is the order in which the relations are joined with each other. This might seem harmless to the unnitiated eye, but in practice it can make the different between making a query run in 1 second or 1 hour. To summarise:</p><ol>
<li>A user issues an SQL query to a database.</li><li>Each query can be answered with many different query execution plans.</li><li>The query optimiser searches for the fastest execution plan.</li><li>The best plan is executed, the results are returned to the user.</li></ol><p>The query optimiser searches for the best QEP by enumerating all the ones that are possible. The query optimiser is full of heuristics that it uses to discard candidates QEPs that it knows have no chance of being the best QEP. For instance, it only considers QEPs that &ldquo;push-down&rdquo; the <code>WHERE</code> as low as possible in the QEP. This process is called <a href=http://mlwiki.org/index.php/Logical_Query_Plan_Optimization>logical optimisation</a>. It&rsquo;s a low hanging fruit that is used in every serious query optimiser (for instance see bullet 4 of this <a href=https://aws.amazon.com/fr/blogs/big-data/amazon-redshift-federated-query-best-practices-and-performance-considerations/>piece of documention</a> from Amazon Redshift, as well as the <a href=https://arxiv.org/pdf/1802.10233.pdf>Apache Calcite paper</a>).</p><p>Once the query optimiser has enumerated a list of candidate QEPs, it has to pick the one which it thinks will perform best by determining the time they will take. The problem is that executing a QEP is the only way to determine the exact time said QEP will take. Alas, executing a QEP in order to determine its running time defeats the purpose of our endeavour. Indeed, we want to determine the best QEP <em>without</em> having to execute it. The only way to proceed is to guess the running time by analysing the structure of the QEP. This is called <strong>cost modeling</strong>.</p><h2 id=cost-modeling>Cost modeling</h2><p>The query optimiser delegates the task of determining the cost of a QEP to the cost model. In other words, the cost model is a submodule of the query optimisation module, as is shown in the following diagram which represents the lifetime of an SQL query.</p><p>
<img src=/img/blog/phd-about/query_lifetime.svg width=90%>
</p><p>The cost model takes as input a QEP and attempts to determine its cost. Note that cost is a loose term. Essentially, its just a number that in an ideal world is strongly correlated with the running time. It doesn&rsquo;t matter to get it exactly right, what matters is to have a reliable way to rank QEPs according to their cost.</p><p>Due to the fact that a QEP is a tree of operators, its cost is the sum of the costs of its operators. An operator refers to an SQL statement, such as a <code>WHERE</code>, a <code>JOIN</code>, or even a <code>GROUP BY</code>. These statements are said to be <em>logical</em>, because they describe <em>what</em> happens to the data. In a QEP, <em>physical</em> operators are instead used. Indeed, a physical operator implements a concrete algorithm that can be executed in order to fulfill the associated logical operator. The textbook example is the <code>JOIN</code> statement, which can be implemented with <a href=https://en.wikipedia.org/wiki/Category:Join_algorithms>different algorithms</a>:</p><ul>
<li>Block nested loop</li><li>Hash join</li><li>Nested loop join</li><li>Sort-merge join</li><li>Symmetric hash join</li></ul><p>Each algorithm has its pros and cons. Indeed, one algorithm might be faster than another, but it might also require more live memory. Therefore, in addition to picking the structure of the QEP, a query optimiser also has to decide which physical operators to use! In practice, a run-of-the-mill query with a half-dozen of relations can easily have thousands of candidate QEPs to consider.</p><p>Luckily, the cost of an operator can be determined in a straighforward fashion. Indeed, the cost of an algorithm is something that can be derived by looking at what it does and what of data structures it uses. For instance, the cost of a nested loop join is proportional to $m \times n$, where $m$ is the length of the left-hand side relation and $n$ is the length of the right-hand side relation. I say proportional because the true cost also has to incorporate the time it takes to process one row, the time it takes to read a page of data from the disk, the time it takes to transfer a chunk of data in case of a network transfer, etc. But these variables don&rsquo;t have too much impact; what matters most is that the cost is a function of $m$ and $n$.</p><p>This is where cost modeling breaks down. Although the <em>formula</em> for determining the cost of an operator is easy to determine, you still have to determine its input sizes. In the previous paragraph, the input sizes are $m$ and $n$. For instance, take a nested loop join between purchases and customers. Let&rsquo;s say that there $300,000$ purchases and $80,000$ customers. The estimated cost of the nested loop join is thus $300,000 \times 80,000 = 24$ billion. The latter is then some multiplied by some constants that determine how long it takes to process one row. What&rsquo;s important to understand is that it&rsquo;s easy to determine the cost of the join because the number of purchases and of customers are known. But what happens if, say, we&rsquo;re joining blond Swedish customers with meatball purchases?</p><p>First of all, we need to determine how many blond Swedes there are. We could do this by scanning the customers relation and count how many time we encounter people who are both blond and Swedish. This is the laziest approach and takes too much time in practice. Instead, we can collect figures that summarise the statistical distribution of the relation&rsquo;s attributes. More on this later.</p><p>Secondly, in our example, we need to determine how many purchases were made for meatballs. This requires joining the items and purchases relations, and then counting how rows have the <code>name</code> field set to <code>'Meatballs'</code>. Alas, we&rsquo;re now allowed to perform the join ourselves. Indeed, our goal is to determine the cost of the plan without being able to execute any part of it whatsoever! Hopefully you&rsquo;re started to get the hint: cost modeling is a hard problem.</p><p>The difficult part of cost modeling lies in determining the amount of rows that result from a filtering operation. For instance, we need to have some way of answering the question &ldquo;how many customers are of Swedish nationality and have blond hair?&rdquo; without scanning the data on-the-fly. We also need to be able to determine the amount of meatball purchases, even though the meatballs information is part of the items relation, not the purchases relation (damn you normalisation). The devil has many faces, and in this case has a name: selectivity estimation. It&rsquo;s essentially what I&rsquo;ve been banging my head against for the past three years.</p><h2 id=selectivity-estimation>Selectivity estimation</h2><p>Selectivity estimation is the most important part of cost modeling. If you produce a correct selectivity, then your cost model will produce a correct cost, which in turn allows the query optimiser the pick the plan with the lowest cost. If the selectivity estimate is incorrect, then getting a correct cost estimated is doomed to failure, and picking the best plan boils down to luck.</p><p>
<img src=/img/blog/phd-about/hierarchy.png width=90%>
</p><p>A lot of ink has been spilled over selectivity estimation. It all started with a seminal paper written by Patricia Selinger published in 1979: <a href=https://www2.cs.duke.edu/courses/compsci516/cps216/spring03/papers/selinger-etal-1979.pdf><em>Access path selection in a relational database management system</em></a>. The latter gives a formal description of what I&rsquo;ve rambling on about until now. The paper proposes a simple way to perform selectivity estimation: histograms. The idea being to precompute histograms for each attribute in the database. In the case of discrete attributes, histograms are just counters. In order to determine how many blond Swedish customers there are, one simply has to look at the counters for hair and nationality.</p><p>Wait, what about correlations? The problem of having one histogram per attribute is that you don&rsquo;t take into account dependencies between attributes. Indeed, if your hair histogram tells you that there are 20% of blond people, and your nationality histogram tells you that there are 30% of Swedish people, how many blond Swedes do you think there are? In her paper, Patricia Selinger proposes to simply multiply both percentages: $0.2 \times 0.3 = 0.06$. In other words, she proposes to <em>assume independency</em>. This is formally known as the <a href=http://www.vldb.org/conf/1997/P486.PDF>&ldquo;attribute value independence&rdquo; assumption</a>, or AVI for short. As you might know, Swedish people tend to be blond. Therefore, assuming independence is wrong. As it might turn out, all the Swedish people in our database might be blond! In that case, the percentage of blond Swedes is equal to the percentage of Swedes, which is 30%, and is well above the 6% estimate.</p><p>How might we solve this? We could potentially build a two-dimensional histogram over the hair and nationality attributes. Sure, why not. But what if you have hundreds of attributes? The problem is that a histogram over $d$ dimensions takes an amount of space that grows exponentially with $d$. Anything over three dimensions becomes too cumbersome. If, say, you have $25$ dimensions, then you have to build ${25 \choose 3} = 2300$ histograms. We could limit the histograms to attribute we known are dependent with each other, but that&rsquo;s difficult in itself. In the previous example, we have some prior assumption about the dependency of nationality on hair. In general, we would like to have some generic method that applies to dependencies that aren&rsquo;t necessarily grounded in human knowledge.</p><p>Virtually all database systems have gone down the road of assuming total independence. In practice, making the AVI assumption means that you only have to build $d$ one-dimensional histograms, which is much less cumbersome. The historical tendency has been to ignore dependencies and instead focus on building a lightweight cost model. As it turns out, query optimisers <a href=https://db.in.tum.de/~leis/papers/lookingglass.pdf>still get things right</a> most of the time. In fact, producing the correct selectivity estimation only matters in a very small number of cases. However, in those cases, getting the selectivity estimation right can make the difference between the query optimiser picking a slugish or a blazing fast QEP. In other words, the query optimiser might make a mistake once in a while, but that mistake might mean that the database is allocated of its resources to processing a long-running QEP, which will its overall performance. Cost modeling, and in particular selectivity estimation, is <a href=https://www.vldb.org/pvldb/vol9/p204-leis.pdf>still not a solved problem</a>. For instance, see this <a href=https://www.postgresql.org/message-id/D0F6E707-701C-40C4-9F4B-D7D282AA0187@cybertec.at>PostgreSQL mailing list thread</a>. As of 2020, a relatively high amount of research is still being poured into it.</p><h2 id=selectivity-estimation--density-estimation>Selectivity estimation â‰ˆ density estimation</h2><p>As you might have picked up by now, selectivity estimation looks uncannily like density estimation. Density estimation is a sub-field of statistics that deals with answering probabilistic queries, such as:</p><ul>
<li>How many customers?</li><li>How many Swedish customers?</li><li>How many purchases from Ikea?</li><li>How many purchases from Ikea for meatballs?</li><li>How many Swedish customers who bought meatballs from Ikea?</li></ul><p>The goal of density estimation is to produce a data structure that can answer all of the above queries in a timely and memory-efficient fashion. For instance, histograms can be seen as one of the simplest ways to perform density estimation. They have a low memory footprint, but they&rsquo;re not very accurate because they ignore dependencies between attributes.</p><p>A lot of density estimation have been proposed for the purpose of selectivity estimation over the years. A lot of histograms have been proposed, both for the one-dimensional cases (see <a href=https://dl.acm.org/doi/10.1145/169725.169708>this</a>, <a href=https://dl.acm.org/doi/10.1145/2505515.2505756>this</a>, and <a href=https://dl.acm.org/doi/10.1145/2723372.2749438>this</a>) as well as multi-dimensional cases (see <a href=https://www.cs.bu.edu/~gkollios/Papers/vldb090.pdf>this</a> and <a href=http://www.vldb.org/conf/1997/P486.PDF>this</a>). There have also been some more esoteric proposals, such as <a href=https://www.academia.edu/35234190/Copula_based_module_for_selectivity_estimation_of_multidimensional_range_queries>this one</a> which uses <a href=https://www.wikiwand.com/en/Copula_(probability_theory)>copulas</a>.</p><p>In recent years, due to the regain in interest in machine learning, a lot of sophisticated answers to the selectivity estimation problem have been proposed. These encompass classical machine learning (see <a href=http://cs.brown.edu/people/makdere/papers/qperf-tr.pdf>this</a> and <a href=https://dl.acm.org/doi/10.5555/2886444.2886453>this</a>), as well as deep learning (see <a href=https://arxiv.org/pdf/1809.00677.pdf>this</a>, <a href=http://www.vldb.org/pvldb/vol12/p1044-dutt.pdf>this</a>, and <a href=https://arxiv.org/pdf/2002.06442.pdf>this</a> which are all papers that were published in last 2 years). All of these proposals have a lot of good ideas, but in my opinion they won&rsquo;t see the light of day in practice. The cold truth is that database systems, and in particular query optimisers, require extremely efficient methods. Machine learning is cute, but I don&rsquo;t see PostgreSQL suddenly switching its cost model when <a href=https://www.postgresql.org/docs/8.3/row-estimation-examples.html>it&rsquo;s been using plain and simple histograms since its inception</a>. This is especially true for deep learning, which as we all know isn&rsquo;t exactly grounded in efficiency.</p><p>In other words, the industry imposes a hard limit on what we&rsquo;re allowed to do. I&rsquo;ve tried to summarise this in the following chart. Ideally, we want to be at the bottom-right. The problem is that we don&rsquo;t know about any selectivity estimation which makes the cut in terms of performance and can capture dependencies between multiple attributes across relations. Therefore, we have to make do with histograms, which are simple enough to be used in a high-performance cost model.</p><p>
<img src=/img/blog/phd-about/complexity_vs_accuracy.svg width=90%>
</p><p>This performance aspect is something that I&rsquo;ve always tried to keep in mind during my PhD. I spent the first 4 months powering through research papers in order to get a grasp of what had been proposed in the past. Surprisingly, I didn&rsquo;t find any of these methods being used in any existing database cost models. It didn&rsquo;t just seem to be because database systems are too old and haven&rsquo;t had the time to update themselves. Indeed, even new query optimisers such as that of <a href=https://prestosql.io/Presto_SQL_on_Everything.pdf>Facebook&rsquo;s Presto</a> and <a href=https://calcite.apache.org/>Apache Calcite</a> seem to be using histograms to estimate selectivities. To be quite frank, I believe that there&rsquo;s a disconnect between industry performance requirements and what researchers explore. Deep learning is trending, but what&rsquo;s point in researching it if it can&rsquo;t be used anyway? The North Star goal of my PhD was to keep things simple and usable in practice. I settled on using Bayesian networks because it <em>felt</em> to me that they were capable of ticking all the boxes.</p><h2 id=conditional-independence-to-the-rescue>Conditional independence to the rescue</h2><p>Let me recap with some light statistical notions to understand where Bayesian networks bring value:</p><ul>
<li>A relation is made of $p$ attributes ${X_1, \dots, X_p}$.</li><li>Each attribute $X_i$ follows an unknown distribution $P(X_i)$.</li><li>$P(X_i = x)$ gives us the probability of a predicate (e.g. <code>name = 'Ikea'</code>).</li><li>$P(X_i)$ can be estimated, for example with a histogram.</li><li>The distribution $P(X_i, X_j)$ captures interactions between $X_i$ and $X_j$ (e.g. <code>name = 'Ikea' AND nationality = 'Swedish'</code>.</li><li>Memorising $P(X_1, \dots, X_p)$ takes $\prod_0^p |X_i|$ units of space.</li></ul><p>The common practice in cost models is to assume independence between attributes. Let&rsquo;s assume ${X_1, \dots, X_p}$ are independent with each other:</p><ul>
<li>We thus have $P(X_1, \dots, X_p) = \prod_0^p P(X_i)$</li><li>Memorising $P(X_1, \dots, X_p)$ now takes $\sum_0^p |X_i|$ units of space</li></ul><p>We&rsquo;ve compromised between accuracy and space. This is the attribute value independence} (AVI) assumption. On the one hand, we can assume a full dependence situation and store big histograms, on the other hand we can assume total independence and store lightweight histograms. As it turns out, we can strike somewhere in the middle by using <a href=https://www.wikiwand.com/en/Conditional_independence>conditional independence</a>. The latter is based on Bayes&rsquo; theorem:</p><p>$$P(A, B) = P(B \mid A) \times P(A)$$</p><p>For instance:</p><p>$$P(hair, country) = P(hair \mid country) \times P(country)$$</p><p>In other words, we can say that hair color is conditioned on the nationality attribute. In case of three variables $A$, $B$, and $C$, $A$ are $B$ are conditionally independent if $C$ determines both of them. In which case:</p><p>$$P(A, B, C) = P(A \mid C) \times P(B \mid C) \times P(C)$$</p><p>Conditional independence can save space without compromising on accuracy. Indeed, we don&rsquo;t need $P(A, B, C)$ because it contains too much information. We can instead make use of the fact that we know which attributes are independent on each other and store smaller conditional distributions. Of course, this isn&rsquo;t obvious if you&rsquo;re not accustomed with this kind of stuff. As my advisors use to tell me: &ldquo;montre nous un exemple&rdquo;. Let&rsquo;s say you have the following relation:</p><table>
<thead>
<tr>
<th>nationality</th><th>hair</th><th>salary</th></tr></thead><tbody>
<tr>
<td>Swedish</td><td>Blond</td><td>42000</td></tr><tr>
<td>Swedish</td><td>Blond</td><td>38000</td></tr><tr>
<td>Swedish</td><td>Blond</td><td>43000</td></tr><tr>
<td>Swedish</td><td>Brown</td><td>37000</td></tr><tr>
<td>American</td><td>Brown</td><td>35000</td></tr><tr>
<td>American</td><td>Brown</td><td>32000</td></tr></tbody></table><p>Let&rsquo;s saying that we&rsquo;re looking build to a statistical summary of the attributes. We might want to use this statistical summary to look for the number of blond Swedes. The truth is:</p><p>$$P(\textcolor{royalblue}{Swedish}, \textcolor{goldenrod}{Blond}) = \frac{3}{6} = 0.5$$</p><p>By assuming total independence, we get:</p><p>$$P(\textcolor{royalblue}{Swedish}) = \frac{4}{6}$$
$$P(\textcolor{goldenrod}{Blond}) = \frac{3}{6}$$
$$P(\textcolor{royalblue}{Swedish}, \textcolor{goldenrod}{Blond}) = P(\textcolor{royalblue}{Swedish}) \times P(\textcolor{goldenrod}{Blond}) = 0.333$$</p><p>With conditional independence, we get:</p><p>$$P(\textcolor{goldenrod}{Blond} \mid \textcolor{royalblue}{Swedish}) = \frac{3}{4}$$</p><p>$$P(\textcolor{royalblue}{Swedish}, \textcolor{goldenrod}{Blond}) = P(\textcolor{goldenrod}{Blond} \mid \textcolor{royalblue}{Swedish}) \times P(\textcolor{royalblue}{Swedish}) = 0.5$$</p><p>Pretty cool, right? The $48,000 question is how do we make use of this for selectivity estimation? The answer is Bayesian networks.</p><h2 id=bayesian-networks>Bayesian networks</h2><p>A Bayesian network is a method for organising conditional independences. A Bayesian network is a special kind of graphical model that can be used for different purposes. Mainly though, Bayesian networks can be used to make sense of a tabular dataset by arranging conditional independences in a topological fashion.</p><p>
<img src=/img/blog/phd-about/bayesian_networks_hierarchy.svg width=90%>
</p><p>For example, the attributes from the previous relation can be arranged as so:</p><p>
<img src=/img/blog/phd-about/bn_example.png width=90%>
</p><p>Each node corresponds to an attribute. Each edge indicates a conditional dependency. Each node is associated with a conditional probability distribution (CPD for short). In the above diagram, each CPD is a table because the attributes are discrete (the salary attribute got binned). The Bayesian network as a whole encodes the full joint distribution as so:</p><p>$$P(N, H, S) = P(N) \times P(H \mid N) \times P(S \mid N)$$</p><p>The Bayesian network can then be queried to answer probabilistic questions, such as the amount of blond Swedes who have a salary above 40k. To do so, the full joint distribution could be reconstructed by multiplying the conditional distributions with each other. That could work, but it would blow up the memory usage. Instead, inference algorithms that traverse the network one node at a time can be used. This is why Bayesian networks are useful: they compress the full joint distribution in a space-efficient manner.</p><p>The running time of an inference algorithm depends on the Bayesian network&rsquo;s structure. The structure also determines the quality of the factorisation. Finding the right structure is of the utmost importance if one is to use Bayesian networks for selectivity estimation. In my work, I focused on <a href=https://www.wikiwand.com/en/Chow%E2%80%93Liu_tree>Chow-Liu trees</a>. This allowed me to use inference algorithms that run in linear time with respect to the number of attributes. The major contribution of my PhD was to propose a method to build Chow-Liu trees that cover multiple relations.</p><p>
<img src=/img/blog/phd-about/linked_bn.png width=90%>
</p><p>I don&rsquo;t want to go into any more detail because I would only be repeating what I&rsquo;ve written in my research papers. I made a big effort to write these in a clear manner that isn&rsquo;t overloaded with formulas and abstractions. The two papers I wrote on Bayesian networks for selectivity estimation are:</p><ul>
<li><a href=https://link.springer.com/chapter/10.1007/978-3-030-18579-4_1>An Approach Based on Bayesian Networks for Query Selectivity Estimation</a></li><li><a href=https://link.springer.com/chapter/10.1007/978-3-662-62386-2_6>Selectivity Estimation with Attribute Value Dependencies Using Linked Bayesian Networks</a></li></ul><p>The takeaway from my research is that Bayesian networks seem to be a viable method to improve the histograms used in query optimiser cost models that are used nowadays. Their accuracy is not as high as more sophisticated methods, including deep learning. However, they offer a better tradeoff with respect to performance, and therefore are a more likely candidate.</p><h2 id=conclusion>Conclusion</h2><p>All in all my PhD was a worthwhile experience. Of course, there were ups and downs. I wasn&rsquo;t 100% happy with the research environment I was in, but that&rsquo;s another story. The PhD gave me the time to hone my skills and become a more knowledgeable data scientist. I&rsquo;m not at all convinced that my work will be used in an industrial grade query optimiser, but I&rsquo;m certain that I pursued the right research direction. Now I&rsquo;m working at <a href=https://alan.com/>Alan</a>, with the intent of developing more business acumen, which in my opinion is a key aspect of becoming a great data scientist.</p><p>If I had to pick one highlight during my PhD, it would be that I got to go to Thailand for a conference!</p><p>
<img src=/img/blog/phd-about/thailand_1.jpg width=90%>
</p><p>
<img src=/img/blog/phd-about/thailand_2.jpg width=90%>
</p><p>
<img src=/img/blog/phd-about/thailand_3.jpg width=90%>
</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script>
<div class=footer>
<div class=do-the-thing>
<div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>
Back to the top
</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.0/elevator.min.js></script>
<script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script>
<style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=site-footer>
<div class=site-footer-item>
<a href=/index.xml><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M12.8 16C12.8 8.978 7.022 3.2.0 3.2V0c8.777.0 16 7.223 16 16h-3.2zM2.194 11.61c1.21.0 2.195.985 2.195 2.196.0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017.0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818.0 10.606 4.79 10.606 10.607z"/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=https://github.com/MaxHalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M8 0C3.58.0.0 3.582.0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385.0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953.0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117.0.0.67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147.0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48.0 1.07-.01 1.93-.01 2.19.0.21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=https://linkedin.com/in/maxhalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235.0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4.0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762.0-1.376-.617-1.376-1.377.0-.758.614-1.375 1.376-1.375.76.0 1.376.617 1.376 1.375.0.76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816.0H1.18C.528.0.0.516.0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652.0 1.185-.516 1.185-1.153V1.153C16 .516 15.467.0 14.815.0z" fill-rule="nonzero"/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=https://twitter.com/halford_max><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812.0-3.282 1.47-3.282 3.28.0.26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21.0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26.0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04.0 9.34-5 9.34-9.33.0-.14.0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=https://kaggle.com/maxhalford><span class=inline-svg><svg role="img" viewBox="0 0 26 26" xmlns="http://www.w3.org/2000/svg"><title>Kaggle icon</title><path fill="currentcolor" d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187.0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236.0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234.0.351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144.0.236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.358"/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=/files/resume_max_halford.pdf><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 392.533 392.533" style="enable-background:new 0 0 392.533 392.533"><g><g><path fill="currentcolor" d="M292.396 324.849H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 329.697 298.473 324.849 292.396 324.849z"/></g></g><g><g><path fill="currentcolor" d="M292.396 277.01H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 281.859 298.473 277.01 292.396 277.01z"/></g></g><g><g><path fill="currentcolor" d="M196.137 45.834c-25.859.0-46.998 21.075-46.998 46.998.0 25.859 21.139 46.933 46.998 46.933s46.998-21.075 46.998-46.998-21.139-46.933-46.998-46.933zm0 72.017c-13.77.0-25.083-11.313-25.083-25.083s11.248-25.083 25.083-25.083 25.083 11.313 25.083 25.083c0 13.769-11.313 25.083-25.083 25.083z"/></g></g><g><g><path fill="currentcolor" d="M258.521 163.362c-39.887-15.515-84.752-15.515-124.638.0-13.059 5.107-21.786 18.101-21.786 32.388v44.347c-.065 6.012 4.849 10.925 10.861 10.925h146.424c6.012.0 10.925-4.848 10.925-10.925V195.75C280.307 181.463 271.58 168.469 258.521 163.362zm0 65.874H133.883v-33.422c0-5.301 3.168-10.214 7.887-12.024 34.844-13.511 74.02-13.511 108.865.0 4.719 1.875 7.887 6.659 7.887 12.024v33.422z"/></g></g><g><g><path fill="currentcolor" d="M313.083.0H131.491c-8.404.0-16.291 3.232-22.238 9.18L57.018 61.414c-5.947 5.948-9.18 13.834-9.18 22.238v277.333c0 17.39 14.158 31.547 31.547 31.547h233.762c17.39.0 31.547-14.158 31.547-31.547V31.547C344.501 14.158 330.343.0 313.083.0zM112.032 37.236v27.022H85.01l27.022-27.022zm210.683 79.58h-40.598c-6.012.0-10.925 4.849-10.925 10.925.0 6.012 4.848 10.925 10.925 10.925h40.598v19.394h-14.869c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h14.869v181.139c0 5.366-4.331 9.697-9.632 9.697H79.192c-5.301.0-9.632-4.331-9.632-9.632V86.044h53.398c6.012.0 10.925-4.848 10.925-10.925V21.721h179.2c5.301.0 9.632 4.331 9.632 9.632v85.463z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href="https://scholar.google.com/citations?user=erRNNi0AAAAJ&hl=en"><span class=inline-svg><svg viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952.0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584.0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288.0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962.0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zm762.098 382.384c-64.358 64.424-141.86 96.57-232.572 96.57H329.144c-90.712.0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572V182.859c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712.0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159V762.033c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162.0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53.0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476.0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86.0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908.0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426.0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432.0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234.0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048.0 6.642.19 12.492.672 18.974H292.574l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382.0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994.0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376.0 103.05 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382.0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.05 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z"/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=https://www.imdb.com/user/ur73044771><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><g><g><path fill="currentcolor" d="M425.17 73.146c.179-1.577.268-3.169.268-4.771.0-23.25-18.915-42.165-42.165-42.165-6.849.0-13.444 1.619-19.354 4.678-9.531-14.757-26.054-24.056-44.185-24.056-11.414.0-22.244 3.62-31.163 10.214C280.754 6.589 268.271.0 254.743.0c-13.007.0-25.097 6.068-32.975 15.906-8.628-5.856-18.899-9.075-29.515-9.075-18.126.0-34.646 9.302-44.176 24.06-5.913-3.06-12.508-4.682-19.351-4.682-23.25.0-42.166 18.915-42.166 42.165.0 1.603.088 3.195.266 4.769-21.658 6.642-36.909 26.605-36.909 50.229.0 11.817 3.952 23.172 11.184 32.401l43.004 347.699c.603 4.871 4.74 8.528 9.648 8.528h284.485c4.907.0 9.046-3.658 9.648-8.528l43.004-347.7c7.238-9.225 11.194-20.578 11.194-32.4C462.083 99.751 446.832 79.789 425.17 73.146zM122.346 492.557 81.465 162.025h44.764l26.618 330.532H122.346zm50.007.0-26.618-330.532H201.2l3.161 104.598c-17.168 14.634-28.086 36.393-28.086 60.667.0 26.007 12.521 49.142 31.849 63.703l3.065 101.563H172.353zm108.993.0h-50.704l-2.736-90.671c8.743 3.303 18.205 5.125 28.09 5.125 9.887.0 19.352-1.823 28.096-5.128L281.346 492.557zm-25.35-104.989c-33.237.0-60.277-27.04-60.277-60.277s27.04-60.277 60.277-60.277 60.277 27.04 60.277 60.277-27.04 60.277-60.277 60.277zM220.653 162.025h70.696l-2.797 92.523c-9.95-4.469-20.962-6.977-32.555-6.977-11.591.0-22.602 2.507-32.548 6.975L220.653 162.025zm80.144 330.532 3.076-101.568c19.325-14.562 31.844-37.694 31.844-63.698.0-24.27-10.915-46.027-28.078-60.661l3.16-104.605h55.457l-26.618 330.532H300.797zm88.847.0h-30.501l26.618-330.532h44.766L389.644 492.557zm46.819-349.975H75.531c-3.986-5.588-6.171-12.266-6.171-19.21.0-15.23 10.052-28.041 24.192-31.923 5.136 7.347 14.332 15.089 27.853 15.089 1.412.0 2.87-.084 4.376-.262 5.086-.601 9.169-4.831 9.023-9.951-.169-5.868-5.336-10.151-11.005-9.396-10.908 1.458-15.263-7.828-16.038-9.747-.013-.032-.029-.062-.042-.095-.012-.027-.017-.057-.029-.084-.779-1.9-1.29-3.885-1.53-5.925-1.495-12.753 8.151-24.497 20.961-25.373 6.62-.452 12.93 1.908 17.606 6.533 1.819 1.799 4.249 2.929 6.806 2.967 4.322.064 8.035-2.652 9.379-6.564 4.598-13.379 17.192-22.369 31.338-22.369 9.432.0 18.433 4.035 24.706 11.075 1.729 1.94 4.172 3.177 6.769 3.314 4.373.231 8.223-2.412 9.675-6.342 3.285-8.897 11.862-14.876 21.341-14.876 9.948.0 18.848 6.611 21.743 16.113.844 2.769 2.688 5.187 5.322 6.388 4.237 1.929 9.045.642 11.766-2.852 6.344-8.145 15.88-12.817 26.163-12.817 12.52.0 23.811 7.04 29.437 17.921-1.79 2.84-3.768 6.681-5.037 11.285-3.664 13.302.342 26.651 11.28 37.59 1.898 1.899 4.386 2.848 6.874 2.848 2.445.0 4.889-.916 6.775-2.749 3.908-3.798 3.579-10.276-.227-14.178-5.623-5.764-7.618-11.593-6.097-17.81.165-.673.362-1.321.582-1.939 1.859-5.235 5.847-9.536 10.945-11.741 3.526-1.524 7.439-2.141 11.453-1.724 11.438 1.189 20.292 11.127 20.277 22.625-.004 2.949-.569 5.829-1.682 8.559-.872 2.139-.985 4.557-.26 6.751 1.247 3.773 4.581 6.325 8.393 6.66 16.932 1.484 30.195 15.978 30.195 33C442.642 130.32 440.454 136.998 436.463 142.582z"/></g></g><g><g><path fill="currentcolor" d="M261.257 62.341c-7.484.0-14.638 1.996-20.875 5.741-6.452-5.745-14.886-9.073-23.702-9.073-19.656.0-35.646 15.99-35.646 35.646s15.989 35.646 35.644 35.646c5.369.0 9.721-4.353 9.721-9.721.0-5.369-4.353-9.721-9.721-9.721-8.935.0-16.203-7.268-16.203-16.203s7.268-16.203 16.203-16.203c5.773.0 10.987 2.99 13.945 7.999 1.542 2.612 4.217 4.354 7.229 4.71 3.015.358 6.02-.714 8.128-2.893 4.047-4.182 9.473-6.484 15.277-6.484 11.725.0 21.264 9.539 21.264 21.266.0 5.369 4.351 9.722 9.721 9.722s9.721-4.353 9.721-9.722C301.965 80.603 283.704 62.341 261.257 62.341z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=https://play.spotify.com/user/1166811350><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 168 168"><path fill="currentcolor" d="m83.996.277C37.747.277.253 37.77.253 84.019c0 46.251 37.494 83.741 83.743 83.741 46.254.0 83.744-37.49 83.744-83.741.0-46.246-37.49-83.738-83.745-83.738l.001-.004zm38.404 120.78c-1.5 2.46-4.72 3.24-7.18 1.73-19.662-12.01-44.414-14.73-73.564-8.07-2.809.64-5.609-1.12-6.249-3.93-.643-2.81 1.11-5.61 3.926-6.25 31.9-7.291 59.263-4.15 81.337 9.34 2.46 1.51 3.24 4.72 1.73 7.18zm10.25-22.805c-1.89 3.075-5.91 4.045-8.98 2.155-22.51-13.839-56.823-17.846-83.448-9.764-3.453 1.043-7.1-.903-8.148-4.35-1.04-3.453.907-7.093 4.354-8.143 30.413-9.228 68.222-4.758 94.072 11.127 3.07 1.89 4.04 5.91 2.15 8.976v-.001zm.88-23.744c-26.99-16.031-71.52-17.505-97.289-9.684-4.138 1.255-8.514-1.081-9.768-5.219-1.254-4.14 1.08-8.513 5.221-9.771 29.581-8.98 78.756-7.245 109.83 11.202 3.73 2.209 4.95 7.016 2.74 10.733-2.2 3.722-7.02 4.949-10.73 2.739z"/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=https://www.goodreads.com/user/show/67553795-lemax><span class=inline-svg><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 463 463" style="enable-background:new 0 0 463 463"><g><path fill="currentcolor" d="M270.615 229.128l-24-72c-1.021-3.063-3.887-5.128-7.115-5.128s-6.094 2.066-7.115 5.128l-24 72c-.513 1.54-.513 3.204.0 4.743l24 72c1.021 3.063 3.887 5.128 7.115 5.128s6.094-2.066 7.115-5.128l24-72C271.128 232.332 271.128 230.668 270.615 229.128zM239.5 279.783 223.406 231.5l16.094-48.283 16.094 48.283L239.5 279.783z"/><path fill="currentcolor" d="M375.5 48h-64c-2.997.0-5.862.57-8.5 1.597V23.5C303 10.542 292.458.0 279.5.0h-80C186.542.0 176 10.542 176 23.5v42.097C173.362 64.57 170.497 64 167.5 64h-80C74.542 64 64 74.542 64 87.5v352c0 12.958 10.542 23.5 23.5 23.5h80c6.177.0 11.801-2.399 16-6.31 4.199 3.911 9.823 6.31 16 6.31h80c6.177.0 11.801-2.399 16-6.31 4.199 3.911 9.823 6.31 16 6.31h64c12.958.0 23.5-10.542 23.5-23.5v-368C399 58.542 388.458 48 375.5 48zM79 135h97v257H79V135zM191 87.5V87h97v289h-97V87.5zm97-16V72h-97V55h97V71.5zM191 391h97v17h-97V391zM303 119h81v273h-81V119zm8.5-56h64c4.687.0 8.5 3.813 8.5 8.5V104h-81V71.5C303 66.813 306.813 63 311.5 63zm-112-48h80c4.687.0 8.5 3.813 8.5 8.5V40h-97V23.5C191 18.813 194.813 15 199.5 15zM87.5 79h80c4.687.0 8.5 3.813 8.5 8.5V120H79V87.5c0-4.687 3.813-8.5 8.5-8.5zm80 369h-80c-4.687.0-8.5-3.813-8.5-8.5V407h97v32.5C176 444.187 172.187 448 167.5 448zm112 0h-80c-4.687.0-8.5-3.813-8.5-8.5V423h97v16.5C288 444.187 284.187 448 279.5 448zm96 0h-64c-4.687.0-8.5-3.813-8.5-8.5V407h81v32.5C384 444.187 380.187 448 375.5 448z"/><path fill="currentcolor" d="M374.615 253.128l-24-72c-1.021-3.063-3.887-5.128-7.115-5.128s-6.094 2.066-7.115 5.128l-24 72c-.513 1.54-.513 3.204.0 4.743l24 72c1.021 3.063 3.887 5.128 7.115 5.128s6.094-2.066 7.115-5.128l24-72C375.128 256.332 375.128 254.668 374.615 253.128zM343.5 303.783 327.406 255.5l16.094-48.283 16.094 48.283L343.5 303.783z"/><path fill="currentcolor" d="M158.615 261.128l-24-72c-1.021-3.063-3.887-5.128-7.115-5.128s-6.094 2.066-7.115 5.128l-24 72c-.513 1.54-.513 3.204.0 4.743l24 72c1.021 3.063 3.887 5.128 7.115 5.128s6.094-2.066 7.115-5.128l24-72C159.128 264.332 159.128 262.668 158.615 261.128zM127.5 311.783 111.406 263.5l16.094-48.283 16.094 48.283L127.5 311.783z"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>
</span>
</a>
</div><div class=site-footer-item>
<a href=mailto:maxhalford25@gmail.com><span class=inline-svg><svg viewBox="0 0 15 20" xmlns="http://www.w3.org/2000/svg"><title>mail</title><path fill="currentcolor" d="M0 4v8c0 .55.45 1 1 1h12c.55.0 1-.45 1-1V4c0-.55-.45-1-1-1H1c-.55.0-1 .45-1 1zm13 0L7 9 1 4h12zM1 5.5l4 3-4 3v-6zM2 12l3.5-3L7 10.5 8.5 9l3.5 3H2zm11-.5-4-3 4-3v6z" fill="#000" fill-rule="evenodd"/></svg>
</span>
</a>
</div></div><div style=margin-bottom:50px;display:flex;justify-content:center>
<iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=35 width=116 style=border:0></iframe>
</div></div></div></article><script></script>
</body></html>