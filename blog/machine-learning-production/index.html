<!doctype html><html lang=en><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-63302552-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-63302552-1')</script><script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script><meta charset=utf-8><meta name=generator content="Hugo 0.82.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/machine-learning-production/"><link rel=canonical href=https://maxhalford.github.io/blog/machine-learning-production/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"A smooth approach to putting machine learning into production","headline":"A smooth approach to putting machine learning into production","description":"Putting machine learning into production is hard. Usually I\u0026rsquo;m doubtful of such statements, but in this case I\u0026rsquo;ve never met anyone for whom everything has gone smoothly. Most data scientists might agree that there is a huge gap between their local environment and a live environment. In fact, \u0026ldquo;productionalizing\u0026rdquo; machine learning is such a complex topic that entire companies have risen to address the issue. I\u0026rsquo;m not just talking about running a gigantic grid search and finding the best model, I\u0026rsquo;m talking about putting a machine learning model live so that it actually has a positive impact on your business\/project.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2019","datePublished":"2019-07-13 00:00:00 \u002b0000 UTC","dateModified":"2019-07-13 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/machine-learning-production\/","keywords":[]}</script><title>A smooth approach to putting machine learning into production - Max Halford</title><meta property="og:title" content="A smooth approach to putting machine learning into production - Max Halford"><meta property="og:type" content="article"><meta name=description content="Putting machine learning into production is hard. Usually I&rsquo;m doubtful of such statements, but in this case I&rsquo;ve never met anyone for whom everything has gone smoothly. Most data scientists might agree that there is a huge gap between their local environment and a live environment. In fact, &ldquo;productionalizing&rdquo; machine learning is such a complex topic that entire companies have risen to address the issue. I&rsquo;m not just talking about running a gigantic grid search and finding the best model, I&rsquo;m talking about putting a machine learning model live so that it actually has a positive impact on your business/project."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link href="https://fonts.googleapis.com/css?family=PT+Serif|PT+Sans|Permanent+Marker" rel=stylesheet><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0,tags:'ams'},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll('mjx-container').forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=post-header><header><div class="signatures site-title"><a href=/>Max Halford</a></div></header><div class="row end-xs"><div><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></div><div class=header-line></div></div><header class=post-header><h1 class=post-title>A smooth approach to putting machine learning into production</h1><div class="row post-desc"><div class=col-xs-12><time class=post-date datetime="2019-07-13 00:00:00 UTC">2019-07-13 - 14 minutes read</time></div></div></header><div class="post-content markdown-body"><p>Putting machine learning into production is hard. Usually I&rsquo;m doubtful of such statements, but in this case I&rsquo;ve never met anyone for whom everything has gone smoothly. Most data scientists might agree that there is a huge gap between their local environment and a live environment. In fact, &ldquo;productionalizing&rdquo; machine learning is such a complex topic that entire companies have risen to address the issue. I&rsquo;m not just talking about running a gigantic grid search and finding the best model, I&rsquo;m talking about putting a machine learning model live so that it actually has a positive impact on your business/project. Off the top of my head: <a href=https://www.cubonacci.com/>Cubonacci</a>, <a href=https://www.h2o.ai/>H2O</a>, <a href=https://cloud.google.com/automl/>Google AutoML</a>, <a href=https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html>Amazon Sagemaker</a>, and <a href=https://www.datarobot.com/>DataRobot</a>. In other words people are making money off businesses because data scientists and engineers are having a hard putting their models into production. In my opinion if a data scientist can&rsquo;t put her model into production herself then something is wrong. Life should be simpler.</p><p>It&rsquo;s mid-2019 and we have all these cool techniques and frameworks for learning from data. On Kaggle, the de-facto standard is to use <a href=https://github.com/microsoft/LightGBM>LightGBM</a> for &ldquo;regular&rdquo; competitions. As far as I understood, the top 20 competitors in the <a href=https://www.kaggle.com/c/gendered-pronoun-resolution>latest NLP competition</a> all used <a href=https://github.com/google-research/bert>BERT</a>. My point is that as a community we&rsquo;re getting very good at building and training extremely strong models. Not everyone is an expert, but my impression is that the average skill and knowledge of data scientists is growing at a steady pace. Differently said, we are converging towards standard practices and common patterns that are transforming machine learning into a robust technology rather than an inexact science. A good example is Martin Zinkevich&rsquo;s <a href=https://developers.google.com/machine-learning/guides/rules-of-ml/>&ldquo;Rules of Machine Learning&rdquo;</a> (Martin is a research scientist at Google and has a lot of interesting things to say).</p><p>Although we are very good at building strong model, things are not so clear when it comes to putting said models into production. There might be packages for doing cutting-edge deep learning and stacking models, but you&rsquo;re sort of left on your own if you want to put your model into production. I&rsquo;ve been lucky enough to have met a fair amount of data science team over the past few years, and my impression is that each team has it&rsquo;s own custom way of going from a local environment to a live environment. Something feels wrong.</p><p><img src=/img/blog/machine-learning-production/standards.png alt=standards></p><p>Although each data science project I&rsquo;ve encountered had it&rsquo;s own unique way to go into production, generally the goal was always the same. Indeed the general idea is to extract features from a dataset, train a model, and then put the model behind an API. For making a prediction for a new instance, features are generated in real-time before being fed to the model. Every once in a while a new model is trained and replaces the current one. It&rsquo;s pretty straightforward when you think about it, and is exactly what are doing all the services I mentioned above. Some people call this pattern the <a href=https://www.wikiwand.com/en/Lambda_architecture>lambda architecture</a>. However, down the road you might realize that is has a lot of kinks and things are no so smooth.</p><p><img src=/img/blog/machine-learning-production/lambda.png alt=lambda></p><p>Here are a few pain points that often arise:</p><ol><li>The rate at which to retrain the model has to be decided upon. Should it be every day? How about every week? If it&rsquo;s every week, then should it take place every Sunday at 10PM or every Tuesday at 3AM?</li><li>You have to wait until a new model is trained in order to exploit new data. In other words, your model isn&rsquo;t able to update itself every time a new observation arrives.</li><li>Training models is going to require more and more computing power because you will have collected more and more data. You could argue that using only the data from, say, the past week is fine, but that&rsquo;s also something you have to choose.</li><li>The features you used to train your model might be difficult to generate in real-time. For example, if you&rsquo;re using aggregate features then this means you need some way of storing them and retrieving them when needed. DataRobot and H2O handle some of this for you, but it is by no means a trivial task. Indeed the features are usually expected to be generated by the user beforehand.</li></ol><p>I believe that these pain points are the reasons why a lot of machine learning projects never see the light of day. Again, putting a machine learning model is hard, which is part of the reason why good data scientists and engineers are being payed so much. The main reason why all these problems arise is because we tend to think through the lens of so-called batch learning. Indeed, we then to think of a dataset as a rectangular dataframe composed of features <code>X</code> and targets <code>y</code>. Typically, we train a model on a training set <code>(X_train, y_train)</code> and predict the outcomes of a test set <code>X_test</code>. The issue is that in real life the situation is different. In a lot of applications, data points arrive one by one. In other words, the data is a stream and not a dataframe. On the one hand we have data arriving from a stream, and on the other hand we have static models that are meant for a batch setting. The only reason why we have to go through the pain of retraining every so often is because our models aren&rsquo;t meant to handle streaming data in the first place. In my opinion, this is classic case of the hammer/nail syndrome. In this case, the hammer is the set of libraries and techniques we have in our toolbox (scikit-learn, Keras, XGBoost, etc.) whilst the nails are the problems we have to solve in real life.</p><p><img src=/img/blog/machine-learning-production/hammer_nail.jpg alt=hammer_nail></p><p>Thankfully, batch learning isn&rsquo;t the only paradigm that exists. There exists another subset of machine learning called <a href=https://www.wikiwand.com/en/Online_machine_learning>online machine learning</a>. In online machine learning, the idea is that your model is only allowed to learn from one example at a time. This is contrast to batch learning where all the available data is processed at once. Batch learning is by far the most popular of the the two regimes. I personally believe that for certain applications, online learning is a much better choice. Actually, I believe that in some cases using batch learning is wrong. However, my experience is that a big share of data scientists (especially the junior ones) are not even aware than online machine learning exists. I&rsquo;ll try to give you a motivating example.</p><p>Suppose you&rsquo;re building a phone application to handle bike sharing data. Your goal would be to help users to visualize and find suitable bike stations. Sometimes the users will want to take a bike, and sometimes they will want to put one back. Apparently Apple has been including this in Apple Maps for some time, a fact of which I wasn&rsquo;t aware of until writing this blog post!</p><p><img src=/img/blog/machine-learning-production/apple.jpg alt=apple></p><p>When a user looks at the map, they might want to find the closest bike station with at least one available bike. Likewise they might want to find one with an available space close to the destination they are heading towards. Currently, most apps tell you how many bikes and spaces are currently available. It would be nice if the app could show you an estimate of the amount of available bikes and spaces at the time you are expected to arrive. That is, how many bikes will be available in station $s$ in $m$ minutes? From a machine learning perspective, this is clearly a regression problem. Bike sharing data is <a href=https://github.com/topics/bike-sharing>freely available</a>, meaning that anyone can have a go at this. In fact, I did exactly something like that three years ago and received a prize of 7000 euros.</p><div align=center><blockquote class=twitter-tweet><p lang=fr dir=ltr>Bravo Ã  <a href="https://twitter.com/OpenBikes_?ref_src=twsrc%5Etfw">@OpenBikes_</a> , prix de la mÃ©tropole au concours <a href="https://twitter.com/hashtag/dataconnex?src=hash&ref_src=twsrc%5Etfw">#dataconnex</a> ions : suivez Max Halford, ca promet ;) <a href=https://t.co/xa9BV9cvUT>pic.twitter.com/xa9BV9cvUT</a></p>&mdash; GaÃ«lle Copienne (@gcopienne) <a href="https://twitter.com/gcopienne/status/694829938051039233?ref_src=twsrc%5Etfw">February 3, 2016</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></div><p>The reason why I&rsquo;m mentioning myself isn&rsquo;t to brag, but instead to explain what I was doing wrong and what I would have done if I did it now. When I built the app, I was but a naive undergraduate and was playing a lot around with <a href=https://scikit-learn.org/stable/>scikit-learn</a>. My idea was to poll bike sharing APIs from all over the world and store the amount of bikes and space of each given bike station. I polled around 25 APIs every minute or so. Then, I spent some time building a model in my nice and idyllic local environment. For simplicity I built a single global model which turned out to use approximately 20 features and a random forest. Users could use a web page and ask how many bikes or spaces would be available in a given amount of time. This basically required loading the model in memory from the disk, building the necessary features (including the weather), and make a forecast. Arbitrarily, I decided to retrain the model every Tuesday night. I had a hammer, and everything looked like a nail.</p><p>My approach worked reasonably well, but it wasn&rsquo;t a smooth process to say the least. I was roughly collecting 2,000,000 events per week, which obviously imposed an increasing amount of stress during the training process. In fact, after a few months, maintaining the app on my own burned me out. It just wasn&rsquo;t fun, especially considering I hadn&rsquo;t planned on making any money from it, and so I took it down. The big problem for me was that putting my model into production felt really hackish, and I hate to make a lot of decisions myself. I couldn&rsquo;t any find any blog post or framework which felt like they suited my needs. The biggest problem I had was generating features in real time before making a prediction. Every time a user queried my service, I had to make a prediction for a given station and a future timestamp. I was using features such as average values per hour and day, along with exponentially weighted averages of recent values. The trick is that I had to handle the logic of building these features outside of my model, mostly because things had to be handled differently in batch and in streaming mode. Nowadays I&rsquo;m aware that services such as <a href=https://cloud.google.com/dataflow/>Google Cloud Dataflow</a> and libraries like <a href=https://beam.apache.org/>Apache Beam</a> handle this for you, but I find them to be overly complex solutions. The crux of the problem is that I wanted to things to be simple and straightforward, not clunky and awkward.</p><p>The trick is that with online machine learning, everything is simplified. To exemplify what I mean, I&rsquo;m going to use a new library I&rsquo;ve been working on called <a href=https://github.com/creme-ml/creme><code>creme</code></a>. In <code>creme</code>, each model has a <code>fit_one</code> method. Transformers have a <code>transform_one</code> method, whilst classifiers and regressors have a <code>predict_one</code> method. Here is how you would define a model in <code>creme</code>:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>datetime</span> <span class=kn>as</span> <span class=nn>dt</span>
<span class=kn>from</span> <span class=nn>creme</span> <span class=kn>import</span> <span class=n>compose</span>
<span class=kn>from</span> <span class=nn>creme</span> <span class=kn>import</span> <span class=n>datasets</span>
<span class=kn>from</span> <span class=nn>creme</span> <span class=kn>import</span> <span class=n>feature_extraction</span>
<span class=kn>from</span> <span class=nn>creme</span> <span class=kn>import</span> <span class=n>linear_model</span>
<span class=kn>from</span> <span class=nn>creme</span> <span class=kn>import</span> <span class=n>metrics</span>
<span class=kn>from</span> <span class=nn>creme</span> <span class=kn>import</span> <span class=n>model_selection</span>
<span class=kn>from</span> <span class=nn>creme</span> <span class=kn>import</span> <span class=n>preprocessing</span>
<span class=kn>from</span> <span class=nn>creme</span> <span class=kn>import</span> <span class=n>stats</span>

<span class=k>def</span> <span class=nf>add_hour</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
<span class=o>...</span>     <span class=n>x</span><span class=p>[</span><span class=s1>&#39;hour&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=s1>&#39;moment&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>hour</span>
<span class=o>...</span>     <span class=k>return</span> <span class=n>x</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>compose</span><span class=o>.</span><span class=n>Whitelister</span><span class=p>(</span><span class=s1>&#39;clouds&#39;</span><span class=p>,</span> <span class=s1>&#39;humidity&#39;</span><span class=p>,</span> <span class=s1>&#39;pressure&#39;</span><span class=p>,</span>
                            <span class=s1>&#39;temperature&#39;</span><span class=p>,</span> <span class=s1>&#39;wind&#39;</span><span class=p>)</span>
<span class=n>model</span> <span class=o>+=</span> <span class=p>(</span>
<span class=o>...</span>     <span class=n>add_hour</span> <span class=o>|</span>
<span class=o>...</span>     <span class=n>feature_extraction</span><span class=o>.</span><span class=n>TargetAgg</span><span class=p>(</span><span class=n>by</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;station&#39;</span><span class=p>,</span> <span class=s1>&#39;hour&#39;</span><span class=p>],</span> <span class=n>how</span><span class=o>=</span><span class=n>stats</span><span class=o>.</span><span class=n>Mean</span><span class=p>())</span>
<span class=o>...</span> <span class=p>)</span>
<span class=n>model</span> <span class=o>+=</span> <span class=n>feature_extraction</span><span class=o>.</span><span class=n>TargetAgg</span><span class=p>(</span><span class=n>by</span><span class=o>=</span><span class=s1>&#39;station&#39;</span><span class=p>,</span> <span class=n>how</span><span class=o>=</span><span class=n>stats</span><span class=o>.</span><span class=n>EWMean</span><span class=p>(</span><span class=mf>0.5</span><span class=p>))</span>
<span class=n>model</span> <span class=o>|=</span> <span class=n>preprocessing</span><span class=o>.</span><span class=n>StandardScaler</span><span class=p>()</span>
<span class=n>model</span> <span class=o>|=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>LinearRegression</span><span class=p>(</span><span class=n>intercept_lr</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</code></pre></div><p>Drawing the model offers some perspective.</p><p><img src=/img/blog/machine-learning-production/bikes_pipeline.svg alt=bikes_pipeline></p><p>The convention is that each incoming observation is represented as a <code>dict</code>, which are usually named <code>x</code>. In the case of bike stations, here is an example:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=p>{</span>
    <span class=s1>&#39;moment&#39;</span><span class=p>:</span> <span class=n>datetime</span><span class=o>.</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2016</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>7</span><span class=p>),</span>
    <span class=s1>&#39;station&#39;</span><span class=p>:</span> <span class=s1>&#39;metro-canal-du-midi&#39;</span><span class=p>,</span>
    <span class=s1>&#39;clouds&#39;</span><span class=p>:</span> <span class=mi>75</span><span class=p>,</span>
    <span class=s1>&#39;description&#39;</span><span class=p>:</span> <span class=s1>&#39;light rain&#39;</span><span class=p>,</span>
    <span class=s1>&#39;humidity&#39;</span><span class=p>:</span> <span class=mi>81</span><span class=p>,</span>
    <span class=s1>&#39;pressure&#39;</span><span class=p>:</span> <span class=mf>1017.0</span><span class=p>,</span>
    <span class=s1>&#39;temperature&#39;</span><span class=p>:</span> <span class=mf>6.54</span><span class=p>,</span>
    <span class=s1>&#39;wind&#39;</span><span class=p>:</span> <span class=mf>9.3</span>
<span class=p>}</span>
</code></pre></div><p>The target values are usually named <code>y</code>, and simply are scalar values, such as 32 in case of regression and <code>True</code> or <code>False</code> for binary classification. What happens is that every time a new observation arrives, the model can be updated:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit_one</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</code></pre></div><p>For making predictions, the model can be used as so:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>predict_one</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</code></pre></div><p>In this case, the model is a linear regression and it&rsquo;s weights are updating using <a href=https://www.wikiwand.com/en/Stochastic_gradient_descent>stochastic gradient descent</a>. The features we are using are the raw weather metrics we have available, along with aggregates of the previous target values. I don&rsquo;t want to get too specific about the details because that&rsquo;s not the point of this article. What I want to convey having models that learn incrementally opens up a world of opportunities. Simply put, you don&rsquo;t have to retrain models every once in a while. Your model can learn on-the-go and make predictions whenever necessary. If a new observation comes in, you can just call <code>fit_one(x, y)</code> and the model will be updated without having to restart from scratch. Another great benefit is that you don&rsquo;t have to store the data! The only reason why the data has to be stored is because batch models learn from scratch and thus the history of the data needs to be recorded. With online machine learning, you&rsquo;re allowed to discard your data as you see fit.</p><p>Online learning has been around for quite some time. Big players such as Google and Yahoo! publish many papers on the topic because it exactly suits their needs (<a href=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf>here</a> is a good example). Indeed, online advertising requires handling such a huge stream of data that makes batch learning out of the question. Having dynamic models that learn on the fly and that require a low amount of memory is key to the success of their operations. However, I believe that online learning doesn&rsquo;t have to restricted to the world of big data. My postulate is that using online learning instead of batch learning can solve a lot headaches for small and medium projects too. Think about it: your model can update itself and make predictions right inside an HTTP request. You don&rsquo;t have think about so many painful things, or pay expensive services that handle it for you.</p><p>I got an epiphany of some sorts back in January 2019, and since then have been trying to advocate online learning to my friends and colleagues. To support my arguments, I built an example app to forecast the amount of time of a <a href=https://www.wikiwand.com/en/League_of_Legends>League of Legends</a> match. As of writing this article, the app is live <a href=http://lol.creme-ml.com/>here</a> and you can find the code <a href=https://github.com/creme-ml/lol-match-duration/>here</a>. The goal of the project is to show how to smoothly integrate a machine learning model into a live application. Predicting the duration of a match is pretty useless, and I have little incentive to build a strong model. I mostly picked League of Legends because their API is free and has a lot of information. Also I didn&rsquo;t want to work on bike stations data again.</p><p>When the application starts, a <code>creme</code> model is instantiated and stored inside a PostgreSQL as a <a href=https://docs.python.org/3/library/pickle.html>pickle</a>. Every time a user asks for the duration of a match, the app queries the League of Legends official API and retrieves the match information. Of course, if the match has already ended then nothing happens. The raw match information is then fed through the model and a predicted match duration is obtained by calling <code>model.predict_one(match_info)</code>. Every once in a while the app polls the API to check if the match has ended. Once the match has ended, the true duration is obtained and the model is updated by calling <code>model.fit_one(match_info, match_duration)</code>. Naturally, this requires storing the information available at the start of the match until it ends, but this isn&rsquo;t very difficult to handle. That&rsquo;s all there really is to it! In fact, implementing the machine learning part of the app only took me a few hours, and was rather easy to do. The following diagram somewhat summarizes what I just explained.</p><p><img src=/img/blog/machine-learning-production/lol_architecture.svg alt=lol_architecture></p><p>If I had to do the same app using, say, scikit-learn, then things would not have been as straightforward. Batch learning is great, but I feel it is a poor choice for applications where the data is by nature a stream. Of course, both approaches are complementary, and it all depends on your use case. If you&rsquo;re worried about the performance of online learning models, then don&rsquo;t be. Online learning models are known to converge at a very reasonable rate towards batch models. What&rsquo;s more, online learning has so many benefits that in most cases it blows batch learning out of the water. I&rsquo;m not saying that it&rsquo;s a one-size fits all, however I do believe that data scientists and engineers should at least consider it when starting a new data science project. As I mentioned above, in my experience most practitioners are not even aware of online learning.</p><p>I hope you enjoyed this article! Feel free to send me an email if you have any questions.</p></div><script type=text/javascript>var s=document.createElement('script');s.setAttribute('src','https://utteranc.es/client.js'),s.setAttribute('repo','MaxHalford/maxhalford.github.io'),s.setAttribute('issue-term','pathname'),s.setAttribute('crossorigin','anonymous'),s.setAttribute('async',null),window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').matches?s.setAttribute('theme','github-dark'):s.setAttribute('theme','github-light'),document.body.appendChild(s)</script><div class=footer><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to top</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.0/elevator.min.js></script><script>var elementButton=document.querySelector('.elevator'),elevator=new Elevator({element:elementButton,mainAudio:'/music/elevator.mp3',endAudio:'/music/ding.mp3'})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=site-footer><div class=site-footer-item><a href=https://github.com/MaxHalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M8 0C3.58.0.0 3.582.0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385.0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953.0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117.0.0.67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147.0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48.0 1.07-.01 1.93-.01 2.19.0.21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg></span></a></div><div class=site-footer-item><a href=https://linkedin.com/in/maxhalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235.0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4.0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762.0-1.376-.617-1.376-1.377.0-.758.614-1.375 1.376-1.375.76.0 1.376.617 1.376 1.375.0.76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816.0H1.18C.528.0.0.516.0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652.0 1.185-.516 1.185-1.153V1.153C16 .516 15.467.0 14.815.0z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://twitter.com/halford_max><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812.0-3.282 1.47-3.282 3.28.0.26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21.0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26.0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04.0 9.34-5 9.34-9.33.0-.14.0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://kaggle.com/maxhalford><span class=inline-svg><svg role="img" viewBox="0 0 26 26" xmlns="http://www.w3.org/2000/svg"><title>Kaggle icon</title><path fill="currentcolor" d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187.0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236.0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234.0.351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144.0.236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.358"/></svg></span></a></div><div class=site-footer-item><a href="https://scholar.google.com/citations?user=erRNNi0AAAAJ&hl=en"><span class=inline-svg><svg viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952.0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584.0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288.0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962.0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zm762.098 382.384c-64.358 64.424-141.86 96.57-232.572 96.57H329.144c-90.712.0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572V182.859c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712.0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159V762.033c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162.0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53.0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476.0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86.0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908.0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426.0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432.0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234.0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048.0 6.642.19 12.492.672 18.974H292.574l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382.0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994.0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376.0 103.05 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382.0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.05 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z"/></svg></span></a></div><div class=site-footer-item><a href=/files/resume_max_halford.pdf><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 392.533 392.533" style="enable-background:new 0 0 392.533 392.533"><g><g><path fill="currentcolor" d="M292.396 324.849H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 329.697 298.473 324.849 292.396 324.849z"/></g></g><g><g><path fill="currentcolor" d="M292.396 277.01H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 281.859 298.473 277.01 292.396 277.01z"/></g></g><g><g><path fill="currentcolor" d="M196.137 45.834c-25.859.0-46.998 21.075-46.998 46.998.0 25.859 21.139 46.933 46.998 46.933s46.998-21.075 46.998-46.998-21.139-46.933-46.998-46.933zm0 72.017c-13.77.0-25.083-11.313-25.083-25.083s11.248-25.083 25.083-25.083 25.083 11.313 25.083 25.083c0 13.769-11.313 25.083-25.083 25.083z"/></g></g><g><g><path fill="currentcolor" d="M258.521 163.362c-39.887-15.515-84.752-15.515-124.638.0-13.059 5.107-21.786 18.101-21.786 32.388v44.347c-.065 6.012 4.849 10.925 10.861 10.925h146.424c6.012.0 10.925-4.848 10.925-10.925V195.75C280.307 181.463 271.58 168.469 258.521 163.362zm0 65.874H133.883v-33.422c0-5.301 3.168-10.214 7.887-12.024 34.844-13.511 74.02-13.511 108.865.0 4.719 1.875 7.887 6.659 7.887 12.024v33.422z"/></g></g><g><g><path fill="currentcolor" d="M313.083.0H131.491c-8.404.0-16.291 3.232-22.238 9.18L57.018 61.414c-5.947 5.948-9.18 13.834-9.18 22.238v277.333c0 17.39 14.158 31.547 31.547 31.547h233.762c17.39.0 31.547-14.158 31.547-31.547V31.547C344.501 14.158 330.343.0 313.083.0zM112.032 37.236v27.022H85.01l27.022-27.022zm210.683 79.58h-40.598c-6.012.0-10.925 4.849-10.925 10.925.0 6.012 4.848 10.925 10.925 10.925h40.598v19.394h-14.869c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h14.869v181.139c0 5.366-4.331 9.697-9.632 9.697H79.192c-5.301.0-9.632-4.331-9.632-9.632V86.044h53.398c6.012.0 10.925-4.848 10.925-10.925V21.721h179.2c5.301.0 9.632 4.331 9.632 9.632v85.463z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></span></a></div><div class=site-footer-item><a href=https://play.spotify.com/user/1166811350><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 168 168"><path fill="currentcolor" d="m83.996.277C37.747.277.253 37.77.253 84.019c0 46.251 37.494 83.741 83.743 83.741 46.254.0 83.744-37.49 83.744-83.741.0-46.246-37.49-83.738-83.745-83.738l.001-.004zm38.404 120.78c-1.5 2.46-4.72 3.24-7.18 1.73-19.662-12.01-44.414-14.73-73.564-8.07-2.809.64-5.609-1.12-6.249-3.93-.643-2.81 1.11-5.61 3.926-6.25 31.9-7.291 59.263-4.15 81.337 9.34 2.46 1.51 3.24 4.72 1.73 7.18zm10.25-22.805c-1.89 3.075-5.91 4.045-8.98 2.155-22.51-13.839-56.823-17.846-83.448-9.764-3.453 1.043-7.1-.903-8.148-4.35-1.04-3.453.907-7.093 4.354-8.143 30.413-9.228 68.222-4.758 94.072 11.127 3.07 1.89 4.04 5.91 2.15 8.976v-.001zm.88-23.744c-26.99-16.031-71.52-17.505-97.289-9.684-4.138 1.255-8.514-1.081-9.768-5.219-1.254-4.14 1.08-8.513 5.221-9.771 29.581-8.98 78.756-7.245 109.83 11.202 3.73 2.209 4.95 7.016 2.74 10.733-2.2 3.722-7.02 4.949-10.73 2.739z"/></svg></span></a></div><div class=site-footer-item><a href=mailto:maxhalford25@gmail.com><span class=inline-svg><svg viewBox="0 0 15 20" xmlns="http://www.w3.org/2000/svg"><title>mail</title><path fill="currentcolor" d="M0 4v8c0 .55.45 1 1 1h12c.55.0 1-.45 1-1V4c0-.55-.45-1-1-1H1c-.55.0-1 .45-1 1zm13 0L7 9 1 4h12zM1 5.5l4 3-4 3v-6zM2 12l3.5-3L7 10.5 8.5 9l3.5 3H2zm11-.5-4-3 4-3v6z" fill="#000" fill-rule="evenodd"/></svg></span></a></div><div class=site-footer-item><a href=/index.xml><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M12.8 16C12.8 8.978 7.022 3.2.0 3.2V0c8.777.0 16 7.223 16 16h-3.2zM2.194 11.61c1.21.0 2.195.985 2.195 2.196.0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017.0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818.0 10.606 4.79 10.606 10.607z"/></svg></span></a></div></div></div></div></article><script></script></body></html>