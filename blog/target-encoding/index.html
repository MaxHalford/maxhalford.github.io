<!doctype html><html lang=en><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-63302552-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-63302552-1')</script><script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script><meta charset=utf-8><meta name=generator content="Hugo 0.81.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/target-encoding/"><link rel=canonical href=https://maxhalford.github.io/blog/target-encoding/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Target encoding done the right way","headline":"Target encoding done the right way","description":"When you\u0026rsquo;re doing supervised learning, you often have to deal with categorical variables. That is, variables which don\u0026rsquo;t have a natural numerical representation. The problem is that most machine learning algorithms require the input data to be numerical. At some point or another a data science pipeline will require converting categorical variables to numerical variables.\nThere are many ways to do so:\n Label encoding where you choose an arbitrary number for each category One-hot encoding where you create one binary column per category Vector representation a.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2018","datePublished":"2018-10-13 00:00:00 \u002b0000 UTC","dateModified":"2018-10-13 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/target-encoding\/","keywords":[]}</script><title>Target encoding done the right way - Max Halford</title><meta property="og:title" content="Target encoding done the right way - Max Halford"><meta property="og:type" content="article"><meta name=description content="When you&rsquo;re doing supervised learning, you often have to deal with categorical variables. That is, variables which don&rsquo;t have a natural numerical representation. The problem is that most machine learning algorithms require the input data to be numerical. At some point or another a data science pipeline will require converting categorical variables to numerical variables.
There are many ways to do so:
 Label encoding where you choose an arbitrary number for each category One-hot encoding where you create one binary column per category Vector representation a."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link href="https://fonts.googleapis.com/css?family=PT+Serif|PT+Sans|Permanent+Marker" rel=stylesheet><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0,tags:'ams'},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll('mjx-container').forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=post-header><header><div class="signatures site-title"><a href=/>Max Halford</a></div></header><div class="row end-xs"><div><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></div><div class=header-line></div></div><header class=post-header><h1 class=post-title>Target encoding done the right way</h1><div class="row post-desc"><div class=col-xs-12><time class=post-date datetime="2018-10-13 00:00:00 UTC">2018-10-13 - 7 minutes read</time></div></div></header><div class="post-content markdown-body"><p>When you&rsquo;re doing supervised learning, you often have to deal with categorical variables. That is, variables which don&rsquo;t have a natural numerical representation. The problem is that most machine learning algorithms require the input data to be numerical. At some point or another a data science pipeline will require converting categorical variables to numerical variables.</p><p>There are many ways to do so:</p><ul><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html>Label encoding</a> where you choose an arbitrary number for each category</li><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html>One-hot encoding</a> where you create one binary column per category</li><li><a href=https://www.tensorflow.org/tutorials/representation/word2vec>Vector representation</a> a.k.a. word2vec where you find a low dimensional subspace that fits your data</li><li><a href=https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst#categorical-feature-support>Optimal binning</a> where you rely on tree-learners such as LightGBM or CatBoost</li><li><a href=http://www.saedsayad.com/encoding.htm>Target encoding</a> where you average the target value by category</li></ul><p>Each and every one of these method has its own pros and cons. The best approach typically depends on your data and your requirements. If a variable has a lot of categories, then a one-hot encoding scheme will produce many columns, which can cause memory issues. In my experience, relying on LightGBM/CatBoost is the best out-of-the-box method. Label encoding is useless and you should never use it. However if your categorical variable happens to be ordinal then you can and should represent it with increasing numbers (for example &ldquo;cold&rdquo; becomes 0, &ldquo;mild&rdquo; becomes 1, and &ldquo;hot&rdquo; becomes 2). <a href=https://www.wikiwand.com/en/Word2vec>Word2vec</a> and others such methods are cool and good but they require some fine-tuning and don&rsquo;t always work out of the box.</p><p>Target encoding is a fast way to get the most out of your categorical variables with little effort. The idea is quite simple. Say you have a categorical variable $x$ and a target $y$ &ndash; $y$ can be binary or continuous, it doesn&rsquo;t matter. For each distinct element in $x$ you&rsquo;re going to compute the average of the corresponding values in $y$. Then you&rsquo;re going to replace each $x_i$ with the according mean. This is rather easy to do in Python and the <code>pandas</code> library.</p><p>First let&rsquo;s create some dummy data.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>pandas</span> <span class=kn>as</span> <span class=nn>pd</span>

<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>({</span>
    <span class=s1>&#39;x_0&#39;</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;a&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>5</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;b&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>5</span><span class=p>,</span>
    <span class=s1>&#39;x_1&#39;</span><span class=p>:</span> <span class=p>[</span><span class=s1>&#39;c&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>9</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;d&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>1</span><span class=p>,</span>
    <span class=s1>&#39;y&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
<span class=p>})</span>
</code></pre></div><table><thead><tr><th>$x_0$</th><th>$x_1$</th><th>$y$</th></tr></thead><tbody><tr><td>$a$</td><td>$c$</td><td>1</td></tr><tr><td>$a$</td><td>$c$</td><td>1</td></tr><tr><td>$a$</td><td>$c$</td><td>1</td></tr><tr><td>$a$</td><td>$c$</td><td>1</td></tr><tr><td>$a$</td><td>$c$</td><td>0</td></tr><tr><td>$b$</td><td>$c$</td><td>1</td></tr><tr><td>$b$</td><td>$c$</td><td>0</td></tr><tr><td>$b$</td><td>$c$</td><td>0</td></tr><tr><td>$b$</td><td>$c$</td><td>0</td></tr><tr><td>$b$</td><td>$d$</td><td>0</td></tr></tbody></table><p>We can start by computing the means of the $x_0$ column.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>means</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;x_0&#39;</span><span class=p>)[</span><span class=s1>&#39;y&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</code></pre></div><p>This results in the following dictionary.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=p>{</span>
    <span class=s1>&#39;a&#39;</span><span class=p>:</span> <span class=mf>0.8</span><span class=p>,</span>
    <span class=s1>&#39;b&#39;</span><span class=p>:</span> <span class=mf>0.2</span>
<span class=p>}</span>
</code></pre></div><p>We can then replace each value in $x_0$ with the matching mean.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_0&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_0&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>means</span><span class=p>)</span>
</code></pre></div><p>We now have the following data frame.</p><table><thead><tr><th>$x_0$</th><th>$x_1$</th><th>$y$</th></tr></thead><tbody><tr><td>0.8</td><td>$c$</td><td>1</td></tr><tr><td>0.8</td><td>$c$</td><td>1</td></tr><tr><td>0.8</td><td>$c$</td><td>1</td></tr><tr><td>0.8</td><td>$c$</td><td>1</td></tr><tr><td>0.8</td><td>$c$</td><td>0</td></tr><tr><td>0.2</td><td>$c$</td><td>1</td></tr><tr><td>0.2</td><td>$c$</td><td>0</td></tr><tr><td>0.2</td><td>$c$</td><td>0</td></tr><tr><td>0.2</td><td>$c$</td><td>0</td></tr><tr><td>0.2</td><td>$d$</td><td>0</td></tr></tbody></table><p>We can do the same for $x_1$.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_1&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_1&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;x_1&#39;</span><span class=p>)[</span><span class=s1>&#39;y&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
</code></pre></div><table><thead><tr><th>$x_0$</th><th>$x_1$</th><th>$y$</th></tr></thead><tbody><tr><td>0.8</td><td>0.444</td><td>1</td></tr><tr><td>0.8</td><td>0.444</td><td>1</td></tr><tr><td>0.8</td><td>0.444</td><td>1</td></tr><tr><td>0.8</td><td>0.444</td><td>1</td></tr><tr><td>0.8</td><td>0.444</td><td>0</td></tr><tr><td>0.2</td><td>0.444</td><td>1</td></tr><tr><td>0.2</td><td>0.444</td><td>0</td></tr><tr><td>0.2</td><td>0.444</td><td>0</td></tr><tr><td>0.2</td><td>0.444</td><td>0</td></tr><tr><td>0.2</td><td>0</td><td>0</td></tr></tbody></table><p>Target encoding is good because it picks up values that can explain the target. In this silly example value $a$ of variable $x_0$ has an average target value of $0.8$. This can greatly help the machine learning classifications algorithms used downstream.</p><p>The problem of target encoding has a name: <strong>over-fitting</strong>. Indeed, relying on an average value isn&rsquo;t always a good idea when the number of values used in the average is low. You&rsquo;ve got to keep in mind that the dataset you&rsquo;re training on is a sample of a larger set. This means that whatever artifacts you may find in the training set might not hold true when applied to another dataset (i.e. the test set).</p><p>In the above xample, the value $d$ of variable $x_1$ is replaced with a 0 because it only appears once and the corresponding value of $y$ is a 0. Therefore, we&rsquo;re over-fitting because we don&rsquo;t have enough values to be <em>sure</em> that 0 is in fact the mean value of $y$ when $x_1$ is equal to $d$. In other words, relying on the average of only a few values is too reckless.</p><p>There are various ways to handle this. A popular way is to use cross-validation and compute the means in each out-of-fold dataset. This is <a href=http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html>what H20 does</a> and what many Kagglers do too. Another approach which I much prefer is to use <a href=https://www.wikiwand.com/en/Additive_smoothing>additive smoothing</a>. This is supposedly <a href=https://www.wikiwand.com/en/Bayes_estimator#/Practical_example_of_Bayes_estimators>what IMDB uses to rate its movies</a>.</p><p>The intuition is as follows. Imagine a new movie is posted on IMDB and it receives three ratings. Taking into account the three ratings gives the movie an average of 9.5. This is surprising because most movies tend to hover around 7, and the very good ones rarely go above 8. The point is that the average can&rsquo;t be trusted because there are too few values. The trick is to &ldquo;smooth&rdquo; the average by <strong>including the average rating over all movies</strong>. In other words, if there aren&rsquo;t many ratings we should rely on the global average rating, whereas if there enough ratings then we can safely rely on the local average.</p><p>Mathematically this is equivalent to:</p><p>$$\begin{equation}
\mu = \frac{n \times \bar{x} + m \times w}{n + m}
\end{equation}$$</p><p>where</p><ul><li>$\mu$ is the mean we&rsquo;re trying to compute (the one that&rsquo;s going to replace our categorical values)</li><li>$n$ is the number of values you have</li><li>$\bar{x}$ is your estimated mean</li><li>$m$ is the &ldquo;weight&rdquo; you want to assign to the overall mean</li><li>$w$ is the overall mean</li></ul><p>In this notation, $m$ is the only parameter you have to set. The idea is that the higher $m$ is, the more you&rsquo;re going to rely on the overall mean $w$. If $m$ is equal to 0 then you&rsquo;re simply going to compute the empirical mean, which is:</p><p>$$\begin{equation}
\mu = \frac{n \times \bar{x} + 0 \times w}{n + 0} = \frac{n \times \bar{x}}{n} = \bar{x}
\end{equation}$$</p><p>In other words you&rsquo;re not doing any smoothing whatsoever.</p><p>Again this is quite easy to do in Python. First we&rsquo;re going to write a method that computes a smooth mean. It&rsquo;s going to take as input a <code>pandas.DataFrame</code>, a categorical column name, the name of the target column, and a weight $m$.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>calc_smooth_mean</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>by</span><span class=p>,</span> <span class=n>on</span><span class=p>,</span> <span class=n>m</span><span class=p>):</span>
    <span class=c1># Compute the global mean</span>
    <span class=n>mean</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>on</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>

    <span class=c1># Compute the number of values and the mean of each group</span>
    <span class=n>agg</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=n>by</span><span class=p>)[</span><span class=n>on</span><span class=p>]</span><span class=o>.</span><span class=n>agg</span><span class=p>([</span><span class=s1>&#39;count&#39;</span><span class=p>,</span> <span class=s1>&#39;mean&#39;</span><span class=p>])</span>
    <span class=n>counts</span> <span class=o>=</span> <span class=n>agg</span><span class=p>[</span><span class=s1>&#39;count&#39;</span><span class=p>]</span>
    <span class=n>means</span> <span class=o>=</span> <span class=n>agg</span><span class=p>[</span><span class=s1>&#39;mean&#39;</span><span class=p>]</span>

    <span class=c1># Compute the &#34;smoothed&#34; means</span>
    <span class=n>smooth</span> <span class=o>=</span> <span class=p>(</span><span class=n>counts</span> <span class=o>*</span> <span class=n>means</span> <span class=o>+</span> <span class=n>m</span> <span class=o>*</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>counts</span> <span class=o>+</span> <span class=n>m</span><span class=p>)</span>

    <span class=c1># Replace each value by the according smoothed mean</span>
    <span class=k>return</span> <span class=n>df</span><span class=p>[</span><span class=n>by</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>smooth</span><span class=p>)</span>
</code></pre></div><p>Let&rsquo;s see what this does in the previous example with a weight of, say, 10.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_0&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>calc_smooth_mean</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>by</span><span class=o>=</span><span class=s1>&#39;x_0&#39;</span><span class=p>,</span> <span class=n>on</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>m</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;x_1&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>calc_smooth_mean</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>by</span><span class=o>=</span><span class=s1>&#39;x_1&#39;</span><span class=p>,</span> <span class=n>on</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>m</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</code></pre></div><table><thead><tr><th>$x_0$</th><th>$x_1$</th><th>$y$</th></tr></thead><tbody><tr><td>0.6</td><td>0.526316</td><td>1</td></tr><tr><td>0.6</td><td>0.526316</td><td>1</td></tr><tr><td>0.6</td><td>0.526316</td><td>1</td></tr><tr><td>0.6</td><td>0.526316</td><td>1</td></tr><tr><td>0.6</td><td>0.526316</td><td>0</td></tr><tr><td>0.4</td><td>0.526316</td><td>1</td></tr><tr><td>0.4</td><td>0.526316</td><td>0</td></tr><tr><td>0.4</td><td>0.526316</td><td>0</td></tr><tr><td>0.4</td><td>0.526316</td><td>0</td></tr><tr><td>0.4</td><td>0.454545</td><td>0</td></tr></tbody></table><p>It&rsquo;s quite noticeable that each computed value is much closer to the overall mean of 0.5. This is because a weight of 10 is rather large for a dataset of only 10 values. The value $d$ of variable $x_1$ has been replaced with 0.454545 instead of the 0 we got earlier. The equation for obtaining it was:</p><p>$$\begin{equation}
d = \frac{1 \times 0 + 10 \times 0.5}{1 + 10} = \frac{0 + 5}{11} \simeq 0.454545
\end{equation}$$</p><p>Meanwhile the new value for replacing the value $a$ of variable $x_0$ was:</p><p>$$\begin{equation}
a = \frac{5 \times 0.8 + 10 \times 0.5}{5 + 10} = \frac{4 + 5}{15} = 0.6
\end{equation}$$</p><p>Computing smooth means can be done extremely quickly. What&rsquo;s more, you only have to choose a single parameter, which is $m$. I find that setting to something like 300 works well in most cases. It&rsquo;s quite intuitive really: you&rsquo;re saying that you require that there must be at least 300 values for the sample mean to overtake the global mean. There are other ways to do target encoding that are somewhat popular on Kaggle, such as <a href=https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf>this one</a>. However, the latter method produces encoded variables which are very correlated with the output of additive smoothing, at the cost of requiring two parameters.</p><p>I&rsquo;m well aware that there are many well-written posts about target encoding elsewhere. I just want to do my bit and help spread the word: don&rsquo;t use vanilla target encoding! Go Bayesian and use priors. For those who are interested in using a scikit-learn compatible implementation, I did that just <a href=https://github.com/MaxHalford/xam/blob/master/docs/feature-extraction.md#smooth-target-encoding>here</a>.</p></div><script type=text/javascript>var s=document.createElement('script');s.setAttribute('src','https://utteranc.es/client.js'),s.setAttribute('repo','MaxHalford/maxhalford.github.io'),s.setAttribute('issue-term','pathname'),s.setAttribute('crossorigin','anonymous'),s.setAttribute('async',null),window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').matches?s.setAttribute('theme','github-dark'):s.setAttribute('theme','github-light'),document.body.appendChild(s)</script><div class=footer><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to top</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.0/elevator.min.js></script><script>var elementButton=document.querySelector('.elevator'),elevator=new Elevator({element:elementButton,mainAudio:'/music/elevator.mp3',endAudio:'/music/ding.mp3'})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=site-footer><div class=site-footer-item><a href=https://github.com/MaxHalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M8 0C3.58.0.0 3.582.0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385.0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953.0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117.0.0.67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147.0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48.0 1.07-.01 1.93-.01 2.19.0.21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg></span></a></div><div class=site-footer-item><a href=https://linkedin.com/in/maxhalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235.0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4.0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762.0-1.376-.617-1.376-1.377.0-.758.614-1.375 1.376-1.375.76.0 1.376.617 1.376 1.375.0.76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816.0H1.18C.528.0.0.516.0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652.0 1.185-.516 1.185-1.153V1.153C16 .516 15.467.0 14.815.0z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://twitter.com/halford_max><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812.0-3.282 1.47-3.282 3.28.0.26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21.0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26.0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04.0 9.34-5 9.34-9.33.0-.14.0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://kaggle.com/maxhalford><span class=inline-svg><svg role="img" viewBox="0 0 26 26" xmlns="http://www.w3.org/2000/svg"><title>Kaggle icon</title><path fill="currentcolor" d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187.0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236.0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234.0.351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144.0.236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.358"/></svg></span></a></div><div class=site-footer-item><a href="https://scholar.google.com/citations?user=erRNNi0AAAAJ&hl=en"><span class=inline-svg><svg viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952.0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584.0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288.0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962.0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zm762.098 382.384c-64.358 64.424-141.86 96.57-232.572 96.57H329.144c-90.712.0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572V182.859c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712.0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159V762.033c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162.0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53.0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476.0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86.0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908.0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426.0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432.0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234.0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048.0 6.642.19 12.492.672 18.974H292.574l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382.0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994.0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376.0 103.05 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382.0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.05 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z"/></svg></span></a></div><div class=site-footer-item><a href=/files/resume_max_halford.pdf><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 392.533 392.533" style="enable-background:new 0 0 392.533 392.533"><g><g><path fill="currentcolor" d="M292.396 324.849H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 329.697 298.473 324.849 292.396 324.849z"/></g></g><g><g><path fill="currentcolor" d="M292.396 277.01H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 281.859 298.473 277.01 292.396 277.01z"/></g></g><g><g><path fill="currentcolor" d="M196.137 45.834c-25.859.0-46.998 21.075-46.998 46.998.0 25.859 21.139 46.933 46.998 46.933s46.998-21.075 46.998-46.998-21.139-46.933-46.998-46.933zm0 72.017c-13.77.0-25.083-11.313-25.083-25.083s11.248-25.083 25.083-25.083 25.083 11.313 25.083 25.083c0 13.769-11.313 25.083-25.083 25.083z"/></g></g><g><g><path fill="currentcolor" d="M258.521 163.362c-39.887-15.515-84.752-15.515-124.638.0-13.059 5.107-21.786 18.101-21.786 32.388v44.347c-.065 6.012 4.849 10.925 10.861 10.925h146.424c6.012.0 10.925-4.848 10.925-10.925V195.75C280.307 181.463 271.58 168.469 258.521 163.362zm0 65.874H133.883v-33.422c0-5.301 3.168-10.214 7.887-12.024 34.844-13.511 74.02-13.511 108.865.0 4.719 1.875 7.887 6.659 7.887 12.024v33.422z"/></g></g><g><g><path fill="currentcolor" d="M313.083.0H131.491c-8.404.0-16.291 3.232-22.238 9.18L57.018 61.414c-5.947 5.948-9.18 13.834-9.18 22.238v277.333c0 17.39 14.158 31.547 31.547 31.547h233.762c17.39.0 31.547-14.158 31.547-31.547V31.547C344.501 14.158 330.343.0 313.083.0zM112.032 37.236v27.022H85.01l27.022-27.022zm210.683 79.58h-40.598c-6.012.0-10.925 4.849-10.925 10.925.0 6.012 4.848 10.925 10.925 10.925h40.598v19.394h-14.869c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h14.869v181.139c0 5.366-4.331 9.697-9.632 9.697H79.192c-5.301.0-9.632-4.331-9.632-9.632V86.044h53.398c6.012.0 10.925-4.848 10.925-10.925V21.721h179.2c5.301.0 9.632 4.331 9.632 9.632v85.463z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></span></a></div><div class=site-footer-item><a href=https://play.spotify.com/user/1166811350><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 168 168"><path fill="currentcolor" d="m83.996.277C37.747.277.253 37.77.253 84.019c0 46.251 37.494 83.741 83.743 83.741 46.254.0 83.744-37.49 83.744-83.741.0-46.246-37.49-83.738-83.745-83.738l.001-.004zm38.404 120.78c-1.5 2.46-4.72 3.24-7.18 1.73-19.662-12.01-44.414-14.73-73.564-8.07-2.809.64-5.609-1.12-6.249-3.93-.643-2.81 1.11-5.61 3.926-6.25 31.9-7.291 59.263-4.15 81.337 9.34 2.46 1.51 3.24 4.72 1.73 7.18zm10.25-22.805c-1.89 3.075-5.91 4.045-8.98 2.155-22.51-13.839-56.823-17.846-83.448-9.764-3.453 1.043-7.1-.903-8.148-4.35-1.04-3.453.907-7.093 4.354-8.143 30.413-9.228 68.222-4.758 94.072 11.127 3.07 1.89 4.04 5.91 2.15 8.976v-.001zm.88-23.744c-26.99-16.031-71.52-17.505-97.289-9.684-4.138 1.255-8.514-1.081-9.768-5.219-1.254-4.14 1.08-8.513 5.221-9.771 29.581-8.98 78.756-7.245 109.83 11.202 3.73 2.209 4.95 7.016 2.74 10.733-2.2 3.722-7.02 4.949-10.73 2.739z"/></svg></span></a></div><div class=site-footer-item><a href=mailto:maxhalford25@gmail.com><span class=inline-svg><svg viewBox="0 0 15 20" xmlns="http://www.w3.org/2000/svg"><title>mail</title><path fill="currentcolor" d="M0 4v8c0 .55.45 1 1 1h12c.55.0 1-.45 1-1V4c0-.55-.45-1-1-1H1c-.55.0-1 .45-1 1zm13 0L7 9 1 4h12zM1 5.5l4 3-4 3v-6zM2 12l3.5-3L7 10.5 8.5 9l3.5 3H2zm11-.5-4-3 4-3v6z" fill="#000" fill-rule="evenodd"/></svg></span></a></div><div class=site-footer-item><a href=/index.xml><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M12.8 16C12.8 8.978 7.022 3.2.0 3.2V0c8.777.0 16 7.223 16 16h-3.2zM2.194 11.61c1.21.0 2.195.985 2.195 2.196.0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017.0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818.0 10.606 4.79 10.606 10.607z"/></svg></span></a></div></div></div></div></article><script></script></body></html>