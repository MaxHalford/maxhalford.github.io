<!doctype html><html lang=en><head><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogImMwMjJhMjg1LWJmY2YtNDc0OC1hYzczLTJhMDQ1Njk3NTI0YyIsICJpZCI6ICIzNjc3NjQ3Ny04MTE2LTRmYWQtYjcwMy1iZmM3YjMwZGJjMjMifQ.A0vHm-VWbXG6uBFZiwuspN_AyfSYNrdZE3IgwgWSt4g></script><meta charset=utf-8><meta name=generator content="Hugo 0.120.4"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/efficient-data-transformation/"><link rel=canonical href=https://maxhalford.github.io/blog/efficient-data-transformation/><meta property="og:title" content="Efficient ELT refreshes"><meta property="og:description" content="A tenant of the modern data stack is the use of ELT (Extract, Load, Transform) over ETL (Extract, Transform, Load). In a nutshell, this means that most of the data transformation is done in the data warehouse. This has become the de facto standard for modern data teams, and is epitomized by dbt and its ecosystem. It&rsquo;s a great time to be a data engineer!
We at Carbonfact fully embrace the ELT paradigm."><meta property="og:type" content="article"><meta property="og:url" content="https://maxhalford.github.io/blog/efficient-data-transformation/"><meta property="og:image" content="https://maxhalford.github.io/img/belle-ile.jpg"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-12-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-12-01T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://maxhalford.github.io/img/belle-ile.jpg"><meta name=twitter:title content="Efficient ELT refreshes"><meta name=twitter:description content="A tenant of the modern data stack is the use of ELT (Extract, Load, Transform) over ETL (Extract, Transform, Load). In a nutshell, this means that most of the data transformation is done in the data warehouse. This has become the de facto standard for modern data teams, and is epitomized by dbt and its ecosystem. It&rsquo;s a great time to be a data engineer!
We at Carbonfact fully embrace the ELT paradigm."><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶î</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Efficient ELT refreshes","headline":"Efficient ELT refreshes","description":"A tenant of the modern data stack is the use of ELT (Extract, Load, Transform) over ETL (Extract, Transform, Load). In a nutshell, this means that most of the data transformation is done in the data warehouse. This has become the de facto standard for modern data teams, and is epitomized by dbt and its ecosystem. It\u0026rsquo;s a great time to be a data engineer!\nWe at Carbonfact fully embrace the ELT paradigm.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2023","datePublished":"2023-12-01 00:00:00 \u002b0000 UTC","dateModified":"2023-12-01 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/efficient-data-transformation\/","keywords":["data-eng"]}</script><title>Efficient ELT refreshes ‚Ä¢ Max Halford</title>
<meta property="og:title" content="Efficient ELT refreshes ‚Ä¢ Max Halford"><meta property="og:type" content="article"><meta name=description content="A tenant of the modern data stack is the use of ELT (Extract, Load, Transform) over ETL (Extract, Transform, Load). In a nutshell, this means that most of the data transformation is done in the data warehouse. This has become the de facto standard for modern data teams, and is epitomized by dbt and its ecosystem. It&rsquo;s a great time to be a data engineer!
We at Carbonfact fully embrace the ELT paradigm."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford „ÉÑ</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>Efficient ELT refreshes</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2023-12-01 00:00:00 UTC">2023-12-01
</time><span class=posts-line-tag>data-eng</span></div></div></header><div class="post-content markdown-body"><p>A tenant of the modern data stack is the use of ELT (Extract, Load, Transform) over ETL (Extract, Transform, Load). In a nutshell, this means that most of the data transformation is done in the data warehouse. This has become the <em>de facto</em> standard for modern data teams, and is epitomized by <a href=https://www.getdbt.com/>dbt</a> and its ecosystem. It&rsquo;s a great time to be a data engineer!</p><p>We at <a href=https://www.carbonfact.com/>Carbonfact</a> fully embrace the ELT paradigm. In fact, our whole platform is powered by BigQuery, which acts as our single source of truth. We have a main BigQuery dataset where we materialize several¬†SQL views that power what our customers see.</p><p>Our SQL views are stored in a GitHub repository. We open a pull request when we want to add/remove/edit a view. We have some CI/CD in place that creates a bespoke dataset for each pull request, and materializes each SQL view to that dataset. We then run unit tests &ndash; written in SQL &ndash; against the dataset. We also run a diff against the production dataset to see what the impact would be in terms of rows/columns added/removed/changed per view. This is what our setup looks like:</p><div align=center><figure style=width:50%><img src=/img/blog/efficient-elt-refreshes/before.png style=box-shadow:none></figure></div><p>We have identified two problems with this setup:</p><ol><li>It is not efficient. In a pull request, all the views are materialized, even if only one view is modified. This wastes time, money, and carbon.</li><li>When we do a diff between the pull request dataset and the production dataset, we are not guaranteed both datasets are using the same source data. This is because the pull request dataset is materialized at the time of the pull request, and the production dataset is periodically refreshed &ndash; every 3 hours in our case.</li></ol><p>This second problem is more subtle and merits some context. Let&rsquo;s say we have a view that counts events on our platform. We edit the view to filter out some undesired events. We commit our change and open a pull request. The pull request dataset is materialized, and the unit tests pass. We then compare the pull request dataset with the production dataset. We are surprised because we notice the number of events has actually increased. This is because the diff is conflating two things: the events filtered out by the new logic, and the new events due to pull request dataset having access to more recent data.</p><p>Here&rsquo;s a schematic of what&rsquo;s happening:</p><div align=center><figure style=width:90%><img src=/img/blog/efficient-elt-refreshes/no-freeze.png style=box-shadow:none></figure></div><p>At Carbonfact, we use PostHog to track events on our platform &ndash; i.e. pageviews and clicks. PostHog exports its data in near real-time to a BigQuery table we&rsquo;ve provided it with. In our analytics repository, we have an <code>events</code> view which consumes the data from the PostHog table. Therefore, the <code>events</code> table in the pull request can have a different number of rows than its counterpart in the main table. Indeed, because it depends on the PostHog table, its number of rows depends on the moment it was materialized.</p><p>When we edit a view, we&rsquo;re interested in understanding whether the new logic is correct or not, and if it breaks anything downstream. To do so, we must perform an apples to apples comparison in terms of source data. The solution we&rsquo;ve found is to freeze the <code>events</code> table in the pull request dataset, as so:</p><div align=center><figure style=width:90%><img src=/img/blog/efficient-elt-refreshes/with-freeze.png style=box-shadow:none></figure></div><p>Even though the <code>metrics</code> view in the pull request is refreshed at a different moment, it can be compared to its equivalent in production dataset. Indeed, both are leveraging the same <code>events</code> table. This kills two birds with one stone:</p><ol><li>We can do a meaningful comparison between the pull request dataset and the production dataset.</li><li>We¬†are being efficient because we only have to materialize modified views and their dependencies, not the whole DAG.</li></ol><p>How do we go about doing this in practice? What needs to happen is for the SQL queries in the pull request to target the correct table references. This necessitates a few pieces of logic:</p><ul><li>Identify the edited files from the git diff.</li><li>For each edited view, target tables from the production dataset.</li><li>For each dependency of the edited views, target the materialized table from the pull request.</li></ul><p>I haven&rsquo;t found an easy way to do this with dbt. I found this <a href=https://discourse.getdbt.com/t/tips-and-tricks-about-working-with-dbt/287/2>tip</a> from the dbt forums about how to feed edited views &ndash; and optionally their dependencies &ndash; to the <code>dbt run</code> command. There&rsquo;s also this <a href=https://gist.github.com/jtalmi/c6265c8a17120cfb150c97512cb68aa6>gist</a> that does something similar. However, I haven&rsquo;t found a way to switch the target dataset dynamically.</p><p>I did stumble on dbt&rsquo;s <a href=https://docs.getdbt.com/reference/node-selection/syntax#stateful-selection>stateful selection</a> documentation, but I don&rsquo;t see a straightforward way to use it in this context. Even the dbt docs <a href=https://docs.getdbt.com/reference/node-selection/methods#the-state-method>admit</a> state selection is complicated. There&rsquo;s a comprehensive example <a href=https://docs.getdbt.com/blog/slim-ci-cd-with-bitbucket-pipelines>here</a>, but it feels rather cumbersome to have to store/download artifacts from a previous run.</p><p>We&rsquo;ve implemented a minimalist dbt-like tool at Carbonfact called <a href=https://github.com/carbonfact/lea>lea</a>. Many people have told me that we&rsquo;re crazy and just stick to dbt. They&rsquo;re probably right. But dbt boils the ocean, and we only need a subset of its features. It&rsquo;s also a great learning experience, and allows us to experiment with new ideas.</p><p>Here is what our CI/CD looks like with lea:</p><div align=center><figure style=width:50%><img src=/img/blog/efficient-elt-refreshes/after.png style=box-shadow:none></figure></div><p>The following command is where the magic happens:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>lea run --select git+ --freeze-unselected
</span></span></code></pre></div><p>The <code>--select git+</code> part selects all the views that have been edited in the pull request, as well as their descendants. This ensures we will materialize all the parts of the DAG that depend on the changes we made. The <code>--freeze-unselected</code> part freezes all the tables which are not selected, meaning that the SQL queries will target the production dataset for the unselected views. This is all done in a stateless manner, meaning we don&rsquo;t have to store artifacts from previous runs.</p><p>I&rsquo;m sure that the same efficiency could be achieved with dbt, but that&rsquo;s not the point. The point is that more and more people are doing analytics, and I believe it&rsquo;s important to put simple tools in their hands. Not everyone needs the full power of dbt, and it&rsquo;s important to have a low barrier to entry. I&rsquo;m not saying that lea is the answer, but I do think we need tools that make it easy to do the right thing &ndash; aka the <a href=https://blog.codinghorror.com/falling-into-the-pit-of-success/>pit of success</a>.</p><p>Feel welcome to reach out if you want to riff on this topic and/or tell me I&rsquo;m doing everything wrong ‚úåÔ∏è</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px;margin-bottom:30px><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div><iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=32 width=114 style=border:0;border-radius:6px></iframe></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script><script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href=/blog/pandas-tricks/>A few intermediate pandas tricks</a></li><li><a href=/blog/grouping-sets/>Dashboards and GROUPING SETS</a></li><li><a href=/blog/dataset-time-travel/>An overview of dataset time travel</a></li></ul></div></div></div></article></body></html>