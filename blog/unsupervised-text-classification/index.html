<!doctype html><html lang=en><head><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogImMwMjJhMjg1LWJmY2YtNDc0OC1hYzczLTJhMDQ1Njk3NTI0YyIsICJpZCI6ICIzNjc3NjQ3Ny04MTE2LTRmYWQtYjcwMy1iZmM3YjMwZGJjMjMifQ.A0vHm-VWbXG6uBFZiwuspN_AyfSYNrdZE3IgwgWSt4g></script><meta charset=utf-8><meta name=generator content="Hugo 0.123.6"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/unsupervised-text-classification/"><link rel=canonical href=https://maxhalford.github.io/blog/unsupervised-text-classification/><meta property="og:title" content="Unsupervised text classification with word embeddings"><meta property="og:description" content="Edit &ndash; since writing this article, I have discovered that the method I describe is a form of zero-shot learning. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.
Edit &ndash; I stumbled on a paper entitled &ldquo;Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings&rdquo; which proposes something very similar. The paper is rather well written, so you might want to check it out."><meta property="og:type" content="article"><meta property="og:url" content="https://maxhalford.github.io/blog/unsupervised-text-classification/"><meta property="og:image" content="https://maxhalford.github.io/img/belle-ile.jpg"><meta property="article:section" content="blog"><meta property="article:published_time" content="2020-10-03T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-03T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://maxhalford.github.io/img/belle-ile.jpg"><meta name=twitter:title content="Unsupervised text classification with word embeddings"><meta name=twitter:description content="Edit &ndash; since writing this article, I have discovered that the method I describe is a form of zero-shot learning. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.
Edit &ndash; I stumbled on a paper entitled &ldquo;Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings&rdquo; which proposes something very similar. The paper is rather well written, so you might want to check it out."><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦†</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Unsupervised text classification with word embeddings","headline":"Unsupervised text classification with word embeddings","description":"Edit \u0026ndash; since writing this article, I have discovered that the method I describe is a form of zero-shot learning. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.\nEdit \u0026ndash; I stumbled on a paper entitled \u0026ldquo;Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings\u0026rdquo; which proposes something very similar. The paper is rather well written, so you might want to check it out.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2020","datePublished":"2020-10-03 00:00:00 \u002b0000 UTC","dateModified":"2020-10-03 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/unsupervised-text-classification\/","keywords":["machine-learning","text-processing"]}</script><title>Unsupervised text classification with word embeddings â€¢ Max Halford</title>
<meta property="og:title" content="Unsupervised text classification with word embeddings â€¢ Max Halford"><meta property="og:type" content="article"><meta name=description content="Edit &ndash; since writing this article, I have discovered that the method I describe is a form of zero-shot learning. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.
Edit &ndash; I stumbled on a paper entitled &ldquo;Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings&rdquo; which proposes something very similar. The paper is rather well written, so you might want to check it out."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford ðŸ¦†</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>Unsupervised text classification with word embeddings</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2020-10-03 00:00:00 UTC">2020-10-03
</time><span class=posts-line-tag>machine-learning</span>
<span class=posts-line-tag>text-processing</span></div></div></header><div class="post-content markdown-body"><div align=center><img height=300px src=/img/blog/document-classification/morpheus.jpg alt=morpheus><br></div><p><strong>Edit</strong> &ndash; <em>since writing this article, I have discovered that the method I describe is a form of <a href=https://www.wikiwand.com/en/Zero-shot_learning>zero-shot learning</a>. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.</em></p><p><strong>Edit</strong> &ndash; <em>I stumbled on a <a href=https://www.aclweb.org/anthology/P19-1036/>paper</a> entitled &ldquo;Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings&rdquo; which proposes something very similar. The paper is rather well written, so you might want to check it out. Note that they call the <code>tech -> technology</code> trick &ldquo;label enrichment&rdquo;.</em></p><p>I recently watched a <a href="https://www.youtube.com/watch?v=IDNXZitcQng">lecture</a> by <a href=https://twitter.com/adamfungi>Adam Tauman Kalai</a> on stereotype bias in text data. The lecture is very good, but something that had nothing to do with the lecture&rsquo;s main topic caught my attention. At <a href="http://www.youtube.com/watch?v=IDNXZitcQng&amp;t=19m20s">19:20</a>, Adam explains that <a href=https://www.wikiwand.com/en/Word_embedding>word embeddings</a> can be used to classify documents when no labeled training data is available. Note that in this article I&rsquo;ll be using <em>word embeddings</em> and <em>word vectors</em> interchangeably. The idea is to exploit the fact that document labels are often textual. For instance, the labels from the <a href=https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge>Toxic Comment Classification Challenge</a> are <em>toxic</em>, <em>severe toxic</em>, <em>obscene</em>, <em>threat</em>, <em>insult</em>, and <em>identity hate</em>. Because the labels are textual, they can be projected into an embedded vector space, just like the words in the document they pertain to. Therefore, a document can be classified by looking at the distance from its word vector centroid with respect to each label. What I&rsquo;m referring to as centroid is the column-wise average of the word embeddings that are extracted from the document. I believe that a visual example will speak for itself.</p><p>Consider the following sentence:</p><blockquote><p>I brought some muffins to church, I baked them myself.</p></blockquote><p>Assuming that we have some function that extracts important words from a sentence, these would be: <em>muffins</em>, <em>church</em>, and <em>baked</em>. Let&rsquo;s say that we want to assign one of three possible labels to the sentence: <em>cooking</em>, <em>religion</em>, and <em>architecture</em>. The first step is to embed the labels. This can be done by using pre-trained word vectors, such as those trained on Wikipedia using <a href=https://fasttext.cc/>fastText</a>, which you can find <a href=https://fasttext.cc/docs/en/pretrained-vectors.html>here</a>. Next, embed each word in the document. Then, compute the <a href=https://www.wikiwand.com/en/Centroid>centroid</a> of the word embeddings. Finally, compute the distances from the centroid to each label vector and return the closest one. This particular sentence is assigned the &ldquo;cooking&rdquo; label because its centroid is mostly influenced by &ldquo;baked&rdquo; and &ldquo;muffins&rdquo;, which are close to &ldquo;cooking&rdquo; in the embedded vector space.</p><div align=center><figure style=width:80%><img style=padding:1em src=/img/blog/document-classification/viz.png alt=viz><figcaption>Made with <a href=https://excalidraw.com/>Excalidraw</a></figcaption></figure></div><p>I think that this is really cool and intuitive! I&rsquo;m not very experienced in NLP, and so I&rsquo;m not aware if this is common knowledge/practice. As far as I can tell, in terms of document classification, word embeddings are more often used as the first layer of a neural network architecture. In any case, as Adam says in his lecture, this is really useful for startups that don&rsquo;t have access to large corpuses of labeled training data. In fact, it turns out that I have a friend who is exactly in this kind of situation. She&rsquo;s working on some newsfeed project where she needs to classify articles scraped from various websites. She doesn&rsquo;t have a lot of labeled training data and is thus looking into <a href=https://www.wikiwand.com/en/Active_learning_(machine_learning)>active learning</a> and <a href=https://www.wikiwand.com/en/Semi-supervised_learning>semi-supervised learning</a>. These are both worthwhile directions to pursue, but I&rsquo;m pretty sure that this word embedding method is a low-hanging fruit that she should check out first. Because I&rsquo;m a nice guy, I decided to code a prototype to help her out. I thought it would be cool to also share this on my blog in order to potentially get some feedback, so here goes.</p><p>As a case study, I&rsquo;m going the BBC news dataset from the <a href=http://mlg.ucd.ie/files/publications/greene06icml.pdf><em>Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering</em></a> paper by Derek Greene and PÃ¡draig Cunningham. The raw text files can be downloaded from <a href=http://mlg.ucd.ie/datasets/bbc.html>this</a> webpage. The documents are organized in directories, with each directory corresponding to one label. Once the data has been unzipped, it can be loaded in memory as so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pathlib</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>docs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>labels</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>directory</span> <span class=o>=</span> <span class=n>pathlib</span><span class=o>.</span><span class=n>Path</span><span class=p>(</span><span class=s1>&#39;bbc&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>label_names</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;business&#39;</span><span class=p>,</span> <span class=s1>&#39;entertainment&#39;</span><span class=p>,</span> <span class=s1>&#39;politics&#39;</span><span class=p>,</span> <span class=s1>&#39;sport&#39;</span><span class=p>,</span> <span class=s1>&#39;tech&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>label_names</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>file</span> <span class=ow>in</span> <span class=n>directory</span><span class=o>.</span><span class=n>joinpath</span><span class=p>(</span><span class=n>label</span><span class=p>)</span><span class=o>.</span><span class=n>iterdir</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>docs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>file</span><span class=o>.</span><span class=n>read_text</span><span class=p>(</span><span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;unicode_escape&#39;</span><span class=p>))</span>
</span></span></code></pre></div><p>Note that we&rsquo;re the storing the document labels, but we won&rsquo;t be using them to train a (supervised) model. On the contrary, we&rsquo;ll only be using them to evaluate our (unsupervised) method. That&rsquo;s the whole appeal of this method: it doesn&rsquo;t require you to have any labeled training data whatsoever. The first document is labeled as &ldquo;business&rdquo; and has the following content:</p><pre tabindex=0><code>&gt;&gt;&gt; doc = docs[0]
&gt;&gt;&gt; doc
UK economy facing &#39;major risks&#39;

The UK manufacturing sector will continue to face &#34;serious challenges&#34; over the next two years, the British Chamber of Commerce (BCC) has said.

The group&#39;s quarterly survey of companies found exports had picked up in the last three months of 2004 to their best levels in eight years. The rise came despite exchange rates being cited as a major concern. However, the BCC found the whole UK economy still faced &#34;major risks&#34; and warned that growth is set to slow. It recently forecast economic growth will slow from more than 3% in 2004 to a little below 2.5% in both 2005 and 2006.

Manufacturers&#39; domestic sales growth fell back slightly in the quarter, the survey of 5,196 firms found. Employment in manufacturing also fell and job expectations were at their lowest level for a year.

&#34;Despite some positive news for the export sector, there are worrying signs for manufacturing,&#34; the BCC said. &#34;These results reinforce our concern over the sector&#39;s persistent inability to sustain recovery.&#34; The outlook for the service sector was &#34;uncertain&#34; despite an increase in exports and orders over the quarter, the BCC noted.

The BCC found confidence increased in the quarter across both the manufacturing and service sectors although overall it failed to reach the levels at the start of 2004. The reduced threat of interest rate increases had contributed to improved confidence, it said. The Bank of England raised interest rates five times between November 2003 and August last year. But rates have been kept on hold since then amid signs of falling consumer confidence and a slowdown in output. &#34;The pressure on costs and margins, the relentless increase in regulations, and the threat of higher taxes remain serious problems,&#34; BCC director general David Frost said. &#34;While consumer spending is set to decelerate significantly over the next 12-18 months, it is unlikely that investment and exports will rise sufficiently strongly to pick up the slack.&#34;
</code></pre><p>The first step take is to clean the text. I wrote a simple function that does just that.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>string</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>clean_text</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>translate</span><span class=p>(</span><span class=nb>str</span><span class=o>.</span><span class=n>maketrans</span><span class=p>(</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>string</span><span class=o>.</span><span class=n>punctuation</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>,</span> <span class=s1>&#39; &#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>text</span><span class=o>.</span><span class=n>split</span><span class=p>())</span>  <span class=c1># remove multiple whitespaces</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>text</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>doc</span> <span class=o>=</span> <span class=n>clean_text</span><span class=p>(</span><span class=n>doc</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=n>doc</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>uk</span> <span class=n>economy</span> <span class=n>facing</span> <span class=n>major</span> <span class=n>risks</span> <span class=n>the</span> <span class=n>uk</span> <span class=n>manufacturing</span> <span class=n>sector</span> <span class=n>will</span> <span class=k>continue</span> <span class=n>to</span> <span class=n>face</span> <span class=n>serious</span> <span class=n>challenges</span> <span class=n>over</span> <span class=n>the</span> <span class=nb>next</span> <span class=n>two</span> <span class=n>years</span> <span class=n>the</span> <span class=n>british</span> <span class=n>chamber</span> <span class=n>of</span> <span class=n>commerce</span> <span class=n>bcc</span> <span class=n>has</span> <span class=n>said</span> <span class=n>the</span> <span class=n>groups</span> <span class=n>quarterly</span> <span class=n>survey</span> <span class=n>of</span> <span class=n>companies</span> <span class=n>found</span> <span class=n>exports</span> <span class=n>had</span> <span class=n>picked</span> <span class=n>up</span> <span class=ow>in</span> <span class=n>the</span> <span class=n>last</span> <span class=n>three</span> <span class=n>months</span> <span class=n>of</span> <span class=mi>2004</span> <span class=n>to</span> <span class=n>their</span> <span class=n>best</span> <span class=n>levels</span> <span class=ow>in</span> <span class=n>eight</span> <span class=n>years</span> <span class=n>the</span> <span class=n>rise</span> <span class=n>came</span> <span class=n>despite</span> <span class=n>exchange</span> <span class=n>rates</span> <span class=n>being</span> <span class=n>cited</span> <span class=k>as</span> <span class=n>a</span> <span class=n>major</span> <span class=n>concern</span> <span class=n>however</span> <span class=n>the</span> <span class=n>bcc</span> <span class=n>found</span> <span class=n>the</span> <span class=n>whole</span> <span class=n>uk</span> <span class=n>economy</span> <span class=n>still</span> <span class=n>faced</span> <span class=n>major</span> <span class=n>risks</span> <span class=ow>and</span> <span class=n>warned</span> <span class=n>that</span> <span class=n>growth</span> <span class=ow>is</span> <span class=nb>set</span> <span class=n>to</span> <span class=n>slow</span> <span class=n>it</span> <span class=n>recently</span> <span class=n>forecast</span> <span class=n>economic</span> <span class=n>growth</span> <span class=n>will</span> <span class=n>slow</span> <span class=kn>from</span> <span class=nn>more</span> <span class=n>than</span> <span class=mi>3</span> <span class=ow>in</span> <span class=mi>2004</span> <span class=n>to</span> <span class=n>a</span> <span class=n>little</span> <span class=n>below</span> <span class=mi>25</span> <span class=ow>in</span> <span class=n>both</span> <span class=mi>2005</span> <span class=ow>and</span> <span class=mi>2006</span> <span class=n>manufacturers</span> <span class=n>domestic</span> <span class=n>sales</span> <span class=n>growth</span> <span class=n>fell</span> <span class=n>back</span> <span class=n>slightly</span> <span class=ow>in</span> <span class=n>the</span> <span class=n>quarter</span> <span class=n>the</span> <span class=n>survey</span> <span class=n>of</span> <span class=mi>5196</span> <span class=n>firms</span> <span class=n>found</span> <span class=n>employment</span> <span class=ow>in</span> <span class=n>manufacturing</span> <span class=n>also</span> <span class=n>fell</span> <span class=ow>and</span> <span class=n>job</span> <span class=n>expectations</span> <span class=n>were</span> <span class=n>at</span> <span class=n>their</span> <span class=n>lowest</span> <span class=n>level</span> <span class=k>for</span> <span class=n>a</span> <span class=n>year</span> <span class=n>despite</span> <span class=n>some</span> <span class=n>positive</span> <span class=n>news</span> <span class=k>for</span> <span class=n>the</span> <span class=n>export</span> <span class=n>sector</span> <span class=n>there</span> <span class=n>are</span> <span class=n>worrying</span> <span class=n>signs</span> <span class=k>for</span> <span class=n>manufacturing</span> <span class=n>the</span> <span class=n>bcc</span> <span class=n>said</span> <span class=n>these</span> <span class=n>results</span> <span class=n>reinforce</span> <span class=n>our</span> <span class=n>concern</span> <span class=n>over</span> <span class=n>the</span> <span class=n>sectors</span> <span class=n>persistent</span> <span class=n>inability</span> <span class=n>to</span> <span class=n>sustain</span> <span class=n>recovery</span> <span class=n>the</span> <span class=n>outlook</span> <span class=k>for</span> <span class=n>the</span> <span class=n>service</span> <span class=n>sector</span> <span class=n>was</span> <span class=n>uncertain</span> <span class=n>despite</span> <span class=n>an</span> <span class=n>increase</span> <span class=ow>in</span> <span class=n>exports</span> <span class=ow>and</span> <span class=n>orders</span> <span class=n>over</span> <span class=n>the</span> <span class=n>quarter</span> <span class=n>the</span> <span class=n>bcc</span> <span class=n>noted</span> <span class=n>the</span> <span class=n>bcc</span> <span class=n>found</span> <span class=n>confidence</span> <span class=n>increased</span> <span class=ow>in</span> <span class=n>the</span> <span class=n>quarter</span> <span class=n>across</span> <span class=n>both</span> <span class=n>the</span> <span class=n>manufacturing</span> <span class=ow>and</span> <span class=n>service</span> <span class=n>sectors</span> <span class=n>although</span> <span class=n>overall</span> <span class=n>it</span> <span class=n>failed</span> <span class=n>to</span> <span class=n>reach</span> <span class=n>the</span> <span class=n>levels</span> <span class=n>at</span> <span class=n>the</span> <span class=n>start</span> <span class=n>of</span> <span class=mi>2004</span> <span class=n>the</span> <span class=n>reduced</span> <span class=n>threat</span> <span class=n>of</span> <span class=n>interest</span> <span class=n>rate</span> <span class=n>increases</span> <span class=n>had</span> <span class=n>contributed</span> <span class=n>to</span> <span class=n>improved</span> <span class=n>confidence</span> <span class=n>it</span> <span class=n>said</span> <span class=n>the</span> <span class=n>bank</span> <span class=n>of</span> <span class=n>england</span> <span class=n>raised</span> <span class=n>interest</span> <span class=n>rates</span> <span class=n>five</span> <span class=n>times</span> <span class=n>between</span> <span class=n>november</span> <span class=mi>2003</span> <span class=ow>and</span> <span class=n>august</span> <span class=n>last</span> <span class=n>year</span> <span class=n>but</span> <span class=n>rates</span> <span class=n>have</span> <span class=n>been</span> <span class=n>kept</span> <span class=n>on</span> <span class=n>hold</span> <span class=n>since</span> <span class=n>then</span> <span class=n>amid</span> <span class=n>signs</span> <span class=n>of</span> <span class=n>falling</span> <span class=n>consumer</span> <span class=n>confidence</span> <span class=ow>and</span> <span class=n>a</span> <span class=n>slowdown</span> <span class=ow>in</span> <span class=n>output</span> <span class=n>the</span> <span class=n>pressure</span> <span class=n>on</span> <span class=n>costs</span> <span class=ow>and</span> <span class=n>margins</span> <span class=n>the</span> <span class=n>relentless</span> <span class=n>increase</span> <span class=ow>in</span> <span class=n>regulations</span> <span class=ow>and</span> <span class=n>the</span> <span class=n>threat</span> <span class=n>of</span> <span class=n>higher</span> <span class=n>taxes</span> <span class=n>remain</span> <span class=n>serious</span> <span class=n>problems</span> <span class=n>bcc</span> <span class=n>director</span> <span class=n>general</span> <span class=n>david</span> <span class=n>frost</span> <span class=n>said</span> <span class=k>while</span> <span class=n>consumer</span> <span class=n>spending</span> <span class=ow>is</span> <span class=nb>set</span> <span class=n>to</span> <span class=n>decelerate</span> <span class=n>significantly</span> <span class=n>over</span> <span class=n>the</span> <span class=nb>next</span> <span class=mi>1218</span> <span class=n>months</span> <span class=n>it</span> <span class=ow>is</span> <span class=n>unlikely</span> <span class=n>that</span> <span class=n>investment</span> <span class=ow>and</span> <span class=n>exports</span> <span class=n>will</span> <span class=n>rise</span> <span class=n>sufficiently</span> <span class=n>strongly</span> <span class=n>to</span> <span class=n>pick</span> <span class=n>up</span> <span class=n>the</span> <span class=n>slack</span>
</span></span></code></pre></div><p>As you can see, I&rsquo;ve removed punctuation marks, removed unnecessary whitespace, got rid of carriage returns, and lowercased all the text. The one thing I haven&rsquo;t taken care of is spelling mistakes. Indeed, if a word is misspelt, then we won&rsquo;t be able to match it with a word vector. Indeed word vectors are essentially discrete dictionaries that map a word to a vector. This dataset has supposedly been scrapped from the <a href=https://www.bbc.com/>BBC website</a>, and thus shouldn&rsquo;t contain too many typos. But you never know! A quick win would be to apply <a href=https://norvig.com/spell-correct.html>Peter Norvig&rsquo;s spelling corrector</a>, however that goes beyond the scope of this article.</p><p>I&rsquo;m going to be using <a href=https://spacy.io/>spaCy</a> for manipulating word embeddings. I&rsquo;ve decided to use the <a href=https://spacy.io/models/en#en_core_web_lg><code>en_core_web_lg</code></a> embeddings, which contains <a href=https://www.wikiwand.com/en/Word2vec>Word2vec</a> embeddings that were fitted on <a href=https://www.wikiwand.com/en/Common_Crawl>Common Crawl</a> data. The embeddings can be downloaded from the command-line as so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ python -m spacy download en_core_web_lg
</span></span></code></pre></div><p>Note that you could use any pre-trained word embeddings, including <code>en_core_web_sm</code> and <code>en_core_web_md</code>, which are smaller variants of <code>en_core_web_lg</code>. The fastText embeddings that I mentionned above would work too. Naturally, the performance of this method is going to be highly dependent on the quality of the word embeddings, as well as their adequacy with the dataset at hand. I&rsquo;ll get back to this point later on.</p><p>The word vectors can be opened with a one-liner:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>spacy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>nlp</span> <span class=o>=</span> <span class=n>spacy</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;en_core_web_lg&#39;</span><span class=p>)</span>
</span></span></code></pre></div><p>I&rsquo;m now going to define an <code>embed</code> function, which takes as input some tokens &ndash; or words, if you prefer &ndash; and outputs the centroid of their respective embeddings. I&rsquo;m going to filter out tokens that are unknown, as well as <a href=https://www.wikiwand.com/en/Stop_word>stop words</a> and those that contain a single character.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>embed</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>nlp</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Return the centroid of the embeddings for the given tokens.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Out-of-vocabulary tokens are cast aside. Stop words are also
</span></span></span><span class=line><span class=cl><span class=s2>    discarded. An array of 0s is returned if none of the tokens
</span></span></span><span class=line><span class=cl><span class=s2>    are valid.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>lexemes</span> <span class=o>=</span> <span class=p>(</span><span class=n>nlp</span><span class=o>.</span><span class=n>vocab</span><span class=p>[</span><span class=n>token</span><span class=p>]</span> <span class=k>for</span> <span class=n>token</span> <span class=ow>in</span> <span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>vectors</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>([</span>
</span></span><span class=line><span class=cl>        <span class=n>lexeme</span><span class=o>.</span><span class=n>vector</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>lexeme</span> <span class=ow>in</span> <span class=n>lexemes</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>lexeme</span><span class=o>.</span><span class=n>has_vector</span>
</span></span><span class=line><span class=cl>        <span class=ow>and</span> <span class=ow>not</span> <span class=n>lexeme</span><span class=o>.</span><span class=n>is_stop</span>
</span></span><span class=line><span class=cl>        <span class=ow>and</span> <span class=nb>len</span><span class=p>(</span><span class=n>lexeme</span><span class=o>.</span><span class=n>text</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>vectors</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>centroid</span> <span class=o>=</span> <span class=n>vectors</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>width</span> <span class=o>=</span> <span class=n>nlp</span><span class=o>.</span><span class=n>meta</span><span class=p>[</span><span class=s1>&#39;vectors&#39;</span><span class=p>][</span><span class=s1>&#39;width&#39;</span><span class=p>]</span>  <span class=c1># typically 300</span>
</span></span><span class=line><span class=cl>        <span class=n>centroid</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>width</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>centroid</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>tokens</span> <span class=o>=</span> <span class=n>doc</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>centroid</span> <span class=o>=</span> <span class=n>embed</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>nlp</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>centroid</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>300</span><span class=p>,)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>centroid</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>array</span><span class=p>([</span><span class=o>-</span><span class=mf>0.25490612</span><span class=p>,</span>  <span class=mf>0.3020584</span> <span class=p>,</span>  <span class=mf>0.06464471</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.02984463</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.00753056</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=o>-</span><span class=mf>0.15310128</span><span class=p>,</span>  <span class=mf>0.00843642</span><span class=p>,</span>  <span class=mf>0.04468439</span><span class=p>,</span>  <span class=mf>0.02751189</span><span class=p>,</span>  <span class=mf>2.1646047</span> <span class=p>],</span>
</span></span><span class=line><span class=cl>      <span class=n>dtype</span><span class=o>=</span><span class=n>float32</span><span class=p>)</span>
</span></span></code></pre></div><p>We&rsquo;re nearly done! The last piece of the puzzle is a mechanism to find the closest label for a given centroid. For this I decided to use the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html><code>NearestNeighbors</code></a> class from scikit-learn. But first, I have to extract the embeddings for each label, which can be done by simply reusing the <code>embed</code> function. This makes sense, because our goal is to project the document and the labels in the same domain.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>label_vectors</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>([</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>embed</span><span class=p>(</span><span class=n>label</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=p>),</span> <span class=n>nlp</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=k>for</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>label_names</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=p>])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>label_vectors</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>300</span><span class=p>)</span>
</span></span></code></pre></div><p>Note that <code>embed</code> expects to be provided with a list of tokens as a first input. In our case, each label is made up of one single word, thus <code>label.split(' ')</code> is equivalent to <code>[label]</code>. If you&rsquo;re in a situation where a label is made up of multiple words, then you&rsquo;ll want to split it appropriately instead of treating it as a single word. Anyway, moving on, here&rsquo;s the code for fitting the nearest neighbors data structure, and searching for the closest label:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>neigh</span> <span class=o>=</span> <span class=n>neighbors</span><span class=o>.</span><span class=n>NearestNeighbors</span><span class=p>(</span><span class=n>n_neighbors</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>neigh</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>label_vectors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>closest_label</span> <span class=o>=</span> <span class=n>neigh</span><span class=o>.</span><span class=n>kneighbors</span><span class=p>([</span><span class=n>centroid</span><span class=p>],</span> <span class=n>return_distance</span><span class=o>=</span><span class=kc>False</span><span class=p>)[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>label_names</span><span class=p>[</span><span class=n>closest_label</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;business&#39;</span>
</span></span></code></pre></div><p>Hurray, our document got classified correctly! I think that this is quite cool, considering that we didn&rsquo;t train any supervised model. But I&rsquo;ve already said that. The $64.000 question, of course, is how well does this perform? Well we can easily process each document with a list comprehension:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>doc</span><span class=p>,</span> <span class=n>nlp</span><span class=p>,</span> <span class=n>neigh</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>doc</span> <span class=o>=</span> <span class=n>clean_text</span><span class=p>(</span><span class=n>doc</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens</span> <span class=o>=</span> <span class=n>doc</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=p>)[:</span><span class=mi>50</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>centroid</span> <span class=o>=</span> <span class=n>embed</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>nlp</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>closest_label</span> <span class=o>=</span> <span class=n>neigh</span><span class=o>.</span><span class=n>kneighbors</span><span class=p>([</span><span class=n>centroid</span><span class=p>],</span> <span class=n>return_distance</span><span class=o>=</span><span class=kc>False</span><span class=p>)[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>closest_label</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>preds</span> <span class=o>=</span> <span class=p>[</span><span class=n>label_names</span><span class=p>[</span><span class=n>predict</span><span class=p>(</span><span class=n>doc</span><span class=p>,</span> <span class=n>nlp</span><span class=p>,</span> <span class=n>neigh</span><span class=p>)]</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>docs</span><span class=p>]</span>
</span></span></code></pre></div><p>This runs in ~2 seconds on my laptop &ndash; note that there are 2,225 documents. We can now use scikit-learn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html><code>classification_report</code></a> to assess the predictive power of our method:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>metrics</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>report</span> <span class=o>=</span> <span class=n>metrics</span><span class=o>.</span><span class=n>classification_report</span><span class=p>(</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>y_true</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>y_pred</span><span class=o>=</span><span class=n>preds</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>labels</span><span class=o>=</span><span class=n>label_names</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=n>report</span><span class=p>)</span>
</span></span><span class=line><span class=cl>               <span class=n>precision</span>    <span class=n>recall</span>  <span class=n>f1</span><span class=o>-</span><span class=n>score</span>   <span class=n>support</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     <span class=n>business</span>       <span class=mf>0.39</span>      <span class=mf>0.99</span>      <span class=mf>0.56</span>       <span class=mi>510</span>
</span></span><span class=line><span class=cl><span class=n>entertainment</span>       <span class=mf>0.82</span>      <span class=mf>0.52</span>      <span class=mf>0.63</span>       <span class=mi>386</span>
</span></span><span class=line><span class=cl>     <span class=n>politics</span>       <span class=mf>0.90</span>      <span class=mf>0.41</span>      <span class=mf>0.56</span>       <span class=mi>417</span>
</span></span><span class=line><span class=cl>        <span class=n>sport</span>       <span class=mf>0.94</span>      <span class=mf>0.77</span>      <span class=mf>0.84</span>       <span class=mi>511</span>
</span></span><span class=line><span class=cl>         <span class=n>tech</span>       <span class=mf>0.52</span>      <span class=mf>0.10</span>      <span class=mf>0.16</span>       <span class=mi>401</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     <span class=n>accuracy</span>                           <span class=mf>0.59</span>      <span class=mi>2225</span>
</span></span><span class=line><span class=cl>    <span class=n>macro</span> <span class=n>avg</span>       <span class=mf>0.71</span>      <span class=mf>0.56</span>      <span class=mf>0.55</span>      <span class=mi>2225</span>
</span></span><span class=line><span class=cl> <span class=n>weighted</span> <span class=n>avg</span>       <span class=mf>0.71</span>      <span class=mf>0.59</span>      <span class=mf>0.57</span>      <span class=mi>2225</span>
</span></span></code></pre></div><p>The performance is not stellar, but it is significantly better than random. One thing I thought about is using a different distance metric when looking for the closest label of each centroid. By default, the <code>NearestNeighbors</code> class uses the <a href=https://www.wikiwand.com/en/Euclidean_distance>Euclidean distance</a>, but nothing is stopping us from using something more exotic, such as <a href=https://www.wikiwand.com/en/Cosine_similarity>cosine similarity</a>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=kn>from</span> <span class=nn>scipy</span> <span class=kn>import</span> <span class=n>spatial</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>neigh</span> <span class=o>=</span> <span class=n>neighbors</span><span class=o>.</span><span class=n>NearestNeighbors</span><span class=p>(</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>n_neighbors</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>metric</span><span class=o>=</span><span class=n>spatial</span><span class=o>.</span><span class=n>distance</span><span class=o>.</span><span class=n>cosine</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>neigh</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>label_vectors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>preds</span> <span class=o>=</span> <span class=p>[</span><span class=n>label_names</span><span class=p>[</span><span class=n>predict</span><span class=p>(</span><span class=n>doc</span><span class=p>,</span> <span class=n>nlp</span><span class=p>,</span> <span class=n>neigh</span><span class=p>)]</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>docs</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>report</span> <span class=o>=</span> <span class=n>metrics</span><span class=o>.</span><span class=n>classification_report</span><span class=p>(</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>y_true</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>y_pred</span><span class=o>=</span><span class=n>preds</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>labels</span><span class=o>=</span><span class=n>label_names</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=n>report</span><span class=p>)</span>
</span></span><span class=line><span class=cl>               <span class=n>precision</span>    <span class=n>recall</span>  <span class=n>f1</span><span class=o>-</span><span class=n>score</span>   <span class=n>support</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     <span class=n>business</span>       <span class=mf>0.50</span>      <span class=mf>0.91</span>      <span class=mf>0.64</span>       <span class=mi>510</span>
</span></span><span class=line><span class=cl><span class=n>entertainment</span>       <span class=mf>0.76</span>      <span class=mf>0.69</span>      <span class=mf>0.72</span>       <span class=mi>386</span>
</span></span><span class=line><span class=cl>     <span class=n>politics</span>       <span class=mf>0.70</span>      <span class=mf>0.74</span>      <span class=mf>0.72</span>       <span class=mi>417</span>
</span></span><span class=line><span class=cl>        <span class=n>sport</span>       <span class=mf>0.93</span>      <span class=mf>0.85</span>      <span class=mf>0.89</span>       <span class=mi>511</span>
</span></span><span class=line><span class=cl>         <span class=n>tech</span>       <span class=mf>0.85</span>      <span class=mf>0.05</span>      <span class=mf>0.10</span>       <span class=mi>401</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     <span class=n>accuracy</span>                           <span class=mf>0.67</span>      <span class=mi>2225</span>
</span></span><span class=line><span class=cl>    <span class=n>macro</span> <span class=n>avg</span>       <span class=mf>0.75</span>      <span class=mf>0.65</span>      <span class=mf>0.62</span>      <span class=mi>2225</span>
</span></span><span class=line><span class=cl> <span class=n>weighted</span> <span class=n>avg</span>       <span class=mf>0.74</span>      <span class=mf>0.67</span>      <span class=mf>0.63</span>      <span class=mi>2225</span>
</span></span></code></pre></div><p>The overall performance went up by a significant margin, even though we only changed the distance metric. There&rsquo;s obviously a lot of other things that could be improved. For instance, an interesting observation is that the performance for the &ldquo;tech&rdquo; label is very weak. This is pure speculation, but I assume that this is because the word embeddings that we&rsquo;re using were not trained on a lot of articles pertaining to technology. Moreover, it could be that the tech articles at our disposal contain a lot of niche terms that do not occur in the <code>en_core_web_lg</code> embeddings.</p><p>There is one simple trick we can apply to improve the performance for the &ldquo;tech&rdquo; label. The trick is to use the term &ldquo;technology&rdquo; instead. Indeed, &ldquo;technology&rdquo; is more likely to be used than &ldquo;tech&rdquo; in a sentence, and thus might have a better representation in the embedded vector space. This may be done as so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>label_names</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=s1>&#39;business&#39;</span><span class=p>,</span> <span class=s1>&#39;entertainment&#39;</span><span class=p>,</span> <span class=s1>&#39;politics&#39;</span><span class=p>,</span> <span class=s1>&#39;sport&#39;</span><span class=p>,</span> <span class=s1>&#39;tech&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>replacements</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;tech&#39;</span><span class=p>:</span> <span class=s1>&#39;technology&#39;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>new_label_names</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>replacements</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>label</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=k>for</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>label_names</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>new_label_names</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=s1>&#39;business&#39;</span><span class=p>,</span> <span class=s1>&#39;entertainment&#39;</span><span class=p>,</span> <span class=s1>&#39;politics&#39;</span><span class=p>,</span> <span class=s1>&#39;sport&#39;</span><span class=p>,</span> <span class=s1>&#39;technology&#39;</span><span class=p>]</span>
</span></span></code></pre></div><p>The label vectors may now be rebuilt, and the benchmark can be run once more.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>label_vectors</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>([</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>embed</span><span class=p>(</span><span class=n>name</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=p>),</span> <span class=n>nlp</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=k>for</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>new_label_names</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>neigh</span> <span class=o>=</span> <span class=n>neighbors</span><span class=o>.</span><span class=n>NearestNeighbors</span><span class=p>(</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>n_neighbors</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>metric</span><span class=o>=</span><span class=n>spatial</span><span class=o>.</span><span class=n>distance</span><span class=o>.</span><span class=n>cosine</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>neigh</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>label_vectors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>preds</span> <span class=o>=</span> <span class=p>[</span><span class=n>label_names</span><span class=p>[</span><span class=n>predict</span><span class=p>(</span><span class=n>doc</span><span class=p>,</span> <span class=n>nlp</span><span class=p>,</span> <span class=n>neigh</span><span class=p>)]</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>docs</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>report</span> <span class=o>=</span> <span class=n>metrics</span><span class=o>.</span><span class=n>classification_report</span><span class=p>(</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>y_true</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>y_pred</span><span class=o>=</span><span class=n>preds</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=o>...</span>     <span class=n>labels</span><span class=o>=</span><span class=n>label_names</span>
</span></span><span class=line><span class=cl><span class=o>...</span> <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=n>report</span><span class=p>)</span>
</span></span><span class=line><span class=cl>               <span class=n>precision</span>    <span class=n>recall</span>  <span class=n>f1</span><span class=o>-</span><span class=n>score</span>   <span class=n>support</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     <span class=n>business</span>       <span class=mf>0.54</span>      <span class=mf>0.91</span>      <span class=mf>0.68</span>       <span class=mi>510</span>
</span></span><span class=line><span class=cl><span class=n>entertainment</span>       <span class=mf>0.78</span>      <span class=mf>0.69</span>      <span class=mf>0.73</span>       <span class=mi>386</span>
</span></span><span class=line><span class=cl>     <span class=n>politics</span>       <span class=mf>0.69</span>      <span class=mf>0.74</span>      <span class=mf>0.72</span>       <span class=mi>417</span>
</span></span><span class=line><span class=cl>        <span class=n>sport</span>       <span class=mf>0.93</span>      <span class=mf>0.85</span>      <span class=mf>0.89</span>       <span class=mi>511</span>
</span></span><span class=line><span class=cl>         <span class=n>tech</span>       <span class=mf>0.95</span>      <span class=mf>0.27</span>      <span class=mf>0.42</span>       <span class=mi>401</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>     <span class=n>accuracy</span>                           <span class=mf>0.71</span>      <span class=mi>2225</span>
</span></span><span class=line><span class=cl>    <span class=n>macro</span> <span class=n>avg</span>       <span class=mf>0.78</span>      <span class=mf>0.69</span>      <span class=mf>0.69</span>      <span class=mi>2225</span>
</span></span><span class=line><span class=cl> <span class=n>weighted</span> <span class=n>avg</span>       <span class=mf>0.77</span>      <span class=mf>0.71</span>      <span class=mf>0.70</span>      <span class=mi>2225</span>
</span></span></code></pre></div><p>It works! This goes to show that the exact text used for each label is important. I&rsquo;m pretty sure there are loads of other tricks to try out. For instance, if you think about it, we&rsquo;re attempting to classify news articles from the BBC website with word embeddings that were fitted on Common Crawl data, which is quite generic. It seems fairly obvious that a big boost would come from using word embeddings that are trained on similar documents to those that we want to classify. This could be done in a couple of ways, either by training a word embedding model from scratch on our documents, or by fine-tuning an existing set of embeddings. Another thing that is worth paying attention to is the manner in which we compute a document&rsquo;s centroid. In the current implementation, the tokens contribute equally to the centroid. One could imagine a different ponderation scheme where tokens that appear in many documents contribute less towards the centroid. I recommend checking out <a href=http://www.offconvex.org/2018/06/17/textembeddings/>this blog post</a> by <a href=https://www.cs.princeton.edu/~arora/>Sanjeev Arora</a> for further discussion along this path.</p><p>I&rsquo;m not an NLP expert, nor am I particularly fond of it to be honest, so I&rsquo;m going to leave it at that. The point of this article was mostly to spark an idea and to nicely present it to my friend. If you have some input or have experience working with word embeddings in such a manner, then please leave a comment. If not, keep lurking.</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px;margin-bottom:30px><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>
Back to the top</div></div><iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=32 width=114 style=border:0;border-radius:6px></iframe></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script><script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href=/blog/machine-learning-production/>A smooth approach to putting machine learning into production</a></li><li><a href=/blog/speeding-up-sklearn-single-predictions/>Speeding up scikit-learn for single predictions</a></li><li><a href=/blog/bayesian-linear-regression/>Bayesian linear regression for practitioners</a></li></ul></div></div></div></article></body></html>