<!doctype html><html lang=en><head><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogImMwMjJhMjg1LWJmY2YtNDc0OC1hYzczLTJhMDQ1Njk3NTI0YyIsICJpZCI6ICIzNjc3NjQ3Ny04MTE2LTRmYWQtYjcwMy1iZmM3YjMwZGJjMjMifQ.A0vHm-VWbXG6uBFZiwuspN_AyfSYNrdZE3IgwgWSt4g></script><meta charset=utf-8><meta name=generator content="Hugo 0.119.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/matrix-inverse-mini-batch/"><link rel=canonical href=https://maxhalford.github.io/blog/matrix-inverse-mini-batch/><meta property="og:title" content="Matrix inverse mini-batch updates"><meta property="og:description" content="The inverse covariance matrix, also called precision matrix, is useful in many places across the field of statistics. For instance, in machine learning, it is used for Bayesian regression and mixture modelling.
What&rsquo;s interesting is that any batch model which uses a precision matrix can be turned into an online model. That is, provided the precision matrix can be estimated in a streaming fashion. For instance, scikit-learn&rsquo;s elliptic envelope method could have an online variant with a partial_fit method."><meta property="og:type" content="article"><meta property="og:url" content="https://maxhalford.github.io/blog/matrix-inverse-mini-batch/"><meta property="og:image" content="https://maxhalford.github.io/img/beach.jpg"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-08-24T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-24T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://maxhalford.github.io/img/beach.jpg"><meta name=twitter:title content="Matrix inverse mini-batch updates"><meta name=twitter:description content="The inverse covariance matrix, also called precision matrix, is useful in many places across the field of statistics. For instance, in machine learning, it is used for Bayesian regression and mixture modelling.
What&rsquo;s interesting is that any batch model which uses a precision matrix can be turned into an online model. That is, provided the precision matrix can be estimated in a streaming fashion. For instance, scikit-learn&rsquo;s elliptic envelope method could have an online variant with a partial_fit method."><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🦔</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Matrix inverse mini-batch updates","headline":"Matrix inverse mini-batch updates","description":"The inverse covariance matrix, also called precision matrix, is useful in many places across the field of statistics. For instance, in machine learning, it is used for Bayesian regression and mixture modelling.\nWhat\u0026rsquo;s interesting is that any batch model which uses a precision matrix can be turned into an online model. That is, provided the precision matrix can be estimated in a streaming fashion. For instance, scikit-learn\u0026rsquo;s elliptic envelope method could have an online variant with a partial_fit method.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2022","datePublished":"2022-08-24 00:00:00 \u002b0000 UTC","dateModified":"2022-08-24 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/matrix-inverse-mini-batch\/","keywords":["online-machine-learning"]}</script><title>Matrix inverse mini-batch updates • Max Halford</title><meta property="og:title" content="Matrix inverse mini-batch updates • Max Halford"><meta property="og:type" content="article"><meta name=description content="The inverse covariance matrix, also called precision matrix, is useful in many places across the field of statistics. For instance, in machine learning, it is used for Bayesian regression and mixture modelling.
What&rsquo;s interesting is that any batch model which uses a precision matrix can be turned into an online model. That is, provided the precision matrix can be estimated in a streaming fashion. For instance, scikit-learn&rsquo;s elliptic envelope method could have an online variant with a partial_fit method."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford 🦔</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>Matrix inverse mini-batch updates</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2022-08-24 00:00:00 UTC">2022-08-24</time>
<span class=posts-line-tag>online-machine-learning</span></div></div></header><div class="post-content markdown-body"><p>The inverse covariance matrix, also called <a href=https://www.wikiwand.com/en/Precision_matrix>precision matrix</a>, is useful in many places across the field of statistics. For instance, in machine learning, it is used for <a href=/blog/bayesian-linear-regression>Bayesian regression</a> and <a href=https://scikit-learn.org/stable/modules/mixture.html#gmm>mixture modelling</a>.</p><p>What&rsquo;s interesting is that any batch model which uses a precision matrix can be turned into an online model. That is, provided the precision matrix can be estimated in a streaming fashion. For instance, scikit-learn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope>elliptic envelope</a> method could have an online variant with a <code>partial_fit</code> method.</p><p>Thankfully, there is a way to (efficiently) estimate a precision matrix online, through the use of the <a href=https://www.wikiwand.com/en/Sherman%E2%80%93Morrison_formula>Sherman-Morrison formula</a>. Markus Thill provides details <a href=https://markusthill.github.io/math/stats/ml/online-estimation-of-the-inverse-covariance-matrix/>here</a> with some accompanying R code.</p><p>Of course, one could just estimate the covariance matrix online, and invert the matrix at each step. But that would be too expensive, due to the fact matrix inversion takes $\mathcal{O}(n^3)$ time. The Sherman-Morrison formula runs in $\mathcal{O}(n^2)$ time. The downside is that the result is not exact, although the error margin is small enough for machine learning purposes &ndash; at least in my experience.</p><p>Here is some Python code for estimating the precision matrix, online, using the Sherman-Morrison formula:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>scipy</span> <span class=k>as</span> <span class=nn>sp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sherman_morrison</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>u</span><span class=p>,</span> <span class=n>v</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>Au</span> <span class=o>=</span> <span class=n>A</span> <span class=o>@</span> <span class=n>u</span>
</span></span><span class=line><span class=cl>    <span class=n>vT</span> <span class=o>=</span> <span class=n>v</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>    <span class=n>alpha</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>vT</span> <span class=o>@</span> <span class=n>Au</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sp</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>blas</span><span class=o>.</span><span class=n>dger</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>alpha</span><span class=o>=</span><span class=n>alpha</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=o>=</span><span class=n>Au</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span><span class=o>=</span><span class=n>vT</span> <span class=o>@</span> <span class=n>A</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>a</span><span class=o>=</span><span class=n>A</span><span class=p>,</span> <span class=n>overwrite_a</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>p</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=n>rng</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>default_rng</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=p>(</span><span class=n>n</span><span class=p>,</span> <span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>M</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asfortranarray</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>X</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>mean</span>
</span></span><span class=line><span class=cl>    <span class=n>mean</span> <span class=o>+=</span> <span class=n>diff</span> <span class=o>/</span> <span class=n>w</span>
</span></span><span class=line><span class=cl>    <span class=n>sherman_morrison</span><span class=p>(</span><span class=n>A</span><span class=o>=</span><span class=n>M</span><span class=p>,</span> <span class=n>u</span><span class=o>=</span><span class=n>diff</span><span class=p>,</span> <span class=n>v</span><span class=o>=</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>inv_cov</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>*</span> <span class=n>M</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>inv_cov</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>array([[ 1.22900496, -0.04588473, -0.01030462],
       [-0.04588473,  1.08510258, -0.20780225],
       [-0.01030462, -0.20780225,  1.22256802]])
</code></pre><p>As you can compare, this is somewhat close to the basic batch implementation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>cov</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>T</span><span class=p>)))</span>
</span></span></code></pre></div><pre tabindex=0><code>array([[ 1.23187713, -0.04647389, -0.01035858],
       [-0.04647389,  1.08650005, -0.21055135],
       [-0.01035858, -0.21055135,  1.22576677]])
</code></pre><p>The inplace <code>sherman_morrison</code> function is a bit cryptic. This is because it&rsquo;s using the <a href=https://netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga458222e01b4d348e9b52b9343d52f828.html#ga458222e01b4d348e9b52b9343d52f828>DGER</a> routine, which is nicely exposed by SciPy. This requires the input to be in F(ortran) order &ndash; don&rsquo;t ask me why &ndash; which explains the call to <code>np.asfortranarray</code>.</p><p>I didn&rsquo;t invent this trick, I got it from Tim Vieira&rsquo;s <a href=https://timvieira.github.io/blog/post/2021/03/25/fast-rank-one-updates-to-matrix-inverse/>excellent article</a> about optimizing the Sherman-Morrison formula for NumPy.</p><p>The Sherman-Morrison formula is great, but processing $n$ samples with $p$ features still requires a non-negligible $\mathcal{O}(np^2)$ amount of computing time. That may be prohibitive, depending on the use case. This is emphasized in a mini-batch scenario &ndash; think scikit-learn&rsquo;s <code>partial_fit</code> &ndash; where hardware acceleration can be used to process a batch of data faster than one sample at a time.</p><p>As it just so happens, the Sherman-Morrison formula is a special case of the <a href=https://www.wikiwand.com/en/Woodbury_matrix_identity>Woodbury matrix identity</a>. Here it is, implemented in NumPy:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>woodbury_identity</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>U</span><span class=p>,</span> <span class=n>V</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>I</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>V</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>AU</span> <span class=o>=</span> <span class=n>A</span> <span class=o>@</span> <span class=n>U</span>
</span></span><span class=line><span class=cl>    <span class=n>A</span> <span class=o>-=</span> <span class=n>AU</span> <span class=o>@</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>I</span> <span class=o>+</span> <span class=n>V</span> <span class=o>@</span> <span class=n>AU</span><span class=p>)</span> <span class=o>@</span> <span class=n>V</span> <span class=o>@</span> <span class=n>A</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>M</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asfortranarray</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>Xb</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=n>Xb</span> <span class=o>-</span> <span class=n>mean</span>
</span></span><span class=line><span class=cl>    <span class=n>mean</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=n>w</span> <span class=o>*</span> <span class=n>mean</span> <span class=o>+</span> <span class=nb>len</span><span class=p>(</span><span class=n>Xb</span><span class=p>)</span> <span class=o>*</span> <span class=n>Xb</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span> <span class=o>/</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=n>w</span> <span class=o>+</span> <span class=nb>len</span><span class=p>(</span><span class=n>Xb</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span> <span class=o>+=</span> <span class=nb>len</span><span class=p>(</span><span class=n>Xb</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>woodbury_identity</span><span class=p>(</span><span class=n>A</span><span class=o>=</span><span class=n>M</span><span class=p>,</span> <span class=n>U</span><span class=o>=</span><span class=n>diff</span><span class=o>.</span><span class=n>T</span><span class=p>,</span> <span class=n>V</span><span class=o>=</span><span class=n>Xb</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>inv_cov</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>*</span> <span class=n>M</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>inv_cov</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>array([[ 1.22900496, -0.04588473, -0.01030462],
       [-0.04588473,  1.08510258, -0.20780225],
       [-0.01030462, -0.20780225,  1.22256802]])
</code></pre><p>The results are identical to the single-instance version. Here I&rsquo;ve split the <code>X</code> matrix into 4 batches of 25 rows. This results in having only to make 4 calls to <code>woodbury_identity</code>, instead of 100 calls to <code>sherman_morrison</code>. Of course, this doesn&rsquo;t necessarily mean the former method is faster. Especially considering the Woodbury matrix identity still requires performing one matrix inversion. The latter applies to the <code>V @ AU</code> matrix, which is of shape <code>(k, k)</code>, with <code>k</code> being the length of each mini-batch.</p><p>Of course, why try to guess when we can measure. We can easily measure how much faster/slower the mini-batch method is compared to the streaming method for different values of <code>k</code> (number of samples) and <code>p</code> (number of features).</p><details><summary>Benchmark code</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>streaming</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>M</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asfortranarray</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>X</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>w</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=n>diff</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>mean</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span> <span class=o>+=</span> <span class=n>diff</span> <span class=o>/</span> <span class=n>w</span>
</span></span><span class=line><span class=cl>        <span class=n>sherman_morrison</span><span class=p>(</span><span class=n>A</span><span class=o>=</span><span class=n>M</span><span class=p>,</span> <span class=n>u</span><span class=o>=</span><span class=n>diff</span><span class=p>,</span> <span class=n>v</span><span class=o>=</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mini_batch</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>M</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asfortranarray</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=n>X</span> <span class=o>-</span> <span class=n>mean</span>
</span></span><span class=line><span class=cl>    <span class=n>mean</span> <span class=o>=</span> <span class=p>(</span><span class=n>w</span> <span class=o>*</span> <span class=n>mean</span> <span class=o>+</span> <span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>*</span> <span class=n>X</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>w</span> <span class=o>+</span> <span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span> <span class=o>+=</span> <span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>woodbury_matrix</span><span class=p>(</span><span class=n>A</span><span class=o>=</span><span class=n>M</span><span class=p>,</span> <span class=n>U</span><span class=o>=</span><span class=n>diff</span><span class=o>.</span><span class=n>T</span><span class=p>,</span> <span class=n>V</span><span class=o>=</span><span class=n>X</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>workload</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>rng</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>default_rng</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span> <span class=o>=</span> <span class=p>[</span><span class=mi>2</span> <span class=o>**</span> <span class=n>k</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>13</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=n>P</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ceil</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>logspace</span><span class=p>(</span><span class=mf>0.3</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>K</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>P</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>X</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=k>yield</span> <span class=n>X</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>run</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>X</span> <span class=ow>in</span> <span class=n>workload</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span><span class=p>,</span> <span class=n>p</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=n>t_streaming</span> <span class=o>=</span> <span class=o>%</span><span class=n>timeit</span> <span class=o>-</span><span class=n>o</span> <span class=o>-</span><span class=n>q</span> <span class=n>streaming</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>t_mini_batch</span> <span class=o>=</span> <span class=o>%</span><span class=n>timeit</span> <span class=o>-</span><span class=n>o</span> <span class=o>-</span><span class=n>q</span> <span class=n>mini_batch</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>yield</span> <span class=n>k</span><span class=p>,</span> <span class=n>p</span><span class=p>,</span> <span class=n>t_streaming</span><span class=o>.</span><span class=n>average</span> <span class=o>/</span> <span class=n>t_mini_batch</span><span class=o>.</span><span class=n>average</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s1>&#39;k&#39;</span><span class=p>:</span> <span class=n>k</span><span class=p>,</span> <span class=s1>&#39;p&#39;</span><span class=p>:</span> <span class=n>p</span><span class=p>,</span> <span class=s1>&#39;speed_up&#39;</span><span class=p>:</span> <span class=n>speed_up</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>p</span><span class=p>,</span> <span class=n>speed_up</span> <span class=ow>in</span> <span class=n>run</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>results</span><span class=p>)</span><span class=o>.</span><span class=n>to_csv</span><span class=p>(</span><span class=s1>&#39;results.csv&#39;</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span></code></pre></div></details><details><summary>Visualization code</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;results.csv&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>pivot</span><span class=p>(</span><span class=s1>&#39;n&#39;</span><span class=p>,</span> <span class=s1>&#39;p&#39;</span><span class=p>,</span> <span class=s1>&#39;speed_up&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>sort_index</span><span class=p>(</span><span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=n>font_scale</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>df</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;Spectral_r&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>center</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1>#square=True,</span>
</span></span><span class=line><span class=cl>    <span class=n>linewidth</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>annot</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>fmt</span><span class=o>=</span><span class=s1>&#39;.2f&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>annot_kws</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;fontsize&#39;</span><span class=p>:</span> <span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;fontweight&#39;</span><span class=p>:</span> <span class=s1>&#39;bold&#39;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cbar</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;batch size&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;number of features&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;speed-up.svg&#39;</span><span class=p>,</span> <span class=n>bbox_inches</span><span class=o>=</span><span class=s1>&#39;tight&#39;</span><span class=p>)</span>
</span></span></code></pre></div></details><div align=center><figure><img src=/img/blog/matrix-inverse-mini-batch/speed-up.svg style=box-shadow:none><figcaption><i>Speed-up summary. A <span style=color:#cd5c5c>red</span> value means the mini-batch version is faster, whereas a <span style=color:#adff2f>green</span> value means the streaming version is preferable.</i></figcaption></figure></div><p>The interpretation of this benchmark is a bit disappointing, albeit interesting. The mini-batch variant is faster for batches of length around 100. In practice, it might be possible to devise some heuristic based on <code>(k, p)</code> to select the fastest method, but that seems to me a bit far-fetched. There&rsquo;s no free lunch!</p><h2 id=appendix>Appendix</h2><p>The fastest solution in Tim Vieira&rsquo;s article uses the <a href=https://netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga458222e01b4d348e9b52b9343d52f828.html#ga458222e01b4d348e9b52b9343d52f828>DGER</a> operator to speed things up. This is a level 2 linear algebra operation, because it performs matrix-vector operations. The Woodbury matrix identity necessitates matrix-matrix operations, and is thus a level 3 operation. From what I could tell, the <a href=https://netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gaeda3cbd99c8fb834a60a6412878226e1.html>DGEMM</a> operator is appropriate. I managed to make this work, as so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>woodbury_matrix</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>U</span><span class=p>,</span> <span class=n>V</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>I</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>V</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>AU</span> <span class=o>=</span> <span class=n>A</span> <span class=o>@</span> <span class=n>U</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>sp</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>blas</span><span class=o>.</span><span class=n>dgemm</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>alpha</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>a</span><span class=o>=</span><span class=n>AU</span> <span class=o>@</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>I</span> <span class=o>+</span> <span class=n>V</span> <span class=o>@</span> <span class=n>AU</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>b</span><span class=o>=</span><span class=n>V</span> <span class=o>@</span> <span class=n>A</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>beta</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>c</span><span class=o>=</span><span class=n>A</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>mean</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>M</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asfortranarray</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>Xb</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span> <span class=o>+=</span> <span class=nb>len</span><span class=p>(</span><span class=n>Xb</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=n>Xb</span> <span class=o>-</span> <span class=n>mean</span>
</span></span><span class=line><span class=cl>    <span class=n>mean</span> <span class=o>=</span> <span class=p>(</span><span class=n>w</span> <span class=o>*</span> <span class=n>mean</span> <span class=o>+</span> <span class=nb>len</span><span class=p>(</span><span class=n>Xb</span><span class=p>)</span> <span class=o>*</span> <span class=n>Xb</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>w</span> <span class=o>+</span> <span class=nb>len</span><span class=p>(</span><span class=n>Xb</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>M</span> <span class=o>=</span> <span class=n>woodbury_matrix</span><span class=p>(</span><span class=n>A</span><span class=o>=</span><span class=n>M</span><span class=p>,</span> <span class=n>U</span><span class=o>=</span><span class=n>diff</span><span class=o>.</span><span class=n>T</span><span class=p>,</span> <span class=n>V</span><span class=o>=</span><span class=n>Xb</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>inv_cov</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>*</span> <span class=n>M</span>
</span></span></code></pre></div><p>Alas, this didn&rsquo;t result in a significant speed-up for me, which is why I&rsquo;m leaving it the appendix. I believe this is because most of the computation happens in <code>np.linalg.inv(I + V @ AU)</code>, which occurs before calling the DGEMM routine.</p><p>I would also like to echo Tim Vieira&rsquo;s suggestion that using the <a href=https://www.wikiwand.com/en/Cholesky_decomposition>Cholesky factorisation</a> is a better way for estimating a covariance matrix, as well as a precision matrix. This saves some computation by leveraging the fact both matrices are symmetric. The Cholesky factorisation should also result in more stable and accurate results. I haven&rsquo;t yet come around to trying this out, but if I do I&rsquo;ll start by checking out Tim Vieira&rsquo;s code <a href=https://github.com/timvieira/arsenal/blob/master/arsenal/maths/cholesky.py>here</a>.</p><p>We&rsquo;ve <a href=https://github.com/online-ml/river/pull/999>recently added</a> an online precision matrix to <a href=https://github.com/online-ml/river>River</a>. It implements the Sherman-Morrison formula, as well as the Woodbury matrix identity. This precision matrix is likely going to become the building block for higher-level algorithms. We&rsquo;re thus always on the lookout for faster solutions. Please feel welcome sharing any insight you way have 👐</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px;margin-bottom:30px><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div><iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=32 width=114 style=border:0;border-radius:6px></iframe></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script>
<script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href="https://www.youtube.com/watch?v=nzFTmJnIakk&amp;list=PLIU25-FciwNaz5PqWPiHmPCMOFYoEsJ8c&amp;index=5">Online machine learning with River @ GAIA</a></li><li><a href=/slides/the-benefits-of-online-learning.html>The benefits of online machine learning @ Airbus Bizlab</a></li><li><a href=/slides/online-ml-in-practice-pydata-pdx.pdf>Online machine learning in practice @ PyData PDX</a></li></ul></div></div></div></article></body></html>