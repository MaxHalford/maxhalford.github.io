<!doctype html><html lang=en><head><script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script><meta charset=utf-8><meta name=generator content="Hugo 0.108.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/speeding-up-sklearn-single-predictions/"><link rel=canonical href=https://maxhalford.github.io/blog/speeding-up-sklearn-single-predictions/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Speeding up scikit-learn for single predictions","headline":"Speeding up scikit-learn for single predictions","description":"It is now common practice to train machine learning models offline before putting them behind an API endpoint to serve predictions. Specifically, we want an API route which can make a prediction for a single row\/instance\/sample\/data point\/individual (call it what you want). Nowadays, we have great tools to do this that care of the nitty-gritty details, such as Cortex, MLFlow, Kubeflow, and Clipper. There are also paid services that hold your hand a bit more, such as DataRobot, H2O, and Cubonacci.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2020","datePublished":"2020-03-31 00:00:00 \u002b0000 UTC","dateModified":"2020-03-31 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/speeding-up-sklearn-single-predictions\/","keywords":["machine-learning"]}</script><title>Speeding up scikit-learn for single predictions â€¢ Max Halford</title><meta property="og:title" content="Speeding up scikit-learn for single predictions â€¢ Max Halford"><meta property="og:type" content="article"><meta name=description content="It is now common practice to train machine learning models offline before putting them behind an API endpoint to serve predictions. Specifically, we want an API route which can make a prediction for a single row/instance/sample/data point/individual (call it what you want). Nowadays, we have great tools to do this that care of the nitty-gritty details, such as Cortex, MLFlow, Kubeflow, and Clipper. There are also paid services that hold your hand a bit more, such as DataRobot, H2O, and Cubonacci."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford ðŸ¦”</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>Speeding up scikit-learn for single predictions</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2020-03-31 00:00:00 UTC">2020-03-31</time>
<span class=posts-line-tag>machine-learning</span></div></div></header><div class="post-content markdown-body"><p>It is now common practice to train machine learning models offline before putting them behind an API endpoint to serve predictions. Specifically, we want an API route which can make a prediction for a single row/instance/sample/data point/individual (<a href="https://www.youtube.com/watch?v=1prhCWO_518">call it what you want</a>). Nowadays, we have great tools to do this that care of the nitty-gritty details, such as <a href=https://github.com/cortexlabs/cortex>Cortex</a>, <a href=https://www.mlflow.org/docs/latest/models.html>MLFlow</a>, <a href=https://www.kubeflow.org/docs/components/serving/>Kubeflow</a>, and <a href=https://github.com/ucbrise/clipper>Clipper</a>. There are also paid services that hold your hand a bit more, such as <a href=https://www.datarobot.com/>DataRobot</a>, <a href=https://www.h2o.ai/>H2O</a>, and <a href=https://www.cubonacci.com/>Cubonacci</a>. One could argue that deploying machine learning models has never been easier.</p><p>These solutions usually assume that you&rsquo;re going to be using a popular library from the data science ecosystem, such as <a href=https://scikit-learn.org/stable/>scikit-learn</a>, <a href=https://keras.io/>Keras</a>, <a href=https://pytorch.org/>PyTorch</a>, or <a href=https://github.com/microsoft/LightGBM>LightGBM</a>, or maybe even <a href=https://github.com/NicolasHug/Surprise>Surprise</a> if you&rsquo;re building a recommender system. The trick is that most of these libraries were never designed to make predictions. In fact, a large part of them were initiated well before putting a machine learning model behind an API endpoint became cool. On the contrary, these libraries are designed to make batch predictions. They are faster at processing 1000 samples 1 time than they are at processing 1 sample 1000 times. This is mostly because they rely on <a href=https://www.wikiwand.com/en/SIMD>SIMD operations</a> and linear algebra tricks that scale well with a lot of data. Therefore these libraries are not optimal for making individual predictions. However, there are some hacks that can make things faster.</p><p>In this post I want to focus on scikit-learn. First of all the <a href=https://scikit-learn.org/stable/modules/computing.html><em>Computing with scikit-learn</em></a> document is a must-read if you&rsquo;re going to use or all already using scikit-learn in production. It includes a lot of tips and tricks for making things faster in general. I&rsquo;m going to be focusing on prediction speed, which in the context of machine learning is sometimes referred to as &ldquo;latency&rdquo; .</p><p>To start off, let&rsquo;s go a ahead and measure the prediction speed of scikit-learn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html><code>LinearRegression</code></a> on a single row. I&rsquo;ll be using IPython&rsquo;s <a href=https://ipython.readthedocs.io/en/stable/interactive/magics.html><code>%timeit</code></a> command to get a reliable estimate. I&rsquo;ll also be the latest version of scikit-learn, which as of now is <code>0.22.2</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>linear_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_boston</span><span class=p>(</span><span class=n>return_X_y</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>lin_reg</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>LinearRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>lin_reg</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>lin_reg</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>[[</span><span class=mi>0</span><span class=p>]])[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><pre tabindex=0><code>44.4 Âµs Â± 3.7 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
</code></pre><p>44.4 microseconds doesn&rsquo;t seem like a lot of time, but it turns out that we can do much better. The thing is that scikit-learn does a lot of checks every time you call <code>predict</code>. For example, it checks that you have provided a 2D numpy array, that the number of features is correct, that non of the values are missing, etc. In many cases these checks might be redundant because you might have already done them yourself as part of a validation step in your application. Therefore, we can write a &ldquo;barebones&rdquo; implementation which skips the details and gets down to the essential, which is evaluating the linear regression. In essence, a linear regression is just a sum of an intercept and a dot product between some weights and the input features. We can implement this ourselves. To do so, we&rsquo;ll define a <code>BarebonesLinearRegression</code> class with a <code>predict_single</code> method which takes as input a 1D array.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BarebonesLinearRegression</span><span class=p>(</span><span class=n>linear_model</span><span class=o>.</span><span class=n>LinearRegression</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict_single</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span>
</span></span></code></pre></div><p>Let&rsquo;s see how fast this is:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>bb_lin_reg</span> <span class=o>=</span> <span class=n>BarebonesLinearRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bb_lin_reg</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>bb_lin_reg</span><span class=o>.</span><span class=n>predict_single</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>1.38 Âµs Â± 31.7 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)
</code></pre><p>The barebones implementation is on average <strong>32 times faster that scikit-learn implementation</strong>. I&rsquo;ll let you be the judge of the significance of this result. To be fair, if you&rsquo;re using an API then most of the latency comes the HTTP protocol, so the benefit of this trick might be amortized. Still, I think it&rsquo;s a cool trick to know, and it has it&rsquo;s applications elsewhere. For instance, it was critical in my team&rsquo;s <a href=https://github.com/MaxHalford/idao-2020-qualifiers>winning solution</a> to the <a href=https://idao.world/results/>qualifiers of the 2020 edition of the IDAO</a>, where we needed to provide a model which had to make a bunch of predictions under a time limit.</p><p>We can apply this trick to other models, such as scikit-learn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression><code>LogisticRegression</code></a>. Let&rsquo;s first evaluate the default implementation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>preprocessing</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_digits</span><span class=p>(</span><span class=n>return_X_y</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>preprocessing</span><span class=o>.</span><span class=n>scale</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>log_reg</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>LogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>log_reg</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>log_reg</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X</span><span class=p>[[</span><span class=mi>0</span><span class=p>]])[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><pre tabindex=0><code>153 Âµs Â± 1.31 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
</code></pre><p>We can implement a <code>predict_single</code> method with scipy&rsquo;s <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html><code>softmax</code></a> function:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy</span> <span class=kn>import</span> <span class=n>special</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BarebonesLogisticRegression</span><span class=p>(</span><span class=n>linear_model</span><span class=o>.</span><span class=n>LogisticRegression</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict_proba_single</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>special</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span></code></pre></div><p>Let&rsquo;s see if we&rsquo;ve gained anything:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>bb_log_reg</span> <span class=o>=</span> <span class=n>BarebonesLogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bb_log_reg</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>bb_log_reg</span><span class=o>.</span><span class=n>predict_proba_single</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>71.3 Âµs Â± 3.59 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
</code></pre><p>That&rsquo;s definitely a boost, but it&rsquo;s nothing to brag home about. We can check what&rsquo;s taking the most time with IPython&rsquo;s <a href=https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-prun><code>%prun</code></a> command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>%</span><span class=n>prun</span> <span class=o>-</span><span class=n>l</span> <span class=mi>10</span> <span class=p>[</span><span class=n>bb_log_reg</span><span class=o>.</span><span class=n>predict_proba_single</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100000</span><span class=p>)]</span>
</span></span></code></pre></div><pre tabindex=0><code>        4400004 function calls in 4.414 seconds

Ordered by: internal time
List reduced from 34 to 10 due to restriction &lt;10&gt;

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
100000    1.068    0.000    3.504    0.000 _logsumexp.py:9(logsumexp)
200000    0.530    0.000    0.530    0.000 {method &#39;reduce&#39; of &#39;numpy.ufunc&#39; objects}
300000    0.315    0.000    1.376    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}
100000    0.257    0.000    3.761    0.000 _logsumexp.py:132(softmax)
100000    0.254    0.000    4.316    0.000 &lt;ipython-input-145-3bbdfc107533&gt;:5(predict_proba_single)
200000    0.245    0.000    0.578    0.000 _ufunc_config.py:39(seterr)
100000    0.232    0.000    0.383    0.000 _util.py:200(_asarray_validated)
200000    0.226    0.000    0.868    0.000 fromnumeric.py:73(_wrapreduction)
200000    0.201    0.000    0.222    0.000 _ufunc_config.py:139(geterr)
100000    0.095    0.000    0.520    0.000 fromnumeric.py:2092(sum)
</code></pre><p>The culprit is quite obvious: it&rsquo;s the <a href=https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.logsumexp.html><code>logsumexp</code></a> function that is called by <code>special.softmax</code>. The way <code>special.softmax</code> is implemented avoids potential numeric underflows and overflows, as explained <a href=https://stackoverflow.com/questions/42599498/numercially-stable-softmax>here</a>. However, it seems to be slower than a custom implementation which is still stable:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>custom_softmax</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=nb>max</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>numerator</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>denominator</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>numerator</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>numerator</span> <span class=o>/</span> <span class=n>denominator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BarebonesLogisticRegression</span><span class=p>(</span><span class=n>linear_model</span><span class=o>.</span><span class=n>LogisticRegression</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict_proba_single</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>custom_softmax</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>coef_</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span></code></pre></div><p>This produces the following performance:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>bb_log_reg</span> <span class=o>=</span> <span class=n>BarebonesLogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bb_log_reg</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>bb_log_reg</span><span class=o>.</span><span class=n>predict_proba_single</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>14.7 Âµs Â± 682 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
</code></pre><p>This is 4.8 faster than with <code>special.softmax</code>, and 10.4 times than scikit-learn&rsquo;s default implementation. Not bad!</p><p>Linear and logistic regression might be simple methods, but according to a <a href=https://arxiv.org/abs/1912.09536>very recent survey paper</a> by a team at Microsoft they are two of the most used classes in scikit-learn, so they merit attention. Now how about scikit-learn <a href=https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html><code>StandardScaler</code></a>? It&rsquo;s a common preprocessing step before any linear model, and as such is likely to be used in conjunction with the two models above. As usual let&rsquo;s start off by measuring the performance of the default implementation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>preprocessing</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>preprocessing</span><span class=o>.</span><span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>scaler</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X</span><span class=p>[[</span><span class=mi>0</span><span class=p>]])[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><pre tabindex=0><code>43 Âµs Â± 1.07 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
</code></pre><p>A standard scaling step requires nothing more than subtracting the column-wise mean and dividing by the column-wise standard deviation of the data, as so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>BarebonesStandardScaler</span><span class=p>(</span><span class=n>preprocessing</span><span class=o>.</span><span class=n>StandardScaler</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>transform_single</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean_</span><span class=p>)</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>var_</span> <span class=o>**</span> <span class=mf>.5</span>
</span></span></code></pre></div><p>This gives us the following performance:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>bb_scaler</span> <span class=o>=</span> <span class=n>BarebonesStandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bb_scaler</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>bb_scaler</span><span class=o>.</span><span class=n>transform_single</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>1.7 Âµs Â± 34.6 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)
</code></pre><p>Now what if we want to use our custom <code>BarebonesStandardScaler</code> with <code>BarebonesLinearRegression</code> or <code>BarebonesLogisticRegression</code> at the same time? Well the canonical way to compose modeling steps in scikit-learn is through the <a href=https://scikit-learn.org/stable/modules/compose.html>use of pipelines</a>. We can implement a custom pipeline class, which inherits from scikit-learn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html><code>pipeline.Pipeline</code></a> and provides a <code>predict_single</code> function as well as a <code>predict_proba_single</code> function. Here goes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>pipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BarebonesPipeline</span><span class=p>(</span><span class=n>pipeline</span><span class=o>.</span><span class=n>Pipeline</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict_single</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>transformer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>steps</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>transformer</span><span class=o>.</span><span class=n>transform_single</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>steps</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>predict_single</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict_proba_single</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>transformer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>steps</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>transformer</span><span class=o>.</span><span class=n>transform_single</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>steps</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>predict_proba_single</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><p>Let&rsquo;s see what execution speed we obtain:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>bb_pp</span> <span class=o>=</span> <span class=n>BarebonesPipeline</span><span class=p>([(</span><span class=s1>&#39;bb_scaler&#39;</span><span class=p>,</span> <span class=n>bb_scaler</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;bb_lin_reg&#39;</span><span class=p>,</span> <span class=n>bb_lin_reg</span><span class=p>)])</span>
</span></span><span class=line><span class=cl><span class=n>bb_pp</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>bb_pp</span><span class=o>.</span><span class=n>predict_single</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></div><pre tabindex=0><code>3.96 Âµs Â± 184 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
</code></pre><p>Now let&rsquo;s compare that with the default implementation from scikit-learn:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>pp</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>Pipeline</span><span class=p>([(</span><span class=s1>&#39;scaler&#39;</span><span class=p>,</span> <span class=n>scaler</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;lin_reg&#39;</span><span class=p>,</span> <span class=n>lin_reg</span><span class=p>)])</span>
</span></span><span class=line><span class=cl><span class=n>pp</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>timeit</span> <span class=n>pp</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>[[</span><span class=mi>0</span><span class=p>]])[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><pre tabindex=0><code>97.6 Âµs Â± 4.19 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
</code></pre><p>Our custom implementation is around 24.7 times faster. Finally, we can check that our implementation produces the correct outputs:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>for</span> <span class=n>xi</span> <span class=ow>in</span> <span class=n>X</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>pp</span><span class=o>.</span><span class=n>predict</span><span class=p>([</span><span class=n>xi</span><span class=p>])[</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=n>bb_pp</span><span class=o>.</span><span class=n>predict_single</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span>
</span></span></code></pre></div><p>That wraps this blog post up! I hope you enjoyed it. The takeaway is that you can speed up making predictions with scikit-learn, but it requires knowing how the algorithms actually work and how they are implemented, which isn&rsquo;t necessarily a bad thing. You can find all the code used in this notebook in <a href=https://gist.github.com/MaxHalford/47cd83f7cb8e23d2db5616ba9b177ea9>this gist</a>. I&rsquo;ve also included an attempt at speeding up scikit-learn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html><code>DecisionTreeRegressor</code></a>, but to little avail. Feel free to drop me a line if you have any similar hacks and want to talk about it!</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div class=footer><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script>
<script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto;margin-bottom:30px}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href=/blog/machine-learning-production/>A smooth approach to putting machine learning into production</a></li><li><a href=/blog/bayesian-linear-regression/>Bayesian linear regression for practitioners</a></li><li><a href=/blog/genetic-algorithms-introduction/>An introduction to genetic algorithms</a></li></ul></div></div></div></article><script></script></body></html>