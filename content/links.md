---
layout: default
title: Links
---

## Papers

- [An Approach Based on Bayesian Networks for Query Selectivity Estimation - DASFAA, 2019](https://link.springer.com/chapter/10.1007/978-3-030-18579-4_1)
- Master 2 year internship at [HelloFresh](https://www.hellofresh.com/) ([report](/files/internships/M2_report.pdf), [slides](/files/internships/M2_slides.pdf))
- Master 1 year internship at [Privateaser](https://www.privateaser.com/) ([report](/files/internships/M1_report.pdf), [slides](/files/internships/M1_slides.pdf))
- Undergraduate internship at [INSA Toulouse](http://www.insa-toulouse.fr/fr/index.html) ([report](/files/internships/L3_report.pdf), [slides](/files/internships/L3_slides.pdf))
- [Detailed solutions to the first 30 Project Euler problems](/files/papers/project_euler_solutions.pdf)

## Talks

- [Online machine learning with decision trees - virtual seminar, 2020](/slides/online-decision-trees.pdf)
- [Our solution to the IDAO 2020 qualifiers - virtual seminar, 2020](/slides/idao-2020-qualifiers.pdf)
- [Global explanation of machine learning with sensitivity analysis - MASCOT-NUM, Paris, 2020](/slides/gdr-mascotnum-2020.pdf)
- [The benefits of online learning - Quantmetry, Paris, 2019](/slides/the-benefits-of-online-learning)
- [The benefits of online learning - Element AI, London, 2019](/slides/the-benefits-of-online-learning)
- [The benefits of online learning - Airbus BizLab, Toulouse, 2019](/slides/the-benefits-of-online-learning)
- [An approach based on Bayesian networks for query selectivity estimation - DASFAA, 2019](/slides/dasfaa-2019.pdf)
- [Machine learning incrémental: des concepts à la pratique - Toulouse Data Science, 2019](/slides/creme-tds)
- [Online machine learning with creme - PyData, Amsterdam, 2019](/slides/creme-pydata)
- [Docker for data science - HelloFresh, Berlin, 2017](/slides/docker-data-science.pdf)
- [Challenge Big Data - Toulouse, 2017](https://www.youtube.com/watch?v=oQd1h-8Srf4&feature=youtu.be)
- [Forecasting bicycle-sharing usage - Toulouse Data Science, 2016](https://www.youtube.com/watch?v=vQGdzKkyPP0)

## Datasets

- [OpenBikes 2016 challenge](https://www.dropbox.com/s/ic8m0b3mf5wxk4r/challenge.zip?dl=0)

## Blogroll

This is a list of blogs I regularly scroll through.

- [Tim Salimans on Data Analysis](http://timsalimans.com/)
- [Randal Olson](http://www.randalolson.com/blog/)
- [Sam & Max](http://sametmax.com/) -- French and NSFW!
- [Sebastian Raschka](http://sebastianraschka.com/blog/index.html)
- [Clean Coder](https://sites.google.com/site/unclebobconsultingllc/)
- [Pythonic Perambulations](https://jakevdp.github.io/)
- [Erik Bernhardsson](http://erikbern.com/)
- [otoro](http://blog.otoro.net/)
- [Terra Incognita](http://blog.christianperone.com/)
- [Real Python](https://realpython.com/blog/)
- [Airbnb Engineering](http://nerds.airbnb.com/)
- [No Free Hunch](http://blog.kaggle.com/)
- [The Unofficial Google Data Science Blog](http://www.unofficialgoogledatascience.com/)
- [will wolf](http://willwolf.io/)
- [Edwin Chen](http://blog.echen.me/)
- [Use the index, Luke!](http://use-the-index-luke.com/)
- [Jack Preston](https://unwttng.com/)
- [Agustinus Kristiadi](https://wiseodd.github.io/)
- [DataGenetics](http://datagenetics.com/blog.html)
- [Katherine Bailey](https://katbailey.github.io/)
- [Netflix Research](https://research.netflix.com/)
- [inFERENce](https://www.inference.vc/)
- [Hyndsight](https://robjhyndman.com/hyndsight/) -- Rob Hyndman is a time series specialist.
- [While My MCMC Gently Samples](https://twiecki.io/)
- [Ines Montani](https://ines.io/) -- By one of the founders of [spaCy](https://spacy.io/).
- [Stephen Smerity](https://smerity.com/articles/articles.html)
- [Peter Norvig](http://norvig.com/)
- [IT Best Kept Secret Is Optimization](https://www.ibm.com/developerworks/community/blogs/jfp/?lang=en) -- By Jean-Francois Puget, aka [CPMP](https://www.kaggle.com/cpmpml).
- [explained.ai](https://explained.ai/)
- [Better Explained](https://betterexplained.com/)
- [Genetic Argonaut](https://geneticargonaut.blogspot.com/)
- [pandas blog](https://pandas-dev.github.io/pandas-blog/)
- [Towards Data Science](https://towardsdatascience.com/)
- [Linear Disgressions](https://lineardigressions.com/) -- data science podcasts.
- [Not so standard deviations](http://nssdeviations.com/) -- more podcasts.
- [Probably Overthinking It](https://www.allendowney.com/blog/)
- [Simply Statistics](https://simplystatistics.org/)
- [Practically Predictable](http://practicallypredictable.com/)
- [koaning](http://koaning.io/) -- By Vincent Warmerdam, who did [this](https://www.youtube.com/watch?v=68ABAU_V8qI&feature=youtu.be) great presentation.
- [blogarithms](https://blogarithms.github.io/)
- [Possibly Wrong ](https://possiblywrong.wordpress.com/)
- [FastML](http://fastml.com/)
- [Parameter-free Learning and Optimization Algorithms](https://parameterfree.com/)
- [Todd W. Schneider](https://toddwschneider.com/) -- This guy is really good at exploratory data analysis.
- [Yann Thaddée](https://espadrine.github.io/blog/) -- Not directly related to data science but interesting nonetheless.
- [Colins Blog](https://www.solipsys.co.uk/new/ColinsBlog.html)
- [Fabien Sanglard](https://fabiensanglard.net) -- Nothing to do with data science, but such good taste!

## Hall of fame

The following is a hall of fame of papers, books, and blog posts that have a very high [signal to noise ratio](https://www.urbandictionary.com/define.php?term=signal%20to%20noise%20ratio) -- at least in my book. I highly recommend reading some of them when you get time.

- [The Elements of Statistical Learning - Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)
- [Machine Learning - Tom Mitchell](http://personal.disco.unimib.it/Vanneschi/McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf) -- I think this wonderful textbook is under-appreciated.
- [Artificial Intelligence: A Modern Approach - Russel & Norvig](http://web.cecs.pdx.edu/~mperkows/CLASS_479/2017_ZZ_00/02__GOOD_Russel=Norvig=Artificial%20Intelligence%20A%20Modern%20Approach%20(3rd%20Edition).pdf)
- [mlcourse.ai](https://mlcourse.ai/) -- Of all the introductions to machine learning I think this is the one that strikes the best balance between theory and practice.
- [Machine learning cheat sheets - Shervine Amidi](https://stanford.edu/~shervine/teaching/cs-229.html)
- [Kalman and Bayesian Filters in Python - Roger Labbe](http://nbviewer.jupyter.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb) -- Kalman filters are notoriously hard to grok, this tutorial nicely builds up the steps to understanding them.
- [CS231n Convolutional Neural Networks for Visual Recognition - Stanford](http://cs231n.github.io/convolutional-networks/)
- [Algorithmes d’optimisation non-linéaire sans contrainte (French) - Michel Bergmann](https://www.math.u-bordeaux.fr/~mbergman/PDF/These/annexeC.pdf)
- [Graphical Models in a Nutshell - Koller et al.](https://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pdf)
- [Rules of Machine Learning: Best Practices for ML Engineering - Martin Zinkevich](https://developers.google.com/machine-learning/guides/rules-of-ml/) -- You should read this once a year.
- [A Few Useful Things to Know about Machine Learning - Pedro Domingos](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) -- This short paper summarizes basic truths in machine learning.
- [Choose Boring Technology - Dan McKinley](http://mcfunley.com/choose-boring-technology)
- [How to Write a Spelling Corrector - Peter Norvig](https://norvig.com/spell-correct.html) -- Magic in 36 lines of code.
- [MCMC sampling for dummies - Thomas Wiecki](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/)
- [Your Easy Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)
- [An Intuitive Explanation of Convolutional Neural Networks - Ujjwal Karn](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)
- [An overview of gradient descent optimization algorithms - Sebastian Ruder](http://ruder.io/optimizing-gradient-descent/)
- [How to explain gradient boosting - Terence Parr and Jeremy Howard](https://explained.ai/gradient-boosting/index.html) -- A very good introduction to vanilla gradient boosting with step by step examples.
- [Why Does XGBoost Win "Every" Machine Learning Competition? - Didrik Nielsen](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf) -- This Master's thesis goes into some of the details of XGBoost without being too bloated.
- [Good sleep, good learning, good life - Piotr Wozniak](https://web.archive.org/web/20181017190008/https://www.supermemo.com/en/articles/sleep) -- Extremely long and nothing to do with data science, but a very thorough essay nonetheless on how to properly sleep.
- [Make for data scientists - Paul Butler](http://blog.kaggle.com/2012/10/15/make-for-data-scientists/) -- I believe Makefiles are yet to be rediscovered for managing data science pipelines.
- [Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations](https://fermatslibrary.com/s/statistical-tests-p-values-confidence-intervals-and-power-a-guide-to-misinterpretations) -- Just read it.
- [The Cramér-Rao Lower Bound on Variance: Adam and Eve’s "Uncertainty Principle" - Michael Powers](https://web.archive.org/web/20100613220918/http://astro.temple.edu/~powersmr/vol7no3.pdf)
- [Kaggle contest on Observing Dark World - Cam Davidson-Pilon](https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter5_LossFunctions/Ch5_LossFunctions_PyMC3.ipynb#Example:-Kaggle-contest-on-Observing-Dark-World) -- If you're not convinced about the power of Bayesian machine learning then read this and get your mind blown.
- [A Concrete Introduction to Probability (using Python) - Peter Norvig](https://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb) -- Extremely elegant Python coding.
- [The Hungarian Maximum Likelihood Trick - Louis Abraham](https://louisabraham.github.io/notebooks/hungarian_trick.html)
- [Machine Learning for Signal Processing - University of Illinois](https://courses.engr.illinois.edu/cs598ps/fa2018/material.html)
- [Don't Call Yourself A Programmer, And Other Career Advice](https://www.kalzumeus.com/2011/10/28/dont-call-yourself-a-programmer/)
- [Tidy Data - Hadley Wickham](https://vita.had.co.nz/papers/tidy-data.pdf) -- If you like playing with data then you need to be aware of this one.
- [Gaussian Process, not quite for dummies - Yuge Shi](https://yugeten.github.io/posts/2019/09/GP/) -- Gaussian processes are quite difficult to understand (at least, for me) but Yuge gives some great visual intuitions.
- [Continuous Delivery for Machine Learning - Martin Fowler](https://martinfowler.com/articles/cd4ml.html)
- [Memos - Sriram Krishnan](https://sriramk.com/memos)
- [Frequentism and Bayesianism: A Python-driven Primer - Jake VanderPlas](https://arxiv.org/pdf/1411.5018.pdf)
- [A Few Useful Things To Know About Machine Learning - Pedro Domingos](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)
- [Multiworld Testing Decision Service: A System for Experimentation, Learning, And Decision-Making](https://arxiv.org/pdf/1606.03966.pdf)
- [Machine Learning:The High-Interest Credit Card of Technical Debt - Google](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43146.pdf)
- [Variational Inference: A Review for Statisticians - David Blei and his flock](https://arxiv.org/pdf/1601.00670.pdf)
- [The Performance of Decision Tree Evaluation Strategies - Andrew Tulloch](http://tullo.ch/articles/decision-tree-evaluation/)
- [Hidden Technical Debt in Machine Learning Systems - Google](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)
- [Distill: Why do we need Flask, Celery, and Redis? (with McDonalds in Between) - Lj Miranda](https://ljvmiranda921.github.io/notebook/2019/11/08/flask-redis-celery-mcdo/) -- A good example of the difference between abstract ideas and implementation details.
- [Darts, Dice, and Coins: Sampling from a Discrete Distribution -- Keith Schwarz](https://www.keithschwarz.com/darts-dice-coins/)
- [Simplifying Graph Convolutional Networks -- Felix Wu et al.](https://arxiv.org/pdf/1902.07153.pdf) -- A nice example of putting the horse before the cart.
- [Data Scientist Jobs for Master’s Graduates](https://online.maryville.edu/online-masters-degrees/data-science/careers/) -- A somewhat up-to-date list of career paths for data science students.
- [MIT 6.867 machine learning course notes -- Tommi Jaakola](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/) -- For people who enjoy concise mathematical notation.

## Eye candy

- [Tyler Hobbs](https://tylerxhobbs.com/) -- The god of generative arts.
- [Some Jean Giraud stuff](https://imgur.com/a/iiTW0Ay)
- [Mauro Martins](https://www.mauromartins.com/)
- [A new way to knit by Petros Vrellis](http://artof01.com/vrellis/works/knit.html)
- [A fascinating article about Manolo Gamboa Naon](https://www.artnome.com/news/2018/8/8/generative-art-finds-its-prodigy)
- [Some Ukiyo-e](https://ukiyo-e.org/)
- [Turtletoy](https://turtletoy.net/)
- [Dwitter](https://www.dwitter.net/)
- [generated.space](https://generated.space/)
- [Pixel art by Marcus Blättermann](https://essenmitsosse.de/pixel/)
- [Nick Barnes' football bible](https://www.the42.ie/bbc-nick-barnes-football-notes-2111888-May2015/)
- [Simon Stålenhag](http://www.simonstalenhag.se/)
- [Syd Mead](https://www.iamag.co/the-art-of-syd-mead/) (who worked on Blade Runner)
- [Michael Fogleman's blog](https://www.michaelfogleman.com/)
- [World of Warcraft art by Dreamwalker](https://imgur.com/user/imadreamwalker/posts)
