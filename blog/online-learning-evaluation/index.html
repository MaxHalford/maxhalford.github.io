<!doctype html><html lang=en>
<head>
<script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.88.1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=author content="Max Halford">
<meta property="og:url" content="https://maxhalford.github.io/blog/online-learning-evaluation/">
<link rel=canonical href=https://maxhalford.github.io/blog/online-learning-evaluation/>
<link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>">
<script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"The correct way to evaluate online machine learning models","headline":"The correct way to evaluate online machine learning models","description":"Motivation Most supervised machine learning algorithms work in the batch setting, whereby they are fitted on a training set offline, and are used to predict the outcomes of new samples. The only way for batch machine learning algorithms to learn from new samples is to train them from scratch with both the old samples and the new ones. Meanwhile, some learning algorithms are online, and can predict as well as update themselves when new samples are available.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2020","datePublished":"2020-06-07 00:00:00 \u002b0000 UTC","dateModified":"2020-06-07 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/online-learning-evaluation\/","keywords":[]}</script>
<title>The correct way to evaluate online machine learning models - Max Halford</title>
<meta property="og:title" content="The correct way to evaluate online machine learning models - Max Halford">
<meta property="og:type" content="article">
<meta name=description content="Motivation Most supervised machine learning algorithms work in the batch setting, whereby they are fitted on a training set offline, and are used to predict the outcomes of new samples. The only way for batch machine learning algorithms to learn from new samples is to train them from scratch with both the old samples and the new ones. Meanwhile, some learning algorithms are online, and can predict as well as update themselves when new samples are available.">
<link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css>
<link rel=stylesheet href=/css/github-markdown.min.css>
<link rel=stylesheet href=/css/highlight/github.css>
<link rel=stylesheet href=/css/index.css>
<link rel=preconnect href=https://fonts.gstatic.com>
<link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0,tags:'ams'},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll('mjx-container').forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head>
<body>
<article class=post id=article>
<div class="row center-xs" style=text-align:left>
<div class="col-xs-12 col-sm-10 col-md-7 col-lg-5">
<div class=post-header>
<header>
<div class="signatures site-title">
<a href=/>Max Halford</a>
</div>
</header>
<div class="row end-xs">
<div>
<a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a>
</div>
</div>
<div class=header-line></div>
</div>
<header class=post-header>
<h1 class=post-title>The correct way to evaluate online machine learning models</h1>
<div class="row post-desc">
<div class=col-xs-12>
<time class=post-date datetime="2020-06-07 00:00:00 UTC">
2020-06-07 Â· 20 minute read
</time>
</div>
</div>
</header>
<div class="post-content markdown-body">
<h2 id=toc>Table of contents</h2>
<nav id=TableOfContents>
<ul>
<li><a href=#motivation>Motivation</a></li>
<li><a href=#cross-validation>Cross-validation</a></li>
<li><a href=#progressive-validation>Progressive validation</a></li>
<li><a href=#delayed-progressive-validation>Delayed progressive validation</a></li>
</ul>
</nav>
<h2 id=motivation>Motivation</h2>
<p>Most supervised machine learning algorithms work in the batch setting, whereby they are fitted on a training set offline, and are used to predict the outcomes of new samples. The only way for batch machine learning algorithms to learn from new samples is to train them from scratch with both the old samples and the new ones. Meanwhile, some learning algorithms are online, and can predict as well as update themselves when new samples are available. This encompasses any model trained with <a href=https://leon.bottou.org/publications/pdf/compstat-2010.pdf>stochastic gradient descent</a> &ndash; which includes deep neural networks, <a href=https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf>factorisation machines</a>, and <a href=https://www.cs.huji.ac.il/~shais/papers/ShalevSiSrCo10.pdf>SVMs</a> &ndash; as well as <a href=https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf>decision trees</a>, <a href=https://ai.stanford.edu/~ang/papers/icml04-onlinemetric.pdf>metric learning</a>, and <a href=https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf>naÃ¯ve Bayes</a>.</p>
<p>Online models are usually weaker than batch models when trained on the same amount of data. However, this discrepancy tends to get smaller as the size of the training data increases. Researchers try to build online models that are guaranteed to reach the same performance as a batch model when the size of the data grows &ndash; they call this <em>convergence</em>. But comparing online models to batch models isn&rsquo;t really fair, because they&rsquo;re not meant to solve the same problems.</p>
<p>
<img src=/img/blog/online-learning-evaluation/meme.png width=30%>
</p>
<p>Batch models are meant to be used when you can afford to retrain your model from scratch every so often. Online models, on the contrary, are meant to be used when you want your model to learn from a stream of data, and therefore never have to restart from scratch. Learning from a stream of data is something a batch model can&rsquo;t do, and is very much different to the usual train/test split paradigm that machine learning practitioners are used to. In fact, there are other ways to evaluate the performance of an online model that make more sense than, say, cross-validation.</p>
<h2 id=cross-validation>Cross-validation</h2>
<p>To begin with, I&rsquo;m going to compare scikit-learn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html><code>SGDRegressor</code></a> and <a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html><code>Ridge</code></a>. In a nutshell, <code>SGDRegressor</code> has the same model parameters as a <code>Ridge</code>, but is trained via stochastic gradient descent, and can thus learn from a stream of data. In practice this happens via the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor.partial_fit><code>partial_fit</code></a> method. Both can be seen as linear regression with some L2 regularisation thrown into the mix. Note that scikit-learn provides <a href=https://scikit-learn.org/stable/modules/computing.html#incremental-learning>a list</a> of it&rsquo;s estimators that support &ldquo;incremental learning&rdquo;, which is a synonym of online learning.</p>
<p>As a running example in this blog post, I&rsquo;m going to be using the <a href=https://www.kaggle.com/c/nyc-taxi-trip-duration>New-York City taxi trip duration dataset</a> from Kaggle. This dataset contains 6 months of taxi trips and is a perfect usecase for online learning. We&rsquo;ll start by loading the data and tidying it up a bit:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>

<span class=n>taxis</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span>
    <span class=s1>&#39;nyc_taxis/train.csv&#39;</span><span class=p>,</span>
    <span class=n>parse_dates</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;pickup_datetime&#39;</span><span class=p>,</span> <span class=s1>&#39;dropoff_datetime&#39;</span><span class=p>],</span>
    <span class=n>index_col</span><span class=o>=</span><span class=s1>&#39;id&#39;</span><span class=p>,</span>
    <span class=n>dtype</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;vendor_id&#39;</span><span class=p>:</span> <span class=s1>&#39;category&#39;</span><span class=p>,</span> <span class=s1>&#39;store_and_fwd_flag&#39;</span><span class=p>:</span> <span class=s1>&#39;category&#39;</span><span class=p>}</span>
<span class=p>)</span>
<span class=n>taxis</span> <span class=o>=</span> <span class=n>taxis</span><span class=o>.</span><span class=n>rename</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=p>{</span>
    <span class=s1>&#39;pickup_longitude&#39;</span><span class=p>:</span> <span class=s1>&#39;pickup_lon&#39;</span><span class=p>,</span>
    <span class=s1>&#39;dropoff_longitude&#39;</span><span class=p>:</span> <span class=s1>&#39;dropoff_lon&#39;</span><span class=p>,</span>
    <span class=s1>&#39;pickup_latitude&#39;</span><span class=p>:</span> <span class=s1>&#39;pickup_lat&#39;</span><span class=p>,</span>
    <span class=s1>&#39;dropoff_latitude&#39;</span><span class=p>:</span> <span class=s1>&#39;dropoff_lat&#39;</span>
<span class=p>})</span>
<span class=n>taxis</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</code></pre></div><table>
<thead>
<tr>
<th style=text-align:left>id</th>
<th style=text-align:right>vendor_id</th>
<th style=text-align:left>pickup_datetime</th>
<th style=text-align:left>dropoff_datetime</th>
<th style=text-align:right>passenger_count</th>
<th style=text-align:right>pickup_lon</th>
<th style=text-align:right>pickup_lat</th>
<th style=text-align:right>dropoff_lon</th>
<th style=text-align:right>dropoff_lat</th>
<th style=text-align:left>store_and_fwd_flag</th>
<th style=text-align:right>trip_duration</th>
<th style=text-align:right>l1_dist</th>
<th style=text-align:right>l2_dist</th>
<th style=text-align:right>day</th>
<th style=text-align:right>weekday</th>
<th style=text-align:right>hour</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left>id0190469</td>
<td style=text-align:right>2</td>
<td style=text-align:left>2016-01-01 00:00:17</td>
<td style=text-align:left>2016-01-01 00:14:26</td>
<td style=text-align:right>5</td>
<td style=text-align:right>-73.9817</td>
<td style=text-align:right>40.7192</td>
<td style=text-align:right>-73.9388</td>
<td style=text-align:right>40.8292</td>
<td style=text-align:left>N</td>
<td style=text-align:right>849</td>
<td style=text-align:right>0.152939</td>
<td style=text-align:right>0.118097</td>
<td style=text-align:right>1</td>
<td style=text-align:right>4</td>
<td style=text-align:right>0</td>
</tr>
<tr>
<td style=text-align:left>id1665586</td>
<td style=text-align:right>1</td>
<td style=text-align:left>2016-01-01 00:00:53</td>
<td style=text-align:left>2016-01-01 00:22:27</td>
<td style=text-align:right>1</td>
<td style=text-align:right>-73.9851</td>
<td style=text-align:right>40.7472</td>
<td style=text-align:right>-73.958</td>
<td style=text-align:right>40.7175</td>
<td style=text-align:left>N</td>
<td style=text-align:right>1294</td>
<td style=text-align:right>0.0567207</td>
<td style=text-align:right>0.0401507</td>
<td style=text-align:right>1</td>
<td style=text-align:right>4</td>
<td style=text-align:right>0</td>
</tr>
<tr>
<td style=text-align:left>id1210365</td>
<td style=text-align:right>2</td>
<td style=text-align:left>2016-01-01 00:01:01</td>
<td style=text-align:left>2016-01-01 00:07:49</td>
<td style=text-align:right>5</td>
<td style=text-align:right>-73.9653</td>
<td style=text-align:right>40.801</td>
<td style=text-align:right>-73.9475</td>
<td style=text-align:right>40.8152</td>
<td style=text-align:left>N</td>
<td style=text-align:right>408</td>
<td style=text-align:right>0.031929</td>
<td style=text-align:right>0.0227259</td>
<td style=text-align:right>1</td>
<td style=text-align:right>4</td>
<td style=text-align:right>0</td>
</tr>
<tr>
<td style=text-align:left>id3888279</td>
<td style=text-align:right>1</td>
<td style=text-align:left>2016-01-01 00:01:14</td>
<td style=text-align:left>2016-01-01 00:05:54</td>
<td style=text-align:right>1</td>
<td style=text-align:right>-73.9823</td>
<td style=text-align:right>40.7513</td>
<td style=text-align:right>-73.9913</td>
<td style=text-align:right>40.7503</td>
<td style=text-align:left>N</td>
<td style=text-align:right>280</td>
<td style=text-align:right>0.0100403</td>
<td style=text-align:right>0.00910266</td>
<td style=text-align:right>1</td>
<td style=text-align:right>4</td>
<td style=text-align:right>0</td>
</tr>
<tr>
<td style=text-align:left>id0924227</td>
<td style=text-align:right>1</td>
<td style=text-align:left>2016-01-01 00:01:20</td>
<td style=text-align:left>2016-01-01 00:13:36</td>
<td style=text-align:right>1</td>
<td style=text-align:right>-73.9701</td>
<td style=text-align:right>40.7598</td>
<td style=text-align:right>-73.9894</td>
<td style=text-align:right>40.743</td>
<td style=text-align:left>N</td>
<td style=text-align:right>736</td>
<td style=text-align:right>0.0360603</td>
<td style=text-align:right>0.0255567</td>
<td style=text-align:right>1</td>
<td style=text-align:right>4</td>
<td style=text-align:right>0</td>
</tr>
</tbody>
</table>
<p>The dataset contains a few anomalies, such as trips that last an very large amount of time. For simplicity we&rsquo;ll only consider the trips that last under an hour, which is the case for over 99% of the them.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=n>taxis</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=s1>&#39;trip_duration &lt; 3600&#39;</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div><p>Now let&rsquo;s add a few features.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=c1># Distances</span>
<span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;l1_dist&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>taxis</span><span class=o>.</span><span class=n>eval</span><span class=p>(</span><span class=s1>&#39;abs(pickup_lon - dropoff_lon) + abs(pickup_lat - dropoff_lat)&#39;</span><span class=p>)</span>
<span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;l2_dist&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>taxis</span><span class=o>.</span><span class=n>eval</span><span class=p>(</span><span class=s1>&#39;sqrt((pickup_lon - dropoff_lon) ** 2 + (pickup_lat - dropoff_lat) ** 2)&#39;</span><span class=p>)</span>

<span class=c1># The usual suspects</span>
<span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;day&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;pickup_datetime&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>day</span>
<span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;weekday&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;pickup_datetime&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>weekday</span>
<span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;hour&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;pickup_datetime&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>hour</span>
</code></pre></div><p>Cross-validation is a well-known machine learning technique, so allow me not to disgress on it. The specifity of our case is that our observations have timestamps. Therefore, performing a cross-sampling with folds chosen at random is a mistake. Indeed, if our goal is to get a faithful idea of the performance of our model for future data, then we need to take into account the temporal aspect of the data. For more material on this, I recommend reading <a href="https://www.google.com/search?client=firefox-b-d&q=temporal+cross-validation">this research paper</a> and <a href=https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection>this CrossValidated thread</a>. To keep things simple, we can split our dataset in two. The test set will be the last month in our dataset, which is June, whilst the training set will contain all the months before that. This isn&rsquo;t cross-validation per say, but what matters here is the general idea.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>preprocessing</span>

<span class=n>is_test</span> <span class=o>=</span> <span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;pickup_datetime&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>month</span> <span class=o>==</span> <span class=mi>6</span>  <span class=c1># i.e. the month of June</span>
<span class=n>not_features</span> <span class=o>=</span> <span class=p>[</span>
    <span class=s1>&#39;vendor_id&#39;</span><span class=p>,</span> <span class=s1>&#39;pickup_datetime&#39;</span><span class=p>,</span> <span class=s1>&#39;dropoff_datetime&#39;</span><span class=p>,</span>
    <span class=s1>&#39;store_and_fwd_flag&#39;</span><span class=p>,</span> <span class=s1>&#39;trip_duration&#39;</span>
<span class=p>]</span>

<span class=n>X</span> <span class=o>=</span> <span class=n>taxis</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=n>not_features</span><span class=p>)</span>
<span class=n>X</span><span class=p>[:]</span> <span class=o>=</span> <span class=n>preprocessing</span><span class=o>.</span><span class=n>scale</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
<span class=n>y</span> <span class=o>=</span> <span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;trip_duration&#39;</span><span class=p>]</span>

<span class=n>X_train</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=o>~</span><span class=n>is_test</span><span class=p>]</span>
<span class=n>y_train</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=o>~</span><span class=n>is_test</span><span class=p>]</span>

<span class=n>X_test</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>is_test</span><span class=p>]</span>
<span class=n>y_test</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>is_test</span><span class=p>]</span>
</code></pre></div><p>Now obtaining a performance score for a batch model is simple: we train it on the training set and we make predictions on the test set. In our case we&rsquo;ll calculate the mean absolute error because this implies that the error will be measured in seconds.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>linear_model</span>
<span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>metrics</span>

<span class=n>lin_reg</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>Ridge</span><span class=p>()</span>

<span class=n>lin_reg</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
<span class=n>y_pred</span> <span class=o>=</span> <span class=n>lin_reg</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

<span class=n>score</span> <span class=o>=</span> <span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</code></pre></div><p>As for the <code>SGDRegressor</code>, we can also train it on the whole training set and evaluate it on the test set. However, we can also train it incrementally by batching the training set. For instance, we can split the training set in 5 cunks and call <code>partial_fit</code> on each chunk. We can therefore see much the amount of training data affects the performance on the test set. Note that we choose 5 because this is equivalent to the number of months in the training set.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>

<span class=n>sgd</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>SGDRegressor</span><span class=p>(</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=s1>&#39;constant&#39;</span><span class=p>,</span>
    <span class=n>eta0</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>

<span class=n>n_rows</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>scores</span> <span class=o>=</span> <span class=p>{}</span>

<span class=k>for</span> <span class=n>X_chunk</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>array_split</span><span class=p>(</span><span class=n>X_train</span><span class=o>.</span><span class=n>iloc</span><span class=p>[::</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=mi>5</span><span class=p>):</span>
    <span class=n>y_chunk</span> <span class=o>=</span> <span class=n>y_train</span><span class=o>.</span><span class=n>loc</span><span class=p>[</span><span class=n>X_chunk</span><span class=o>.</span><span class=n>index</span><span class=p>]</span>

    <span class=n>sgd</span> <span class=o>=</span> <span class=n>sgd</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>(</span><span class=n>X_chunk</span><span class=p>,</span> <span class=n>y_chunk</span><span class=p>)</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>sgd</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>

    <span class=n>n_rows</span> <span class=o>+=</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_chunk</span><span class=p>)</span>
    <span class=n>scores</span><span class=p>[</span><span class=n>n_rows</span><span class=p>]</span> <span class=o>=</span> <span class=n>metrics</span><span class=o>.</span><span class=n>mean_absolute_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</code></pre></div><p>Let&rsquo;s see how this looks on a chart.</p>
<details>
<summary>Click to see the code</summary>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>

<span class=n>ax</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Batch linear regression&#39;</span><span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span>
    <span class=nb>list</span><span class=p>(</span><span class=n>scores</span><span class=o>.</span><span class=n>keys</span><span class=p>()),</span>
    <span class=nb>list</span><span class=p>(</span><span class=n>scores</span><span class=o>.</span><span class=n>values</span><span class=p>()),</span>
    <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Incremental linear regression&#39;</span>
<span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=s1>&#39;lower center&#39;</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylim</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>score</span> <span class=o>*</span> <span class=mf>1.1</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>ticklabel_format</span><span class=p>(</span><span class=n>style</span><span class=o>=</span><span class=s1>&#39;sci&#39;</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=n>scilimits</span><span class=o>=</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
<span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Number of observations&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Mean absolute error&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Batch vs. incremental linear regression&#39;</span><span class=p>,</span> <span class=n>pad</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
<span class=n>fig</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;batch_vs_incremental.svg&#39;</span><span class=p>,</span> <span class=n>bbox_inches</span><span class=o>=</span><span class=s1>&#39;tight&#39;</span><span class=p>)</span>
</code></pre></div></details>
<p><img src=/img/blog/online-learning-evaluation/batch_vs_incremental.svg alt=batch_vs_incremental></p>
<p>As we can see, both models seem to be performing just as well. The average error is just north of 5 minutes. It seems that the amount of data doesn&rsquo;t have too much of an impact on performance. But this isn&rsquo;t telling the whole story.</p>
<h2 id=progressive-validation>Progressive validation</h2>
<p>In the case of online learning, the shortcoming of cross-validation is that it doesn&rsquo;t faithfully reproduce the steps that the model will undergo. Cross-validation assumes that the model is trained once and remains static from thereon. However, an online model keeps learning, and can make predictions at any point in it&rsquo;s lifetime. Remember, our goal is to obtain a measure of how well the model would perform in a production environment. Cross-validation will produce a proxy of this measure, but we can do even better.</p>
<p>In the case of online machine learning, we have another validation tool at our disposal called <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.3925&rep=rep1&type=pdf">progressive validation</a>. In an online setting, observations arrive from a stream in sequential order. Each observation can be denoted as $(x_i, y_i)$, where $x_i$ is a set of features, $y_i$ is a label, and $i$ is used to denote time (i.e., it can be an integer or a timestamp). Before updating the model with the pair $(x_i, y_i)$, we can ask the model to predict the output of $x_i$, and thus obtain $\hat{y}_i$. We can then update a live metric by providing it with $y_i$ and $\hat{y}_i$. Indeed, common metrics such as accuracy, MSE, and ROC AUC are all sums and can thus be updated online. By doing so, the model is trained with all the data in a single pass, and all the data is as well used as a validation set. Think about that, because it&rsquo;s quite a powerful idea. Moreover, the data is processed in the order in which it arrives, which means that it is virtually impossible to introduce <a href=https://www.quora.com/Whats-data-leakage-in-data-science>data leakage</a> &ndash; including <a href=https://www.datarobot.com/wiki/target-leakage/>target leakage</a>.</p>
<p>Let&rsquo;s apply progressive validation to our <code>SGDRegressor</code> on the whole taxi trips dataset. I encourage you to go through the code because it&rsquo;s quite self-explanatory. I&rsquo;ve added comments to separate the sequences of steps that are performed. In short these are: 1) get the next sample, 2) make a prediction, 3) update a running average of the error, 4) update the model. An <a href=https://www.wikiwand.com/en/Moving_average#/Exponential_moving_average>exponentially weighted average</a> of the MAE is stored in addition to the overall running average. This allows to get an idea of the recent performance of the model at every point in time. To keep things clear in the resulting chart, I&rsquo;ve limited the number of samples to 38,000, which roughly corresponds to a week of data.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>exceptions</span>

<span class=n>sgd</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>SGDRegressor</span><span class=p>(</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=s1>&#39;constant&#39;</span><span class=p>,</span>
    <span class=n>eta0</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>scores</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>exp_scores</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>running_mae</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>exp_mae</span> <span class=o>=</span> <span class=mi>0</span>

<span class=n>X_y</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>(),</span> <span class=n>y</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>())</span>
<span class=n>dates</span> <span class=o>=</span> <span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;pickup_datetime&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>()</span>

<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>date</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>dates</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>

    <span class=n>xi</span><span class=p>,</span> <span class=n>yi</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>X_y</span><span class=p>)</span>

    <span class=c1># Make a prediction before the model learns</span>
    <span class=k>try</span><span class=p>:</span>
        <span class=n>y_pred</span> <span class=o>=</span> <span class=n>sgd</span><span class=o>.</span><span class=n>predict</span><span class=p>([</span><span class=n>xi</span><span class=p>])[</span><span class=mi>0</span><span class=p>]</span>
    <span class=k>except</span> <span class=n>exceptions</span><span class=o>.</span><span class=n>NotFittedError</span><span class=p>:</span>  <span class=c1># happens if partial_fit hasn&#39;t been called yet</span>
        <span class=n>y_pred</span> <span class=o>=</span> <span class=mf>0.</span>

    <span class=c1># Update the running mean absolute error</span>
    <span class=n>mae</span> <span class=o>=</span> <span class=nb>abs</span><span class=p>(</span><span class=n>y_pred</span> <span class=o>-</span> <span class=n>yi</span><span class=p>)</span>
    <span class=n>running_mae</span> <span class=o>+=</span> <span class=p>(</span><span class=n>mae</span> <span class=o>-</span> <span class=n>running_mae</span><span class=p>)</span> <span class=o>/</span> <span class=n>i</span>

    <span class=c1># Update the exponential moving average of the MAE</span>
    <span class=n>exp_mae</span> <span class=o>=</span> <span class=mf>.1</span> <span class=o>*</span> <span class=n>mae</span> <span class=o>+</span> <span class=mf>.9</span> <span class=o>*</span> <span class=n>exp_mae</span>

    <span class=c1># Store the metric at the current time</span>
    <span class=k>if</span> <span class=n>i</span> <span class=o>&gt;=</span> <span class=mi>10</span><span class=p>:</span>
        <span class=n>scores</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>date</span><span class=p>,</span> <span class=n>running_mae</span><span class=p>))</span>
        <span class=n>exp_scores</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>date</span><span class=p>,</span> <span class=n>exp_mae</span><span class=p>))</span>

    <span class=c1># Finally, make the model learn</span>
    <span class=n>sgd</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>([</span><span class=n>xi</span><span class=p>],</span> <span class=p>[</span><span class=n>yi</span><span class=p>])</span>

    <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>38000</span><span class=p>:</span>
        <span class=k>break</span>
</code></pre></div><p>Now let&rsquo;s how this looks:</p>
<details>
<summary>Click to see the code</summary>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=kn>import</span> <span class=nn>matplotlib.dates</span> <span class=k>as</span> <span class=nn>mdates</span>

<span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>

<span class=n>hours</span> <span class=o>=</span> <span class=n>mdates</span><span class=o>.</span><span class=n>HourLocator</span><span class=p>(</span><span class=n>interval</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
<span class=n>h_fmt</span> <span class=o>=</span> <span class=n>mdates</span><span class=o>.</span><span class=n>DateFormatter</span><span class=p>(</span><span class=s1>&#39;%A %H:%M&#39;</span><span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span>
    <span class=p>[</span><span class=n>d</span> <span class=k>for</span> <span class=n>d</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>scores</span><span class=p>],</span>
    <span class=p>[</span><span class=n>s</span> <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>s</span> <span class=ow>in</span> <span class=n>scores</span><span class=p>],</span>
    <span class=n>linewidth</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Running average&#39;</span><span class=p>,</span>
    <span class=n>alpha</span><span class=o>=</span><span class=mf>.7</span>
<span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span>
    <span class=p>[</span><span class=n>d</span> <span class=k>for</span> <span class=n>d</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>exp_scores</span><span class=p>],</span>
    <span class=p>[</span><span class=n>s</span> <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>s</span> <span class=ow>in</span> <span class=n>exp_scores</span><span class=p>],</span>
    <span class=n>linewidth</span><span class=o>=</span><span class=mf>.3</span><span class=p>,</span>
    <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Exponential moving average&#39;</span><span class=p>,</span>
    <span class=n>alpha</span><span class=o>=</span><span class=mf>.7</span>
<span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylim</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>600</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>set_major_locator</span><span class=p>(</span><span class=n>hours</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>set_major_formatter</span><span class=p>(</span><span class=n>h_fmt</span><span class=p>)</span>
<span class=n>fig</span><span class=o>.</span><span class=n>autofmt_xdate</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Time&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Mean absolute error&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Progressive validation&#39;</span><span class=p>,</span> <span class=n>pad</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
<span class=n>fig</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;progressive_validation.svg&#39;</span><span class=p>,</span> <span class=n>bbox_inches</span><span class=o>=</span><span class=s1>&#39;tight&#39;</span><span class=p>)</span>
</code></pre></div></details>
<p><img src=/img/blog/online-learning-evaluation/progressive_validation.svg alt=progressive_validation></p>
<p>There are two interesting things to notice. First of all, the average performance of the online model is around 200 seconds, which is better than when using cross-validation. This should make sense, because in this online paradigm the model gets to learn every time a sample arrives, whereas previously it was static. You could potentially obtain the same performance with a batch model, but you would need to retrain it from scratch every time a sample arrives. At the very least it would have to be retrained as frequently as possible. The other thing to notice is that the performance of the model seems to oscillate periodically. This could mean that there is an underlying seasonality that the model is not capturing. It could also mean that the variance of the durations changes along time. In fact, this can be verified by looking at the average and the variance of the trip durations per hour of the day.</p>
<details>
<summary>Click to see the code</summary>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=n>agg</span> <span class=o>=</span> <span class=p>(</span>
    <span class=n>taxis</span><span class=o>.</span><span class=n>assign</span><span class=p>(</span><span class=n>hour</span><span class=o>=</span><span class=n>taxis</span><span class=o>.</span><span class=n>pickup_datetime</span><span class=o>.</span><span class=n>dt</span><span class=o>.</span><span class=n>hour</span><span class=p>)</span>
    <span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;hour&#39;</span><span class=p>)[</span><span class=s1>&#39;trip_duration&#39;</span><span class=p>]</span>
    <span class=o>.</span><span class=n>agg</span><span class=p>([</span><span class=s1>&#39;mean&#39;</span><span class=p>,</span> <span class=s1>&#39;std&#39;</span><span class=p>])</span>
<span class=p>)</span>

<span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>

<span class=n>color</span> <span class=o>=</span> <span class=s1>&#39;tab:red&#39;</span>
<span class=n>agg</span><span class=p>[</span><span class=s1>&#39;mean&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Average trip duration&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylim</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>

<span class=n>ax2</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>twinx</span><span class=p>()</span>
<span class=n>color</span> <span class=o>=</span> <span class=s1>&#39;tab:blue&#39;</span>
<span class=n>agg</span><span class=p>[</span><span class=s1>&#39;std&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>ax</span><span class=o>=</span><span class=n>ax2</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
<span class=n>ax2</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Trip duration standard deviation&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>color</span><span class=p>)</span>
<span class=n>ax2</span><span class=o>.</span><span class=n>set_ylim</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>750</span><span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>24</span><span class=p>))</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Hour of departure&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Trip duration distribution per hour&#39;</span><span class=p>,</span> <span class=n>pad</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
<span class=n>fig</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;hourly_averages.svg&#39;</span><span class=p>,</span> <span class=n>bbox_inches</span><span class=o>=</span><span class=s1>&#39;tight&#39;</span><span class=p>)</span>
</code></pre></div></details>
<p><img src=/img/blog/online-learning-evaluation/hourly_averages.svg alt=hourly_averages></p>
<p>We can see that there is much more variance for trips that depart at the beginning of the afternoon than there is for those that occur at night. There are many potential explanations, but that isn&rsquo;t the topic of this blog post. The above chart just helps to explain where the cyclicity in the model&rsquo;s performance is coming from.</p>
<p>In an online setting, progressive validation is a natural method and is often used in practice. For instance, it is mentioned in subsection 5.1 of <a href=https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/41159.pdf><em>Ad Click Prediction: a View from the Trenches</em></a>. In this paper, writter by Google researchers, they use progressive validation to evaluate an ad <a href=https://www.wikiwand.com/en/Click-through_rate>clickâ€“through rate (CTR)</a> model. The authors of the paper remark that models which are based on the gradient of a loss function require computing a prediction anyway, in which case progressive validation can essentially be performed for free. Progressive validation is appealing because it attempts to simulate a live environment wherein the model has to predict the outcome of $x_i$ before the ground truth $y_i$ is made available. For instance, in the case of a CTR task, the label $y_i \in {0, 1}$ is available once the user has clicked on the ad (i.e., $y_i = 1$), has navigated to another page (i.e., $y_i = 0$), or a given amount of time has passed (i.e., $y_i = 0$). Indeed, in a live environment, there is a delay between the query (i.e., predicting the outcome of $x_i$) and the answer (i.e., when $y_i$ is revealed to the model). Note that machine learning is used in the first place because we want to guess $y_i$ before it happens. The larger the delay, the lesser the chance that $x_i$ and $y_i$ will arrive in perfect sequence. Before $y_i$ is made available, any of $x_{i+1}, x_{i+2}, \dots$ could potentially arrive and require predictions to be made. However, when using progressive validation, we implicitly assume that there is no delay. In other words the model has access to $y_i$ immediately after having produced $\hat{y}_i$. Therefore, progressive validation is not necessarily a faithful simulation of a live environment. In fact progressive validation is overly optimistic when the data contains seasonal patterns.</p>
<h2 id=delayed-progressive-validation>Delayed progressive validation</h2>
<p>In a CTR task, the delay between the query $x_i$ and the answer $y_i$ is usually quite small and can be measured in seconds. However, for other tasks, the gap can be quite large because of the nature of the problem. If a model predicts the duration of a taxi trip, then obviously the duration of the trip, is only known once the taxi arrives at the desired destination. However, when using progressive validation, the model is given access to the true duration right after it has made a prediction. If the model is then asked to predict the duration of another trip which departs at a similar time as the previous trip, then it will be cheating because it knows how long the previous trip lasts. In a live environment this situation can&rsquo;t occur because the future is obviously unknown. However, in a local environment this kind of leakage can occur if one is not careful. To accurately simulate a live environment and thus get a reliable estimate of the performance of a model, we thus need to take into account the delay in arrival times between $x_i$ and $y_i$. The problem with progressive validation is that it doesn&rsquo;t take said delay into account.</p>
<p>The gold standard is to have a log file with the arrival times of each set of features $x_i$ and each outcome $y_i$. We can then ask the model to make a prediction when $x_i$ arrives, and update itself with $(x_i, y_i)$ once $y_i$ is available. In a fraud detection system for credit card transactions, $x_i$ would contain details about the transaction, whilst $y_i$ would be made available once a human expert has confirmed the transaction as fraudulent or not. However, a log file might not always be available. Indeed, most of the time datasets do not indicate the times at which both the features and the targets arrived.</p>
<p>A method for alleviating this issue is called &ldquo;delayed progressive validation&rdquo;. I&rsquo;ve added quotes because I actually coined it myself. The short story is that I wanted to publish a paper on the topic. A short while after, during an exchange with <a href=http://albertbifet.com/>Albert Bifet</a>, he told me that his team had very recently published <a href=https://link.springer.com/article/10.1007/s10618-019-00654-y>a paper on the topic</a>. I cursed a tiny bit and decided to write a blog post instead!</p>
<p>Delayed progressive validation is quite intuitive. Instead of updating the model immediately after it has made a prediction, the idea is to update it once the ground truth <em>would</em> be available. This way the model learns and predicts samples without leakage. To do so, we can pick a delay $d > 0$ and append a quadruplet $(i + d, x_i, y_i, \hat{y}_i)$ into a sorted list which we&rsquo;ll call $Q$. Once we reach $i + d$, the model is given access to $y_i$ and can therefore be updated, whilst the metric can be updated with $y_i$ and $\hat{y}_i$. We can check if we&rsquo;ve reached $i + d$ every time a new observation comes in.</p>
<p>For various reasons you might not be able to assign an exact value to $d$. The nice thing is that $d$ can be anything you like, and doesn&rsquo;t necessarily have to be the same for every observation. This provides the flexibility of either using a constant, a random variable, or a value that depends on one or more attributes of $x_i$. For instance, in a credit card fraud detection task, it might be that the delay varies according to the credit card issuer. In the case of taxi trips, $d$ is nothing more than the duration of the trip.</p>
<p>Initially, $Q$ is empty, and grows every time the model makes a prediction. Once the next observation arrives, we loop over $Q$ in insertion order. For each quadruplet in $Q$ which is old enough, we update the model and the metric, before removing the quadruplet from $Q$. Because $Q$ is ordered, we can break the loop over $Q$ whenever a quadruplet is not old enough. Once we have depleted the stream of data, $Q$ will still contain some quadruplets, and so the final step of the procedure is to update the metric with the remaining quadruplets.</p>
<p>On the one hand, delayed progressive validation will perform as many predictions and model updates as progressive validation. Indeed, every observation is used once for making prediction, and once for updating the model. The only added cost comes from inserting $(x_i, y_i, \hat{y}_i, i + d)$ into $Q$ so that $Q$ remains sorted. This can be done in $\mathcal{O}(log(|Q|))$ time by using the bisection method, with $|Q|$ being the length of $Q$. In the special case where the delay $d$ is constant, the bisection method can be avoided because each quadruplet $(x_i, y_i, \hat{y}_i, i + d)$ can simply be inserted at the beginning of $Q$. Other operations, namely comparing timestamps and picking a delay, are trivial. On the other hand, the space complexity is higher than progressive validation because $Q$ has to be maintained in memory.</p>
<p>Naturally, the size of the queue is proportional to the delay. For most cases this shouldn&rsquo;t be an issue because the observations are being processed one at a time, which means that quadruplets are added and dropped from the queue at a very similar rate. You can also place an upper bound on the expected size of $Q$ by looking at the average value of $d$ and the arrival rate, but we&rsquo;ll skip that for the time being. In practice, if the amount of available memory runs out, then $Q$ can be written out to the disk, but this is very much an edge case. Finally, note that progressive validation can be seen as a special case of delayed progressive validation when the delay is set to 0. Indeed, in this case $Q$ will contain at most one element, whilst the predictions and model updates will be perfectly interleaved.</p>
<p>Let&rsquo;s go about implementing this. We&rsquo;ll use Python&rsquo;s <a href=https://docs.python.org/3/library/bisect.html><code>bisect</code></a> module to insert quadruplets into the queue. Each quadruplet is a tuple that stands for a trip. We place the arrival date at the start of the tuple in order to be able to compare trips according to their arrival time. Indeed, Python compares tuples position by position, as explained in <a href=https://stackoverflow.com/questions/5292303/how-does-tuple-comparison-work-in-python>this</a> StackOverflow post.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=kn>import</span> <span class=nn>bisect</span>
<span class=kn>import</span> <span class=nn>datetime</span> <span class=k>as</span> <span class=nn>dt</span>

<span class=k>def</span> <span class=nf>simulate_qa</span><span class=p>(</span><span class=n>X_y</span><span class=p>,</span> <span class=n>departure_dates</span><span class=p>):</span>

    <span class=n>trips_in_progress</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>departure_date</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>departure_dates</span><span class=p>,</span> <span class=n>start</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>

        <span class=c1># Go through the trips and progress and check if they&#39;re finished</span>
        <span class=k>while</span> <span class=n>trips_in_progress</span><span class=p>:</span>
            <span class=n>trip</span> <span class=o>=</span> <span class=n>trips_in_progress</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
            <span class=n>arrival_date</span> <span class=o>=</span> <span class=n>trip</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
            <span class=k>if</span> <span class=n>arrival_date</span> <span class=o>&lt;</span> <span class=n>departure_date</span><span class=p>:</span>
                <span class=k>yield</span> <span class=n>trip</span>
                <span class=k>del</span> <span class=n>trips_in_progress</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
                <span class=k>continue</span>
            <span class=k>break</span>

        <span class=n>xi</span><span class=p>,</span> <span class=n>yi</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=n>X_y</span><span class=p>)</span>

        <span class=c1># Show the features, hide the target</span>
        <span class=k>yield</span> <span class=n>departure_date</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>xi</span><span class=p>,</span> <span class=kc>None</span>

        <span class=c1># Store the trip for later use</span>
        <span class=n>arrival_date</span> <span class=o>=</span> <span class=n>departure_date</span> <span class=o>+</span> <span class=n>dt</span><span class=o>.</span><span class=n>timedelta</span><span class=p>(</span><span class=n>seconds</span><span class=o>=</span><span class=nb>int</span><span class=p>(</span><span class=n>yi</span><span class=p>))</span>
        <span class=n>trip</span> <span class=o>=</span> <span class=p>(</span><span class=n>arrival_date</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>xi</span><span class=p>,</span> <span class=n>yi</span><span class=p>)</span>
        <span class=n>bisect</span><span class=o>.</span><span class=n>insort</span><span class=p>(</span><span class=n>trips_in_progress</span><span class=p>,</span> <span class=n>trip</span><span class=p>)</span>

    <span class=c1># Terminate the rest of the trips in progress</span>
    <span class=k>yield from</span> <span class=n>trips_in_progress</span>
</code></pre></div><p>To differentiate between departures and arrivals, we&rsquo;re yielding a trip with a duration set to <code>None</code>. In other words, a <code>None</code> value implicitely signals a taxi departure which requires a prediction. This also avoids any leakage concerns that may occur. Let&rsquo;s do a quick sanity check to verify that our implementation behaves correctly. The following example also helps to understand and visualise what the above implementation is doing.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=n>time_table</span> <span class=o>=</span> <span class=p>[</span>
    <span class=p>(</span><span class=n>dt</span><span class=o>.</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span>  <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span>  <span class=mi>900</span><span class=p>),</span>
    <span class=p>(</span><span class=n>dt</span><span class=o>.</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span> <span class=mi>1800</span><span class=p>),</span>
    <span class=p>(</span><span class=n>dt</span><span class=o>.</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span>  <span class=mi>300</span><span class=p>),</span>
    <span class=p>(</span><span class=n>dt</span><span class=o>.</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>45</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span>  <span class=mi>400</span><span class=p>),</span>
    <span class=p>(</span><span class=n>dt</span><span class=o>.</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span>  <span class=mi>240</span><span class=p>),</span>
    <span class=p>(</span><span class=n>dt</span><span class=o>.</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>55</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span>  <span class=mi>450</span><span class=p>)</span>
<span class=p>]</span>

<span class=n>X_y</span> <span class=o>=</span> <span class=p>((</span><span class=kc>None</span><span class=p>,</span> <span class=n>duration</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>duration</span> <span class=ow>in</span> <span class=n>time_table</span><span class=p>)</span>
<span class=n>departure_dates</span> <span class=o>=</span> <span class=p>(</span><span class=n>date</span> <span class=k>for</span> <span class=n>date</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>time_table</span><span class=p>)</span>

<span class=k>for</span> <span class=n>date</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>xi</span><span class=p>,</span> <span class=n>yi</span> <span class=ow>in</span> <span class=n>simulate_qa</span><span class=p>(</span><span class=n>X_y</span><span class=p>,</span> <span class=n>departure_dates</span><span class=p>):</span>

    <span class=k>if</span> <span class=n>yi</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>date</span><span class=si>}</span><span class=s1> - trip #</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s1> departs&#39;</span><span class=p>)</span>
    <span class=k>else</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>date</span><span class=si>}</span><span class=s1> - trip #</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s1> arrives after </span><span class=si>{</span><span class=n>yi</span><span class=si>}</span><span class=s1> seconds&#39;</span><span class=p>)</span>
</code></pre></div><pre tabindex=0><code>2020-01-01 20:00:00 - trip #1 departs
2020-01-01 20:10:00 - trip #2 departs
2020-01-01 20:15:00 - trip #1 arrives after 900 seconds
2020-01-01 20:20:00 - trip #3 departs
2020-01-01 20:25:00 - trip #3 arrives after 300 seconds
2020-01-01 20:40:00 - trip #2 arrives after 1800 seconds
2020-01-01 20:45:00 - trip #4 departs
2020-01-01 20:50:00 - trip #5 departs
2020-01-01 20:51:40 - trip #4 arrives after 400 seconds
2020-01-01 20:54:00 - trip #5 arrives after 240 seconds
2020-01-01 20:55:00 - trip #6 departs
2020-01-01 21:02:30 - trip #6 arrives after 450 seconds
</code></pre><p>Now let&rsquo;s re-evaluate our model with delayed progressive cross-validation. There is very little we have to modify in the existing evaluation code. The biggest change is that we need to store the predictions while we wait for their associated ground truths to be available. We can release the prediciton from memory once the relevant ground truth arrives &ndash; i.e. a taxi arrives.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>linear_model</span>

<span class=n>sgd</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>SGDRegressor</span><span class=p>(</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=s1>&#39;constant&#39;</span><span class=p>,</span>
    <span class=n>eta0</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
<span class=p>)</span>
<span class=n>scores</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>exp_scores</span> <span class=o>=</span> <span class=p>[]</span>
<span class=n>running_mae</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>exp_mae</span> <span class=o>=</span> <span class=mi>0</span>

<span class=n>X_y</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>(),</span> <span class=n>y</span><span class=o>.</span><span class=n>to_numpy</span><span class=p>())</span>
<span class=n>departure_dates</span> <span class=o>=</span> <span class=n>taxis</span><span class=p>[</span><span class=s1>&#39;pickup_datetime&#39;</span><span class=p>]</span>
<span class=n>trips</span> <span class=o>=</span> <span class=n>simulate_qa</span><span class=p>(</span><span class=n>X_y</span><span class=p>,</span> <span class=n>departure_dates</span><span class=p>)</span>

<span class=n>predictions</span> <span class=o>=</span> <span class=p>{}</span>
<span class=n>n_preds</span> <span class=o>=</span> <span class=mi>0</span>

<span class=k>for</span> <span class=n>date</span><span class=p>,</span> <span class=n>trip_id</span><span class=p>,</span> <span class=n>xi</span><span class=p>,</span> <span class=n>yi</span> <span class=ow>in</span> <span class=n>trips</span><span class=p>:</span>

    <span class=k>if</span> <span class=n>yi</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>

        <span class=c1># Make a prediction</span>
        <span class=k>try</span><span class=p>:</span>
            <span class=n>y_pred</span> <span class=o>=</span> <span class=n>sgd</span><span class=o>.</span><span class=n>predict</span><span class=p>([</span><span class=n>xi</span><span class=p>])[</span><span class=mi>0</span><span class=p>]</span>
        <span class=k>except</span> <span class=n>exceptions</span><span class=o>.</span><span class=n>NotFittedError</span><span class=p>:</span>  <span class=c1># happens if partial_fit hasn&#39;t been called yet</span>
            <span class=n>y_pred</span> <span class=o>=</span> <span class=mf>0.</span>

        <span class=n>predictions</span><span class=p>[</span><span class=n>trip_id</span><span class=p>]</span> <span class=o>=</span> <span class=n>y_pred</span>
        <span class=k>continue</span>

    <span class=c1># Update the running mean absolute error</span>
    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>predictions</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=n>trip_id</span><span class=p>)</span>
    <span class=n>mae</span> <span class=o>=</span> <span class=nb>abs</span><span class=p>(</span><span class=n>y_pred</span> <span class=o>-</span> <span class=n>yi</span><span class=p>)</span>
    <span class=n>n_preds</span> <span class=o>+=</span> <span class=mi>1</span>
    <span class=n>running_mae</span> <span class=o>+=</span> <span class=p>(</span><span class=n>mae</span> <span class=o>-</span> <span class=n>running_mae</span><span class=p>)</span> <span class=o>/</span> <span class=n>n_preds</span>

    <span class=c1># Update the exponential moving average of the MAE</span>
    <span class=n>exp_mae</span> <span class=o>=</span> <span class=mf>.1</span> <span class=o>*</span> <span class=n>mae</span> <span class=o>+</span> <span class=mf>.9</span> <span class=o>*</span> <span class=n>exp_mae</span>

    <span class=c1># Store the metric at the current time</span>
    <span class=k>if</span> <span class=n>trip_id</span> <span class=o>&gt;=</span> <span class=mi>10</span><span class=p>:</span>
        <span class=n>scores</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>date</span><span class=p>,</span> <span class=n>running_mae</span><span class=p>))</span>
        <span class=n>exp_scores</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>date</span><span class=p>,</span> <span class=n>exp_mae</span><span class=p>))</span>

    <span class=c1># Finally, make the model learn</span>
    <span class=n>sgd</span><span class=o>.</span><span class=n>partial_fit</span><span class=p>([</span><span class=n>xi</span><span class=p>],</span> <span class=p>[</span><span class=n>yi</span><span class=p>])</span>

    <span class=k>if</span> <span class=n>n_preds</span> <span class=o>==</span> <span class=mi>38000</span><span class=p>:</span>
        <span class=k>break</span>
</code></pre></div><p>I agree that the code can seem a bit verbose. However, it&rsquo;s very easy to generalise and the logic can be encapsulated in a higher-level function, including the <code>simulate_qa</code> function. In fact, the <a href=https://github.com/creme-ml/creme><code>creme</code></a> library has a <code>progressive_val_score</code> function in it&rsquo;s <code>model_selection</code> module that does just that. Now let&rsquo;s see what that the performance looks like on a chart.</p>
<details>
<summary>Click to see the code</summary>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>

<span class=n>hours</span> <span class=o>=</span> <span class=n>mdates</span><span class=o>.</span><span class=n>HourLocator</span><span class=p>(</span><span class=n>interval</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
<span class=n>h_fmt</span> <span class=o>=</span> <span class=n>mdates</span><span class=o>.</span><span class=n>DateFormatter</span><span class=p>(</span><span class=s1>&#39;%A %H:%M&#39;</span><span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span>
    <span class=p>[</span><span class=n>d</span><span class=o>.</span><span class=n>to_datetime64</span><span class=p>()</span> <span class=k>for</span> <span class=n>d</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>scores</span><span class=p>],</span>
    <span class=p>[</span><span class=n>s</span> <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>s</span> <span class=ow>in</span> <span class=n>scores</span><span class=p>],</span>
    <span class=n>linewidth</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
    <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Running average&#39;</span><span class=p>,</span>
    <span class=n>alpha</span><span class=o>=</span><span class=mf>.7</span>
<span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span>
    <span class=p>[</span><span class=n>d</span><span class=o>.</span><span class=n>to_datetime64</span><span class=p>()</span> <span class=k>for</span> <span class=n>d</span><span class=p>,</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>exp_scores</span><span class=p>],</span>
    <span class=p>[</span><span class=n>s</span> <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>s</span> <span class=ow>in</span> <span class=n>exp_scores</span><span class=p>],</span>
    <span class=n>linewidth</span><span class=o>=</span><span class=mf>.3</span><span class=p>,</span>
    <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Exponential moving average&#39;</span><span class=p>,</span>
    <span class=n>alpha</span><span class=o>=</span><span class=mf>.7</span>
<span class=p>)</span>

<span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylim</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>600</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>set_major_locator</span><span class=p>(</span><span class=n>hours</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>xaxis</span><span class=o>.</span><span class=n>set_major_formatter</span><span class=p>(</span><span class=n>h_fmt</span><span class=p>)</span>
<span class=n>fig</span><span class=o>.</span><span class=n>autofmt_xdate</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Time&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Mean absolute error&#39;</span><span class=p>,</span> <span class=n>labelpad</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
<span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Delayed progressive validation&#39;</span><span class=p>,</span> <span class=n>pad</span><span class=o>=</span><span class=mi>16</span><span class=p>)</span>
<span class=n>fig</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;delayed_progressive_validation.svg&#39;</span><span class=p>,</span> <span class=n>bbox_inches</span><span class=o>=</span><span class=s1>&#39;tight&#39;</span><span class=p>)</span>
</code></pre></div></details>
<p><img src=/img/blog/online-learning-evaluation/delayed_progressive_validation.svg alt=delayed_progressive_validation></p>
<p>If this looks similar to the previous chart, that&rsquo;s because it is. The errors line are slightly higher but that&rsquo;s about it. For this particular dataset, taking into account the delay doesn&rsquo;t affect the metric too much. But that conclusion may be different for other datasets. The point is with delayed progressive validation, we are now 100% sure that our estimate of the model&rsquo;s performance is reliable. We are certain of this fact because delayed progressive validation reproduces the real state of things by taking into account the order in which events transpire. Cross-validation and plain progressive validation, on the other, do not.</p>
<p>I recommend for futher reading the papers I mentionned throughout this post, namely:</p>
<ul>
<li><a href=https://dl.acm.org/doi/pdf/10.1145/307400.307439>Beating the hold-out: Bounds for K-fold and progressive cross-validation</a></li>
<li><a href=https://link.springer.com/article/10.1007/s10618-019-00654-y>Delayed labelling evaluation for data streams</a></li>
<li><a href=https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/41159.pdf>Ad Click Prediction: a View from the Trenches</a>, in particular subsection 5.1.</li>
</ul>
<p>Peace out.</p>
</div>
<script type=text/javascript>var s=document.createElement('script');s.setAttribute('src','https://utteranc.es/client.js'),s.setAttribute('repo','MaxHalford/maxhalford.github.io'),s.setAttribute('issue-term','pathname'),s.setAttribute('crossorigin','anonymous'),s.setAttribute('async',null),s.setAttribute('theme','github-light'),document.body.appendChild(s)</script>
<div class=footer>
<div class=do-the-thing>
<div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>
Back to the top
</div>
</div>
</div>
<script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.0/elevator.min.js></script>
<script>var elementButton=document.querySelector('.elevator'),elevator=new Elevator({element:elementButton,mainAudio:'/music/elevator.mp3',endAudio:'/music/ding.mp3'})</script>
<style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style>
<div class=site-footer>
<div class=site-footer-item>
<a href=https://github.com/MaxHalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M8 0C3.58.0.0 3.582.0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385.0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953.0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117.0.0.67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147.0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48.0 1.07-.01 1.93-.01 2.19.0.21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://linkedin.com/in/maxhalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235.0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4.0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762.0-1.376-.617-1.376-1.377.0-.758.614-1.375 1.376-1.375.76.0 1.376.617 1.376 1.375.0.76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816.0H1.18C.528.0.0.516.0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652.0 1.185-.516 1.185-1.153V1.153C16 .516 15.467.0 14.815.0z" fill-rule="nonzero"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://twitter.com/halford_max><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812.0-3.282 1.47-3.282 3.28.0.26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21.0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26.0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04.0 9.34-5 9.34-9.33.0-.14.0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://kaggle.com/maxhalford><span class=inline-svg><svg role="img" viewBox="0 0 26 26" xmlns="http://www.w3.org/2000/svg"><title>Kaggle icon</title><path fill="currentcolor" d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187.0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236.0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234.0.351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144.0.236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.358"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href="https://scholar.google.com/citations?user=erRNNi0AAAAJ&hl=en"><span class=inline-svg><svg viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952.0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584.0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288.0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962.0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zm762.098 382.384c-64.358 64.424-141.86 96.57-232.572 96.57H329.144c-90.712.0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572V182.859c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712.0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159V762.033c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162.0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53.0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476.0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86.0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908.0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426.0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432.0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234.0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048.0 6.642.19 12.492.672 18.974H292.574l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382.0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994.0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376.0 103.05 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382.0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.05 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=/files/resume_max_halford.pdf><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 392.533 392.533" style="enable-background:new 0 0 392.533 392.533"><g><g><path fill="currentcolor" d="M292.396 324.849H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 329.697 298.473 324.849 292.396 324.849z"/></g></g><g><g><path fill="currentcolor" d="M292.396 277.01H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 281.859 298.473 277.01 292.396 277.01z"/></g></g><g><g><path fill="currentcolor" d="M196.137 45.834c-25.859.0-46.998 21.075-46.998 46.998.0 25.859 21.139 46.933 46.998 46.933s46.998-21.075 46.998-46.998-21.139-46.933-46.998-46.933zm0 72.017c-13.77.0-25.083-11.313-25.083-25.083s11.248-25.083 25.083-25.083 25.083 11.313 25.083 25.083c0 13.769-11.313 25.083-25.083 25.083z"/></g></g><g><g><path fill="currentcolor" d="M258.521 163.362c-39.887-15.515-84.752-15.515-124.638.0-13.059 5.107-21.786 18.101-21.786 32.388v44.347c-.065 6.012 4.849 10.925 10.861 10.925h146.424c6.012.0 10.925-4.848 10.925-10.925V195.75C280.307 181.463 271.58 168.469 258.521 163.362zm0 65.874H133.883v-33.422c0-5.301 3.168-10.214 7.887-12.024 34.844-13.511 74.02-13.511 108.865.0 4.719 1.875 7.887 6.659 7.887 12.024v33.422z"/></g></g><g><g><path fill="currentcolor" d="M313.083.0H131.491c-8.404.0-16.291 3.232-22.238 9.18L57.018 61.414c-5.947 5.948-9.18 13.834-9.18 22.238v277.333c0 17.39 14.158 31.547 31.547 31.547h233.762c17.39.0 31.547-14.158 31.547-31.547V31.547C344.501 14.158 330.343.0 313.083.0zM112.032 37.236v27.022H85.01l27.022-27.022zm210.683 79.58h-40.598c-6.012.0-10.925 4.849-10.925 10.925.0 6.012 4.848 10.925 10.925 10.925h40.598v19.394h-14.869c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h14.869v181.139c0 5.366-4.331 9.697-9.632 9.697H79.192c-5.301.0-9.632-4.331-9.632-9.632V86.044h53.398c6.012.0 10.925-4.848 10.925-10.925V21.721h179.2c5.301.0 9.632 4.331 9.632 9.632v85.463z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://play.spotify.com/user/1166811350><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 168 168"><path fill="currentcolor" d="m83.996.277C37.747.277.253 37.77.253 84.019c0 46.251 37.494 83.741 83.743 83.741 46.254.0 83.744-37.49 83.744-83.741.0-46.246-37.49-83.738-83.745-83.738l.001-.004zm38.404 120.78c-1.5 2.46-4.72 3.24-7.18 1.73-19.662-12.01-44.414-14.73-73.564-8.07-2.809.64-5.609-1.12-6.249-3.93-.643-2.81 1.11-5.61 3.926-6.25 31.9-7.291 59.263-4.15 81.337 9.34 2.46 1.51 3.24 4.72 1.73 7.18zm10.25-22.805c-1.89 3.075-5.91 4.045-8.98 2.155-22.51-13.839-56.823-17.846-83.448-9.764-3.453 1.043-7.1-.903-8.148-4.35-1.04-3.453.907-7.093 4.354-8.143 30.413-9.228 68.222-4.758 94.072 11.127 3.07 1.89 4.04 5.91 2.15 8.976v-.001zm.88-23.744c-26.99-16.031-71.52-17.505-97.289-9.684-4.138 1.255-8.514-1.081-9.768-5.219-1.254-4.14 1.08-8.513 5.221-9.771 29.581-8.98 78.756-7.245 109.83 11.202 3.73 2.209 4.95 7.016 2.74 10.733-2.2 3.722-7.02 4.949-10.73 2.739z"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=mailto:maxhalford25@gmail.com><span class=inline-svg><svg viewBox="0 0 15 20" xmlns="http://www.w3.org/2000/svg"><title>mail</title><path fill="currentcolor" d="M0 4v8c0 .55.45 1 1 1h12c.55.0 1-.45 1-1V4c0-.55-.45-1-1-1H1c-.55.0-1 .45-1 1zm13 0L7 9 1 4h12zM1 5.5l4 3-4 3v-6zM2 12l3.5-3L7 10.5 8.5 9l3.5 3H2zm11-.5-4-3 4-3v6z" fill="#000" fill-rule="evenodd"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=/index.xml><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M12.8 16C12.8 8.978 7.022 3.2.0 3.2V0c8.777.0 16 7.223 16 16h-3.2zM2.194 11.61c1.21.0 2.195.985 2.195 2.196.0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017.0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818.0 10.606 4.79 10.606 10.607z"/></svg>
</span>
</a>
</div>
</div>
<div style=margin-bottom:50px;display:flex;justify-content:center>
<iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=35 width=116 style=border:0></iframe>
</div>
</div>
</div>
</article>
<script></script>
</body>
</html>