<!doctype html><html lang=en><head><script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script><meta charset=utf-8><meta name=generator content="Hugo 0.110.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/subsampling-1/"><link rel=canonical href=https://maxhalford.github.io/blog/subsampling-1/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"Subsampling a training set to match a test set - Part 1","headline":"Subsampling a training set to match a test set - Part 1","description":"Edit: it\u0026rsquo;s 2022 and I still haven\u0026rsquo;t written a part 2. That\u0026rsquo;s because I believe this problem is easily solved with adversarial validation.\nSome friends and I recently qualified for the final of the 2017 edition of the Data Science Game competition. The first part was a Kaggle competition with data provided by Deezer. The problem was a binary classification task where one had to predict if a user was going to listen to a song that was proposed to him.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2017","datePublished":"2017-06-19 00:00:00 \u002b0000 UTC","dateModified":"2017-06-19 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/subsampling-1\/","keywords":["machine-learning"]}</script><title>Subsampling a training set to match a test set - Part 1 â€¢ Max Halford</title><meta property="og:title" content="Subsampling a training set to match a test set - Part 1 â€¢ Max Halford"><meta property="og:type" content="article"><meta name=description content="Edit: it&rsquo;s 2022 and I still haven&rsquo;t written a part 2. That&rsquo;s because I believe this problem is easily solved with adversarial validation.
Some friends and I recently qualified for the final of the 2017 edition of the Data Science Game competition. The first part was a Kaggle competition with data provided by Deezer. The problem was a binary classification task where one had to predict if a user was going to listen to a song that was proposed to him."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=header><header class=header-parts><div class="signatures site-title"><a href=/>Max Halford ðŸ¦”</a></div><div class=header-links><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></header></div><header class=post-header><h1 class=post-title>Subsampling a training set to match a test set - Part 1</h1><div class="row post-desc"><div class="col-xs-12 post-desc-items"><time class=post-date datetime="2017-06-19 00:00:00 UTC">2017-06-19</time>
<span class=posts-line-tag>machine-learning</span></div></div></header><div class="post-content markdown-body"><p><em>Edit: it&rsquo;s 2022 and I still haven&rsquo;t written a part 2. That&rsquo;s because I believe this problem is easily solved with <a href=https://www.kaggle.com/carlmcbrideellis/what-is-adversarial-validation>adversarial validation</a></em>.</p><p>Some friends and I recently qualified for the final of the 2017 edition of the <a href=http://www.datasciencegame.com>Data Science Game</a> competition. The first part was a Kaggle competition with data provided by Deezer. The problem was a binary classification task where one had to predict if a user was going to listen to a song that was proposed to him. Like many teams we extracted clever features and trained an XGBoost classifier, classic. However, the one special thing we did was to subsample our training set so that it was more representative of the test set.</p><p>One of the basic requirements for a machine learning model to do well is that the data it is trained on and the one it predicts should originate from the same distribution. For example there isn&rsquo;t much sense in training a model on users who are in their 20s when the users in the test set who are in their 60s. Of course this is never really case, however the previous statement is also somewhat true if users in the test set are in their 30s. Machine learning models learn data representations, or in other words distributions. For each feature, having a distribution in the training set that matches the one in the test set helps a lot.</p><p>A shift in the test set distribution can occur for many reasons. For example when Facebook split it&rsquo;s messaging system to a new app called Messenger the average time people spent on the main app probably decreased by a lot, but it probably also didn&rsquo;t impact churn, quite the contrary I imagine. Shifts like these can occur through time and make part of historical data useless after a while. In the case of the Deezer competition, we had a feature which measured the number of songs a user had listened to up to when the listen occurred. This feature was on average lower than in the training set. In both datasets the data followed an exponential distribution but it was more accentuated in the test set. After subsampling the training set so that this particular feature had the same distribution as in the test set we gained a few percentage points in ROC AUC and climbed the ladder by around 20 spots.</p><p>Here is an example of what the difference in distribution looks like. Here and in the rest of the blog I&rsquo;ll use <a href=https://www.wikiwand.com/en/Kernel_density_estimation>KDEs</a> to represent distributions.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>seaborn</span> <span class=k>as</span> <span class=nn>sns</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>style</span><span class=o>.</span><span class=n>use</span><span class=p>(</span><span class=s1>&#39;fivethirtyeight&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>exponential</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>100000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>exponential</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>10000</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>plt</span><span class=o>.</span><span class=n>xkcd</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>sns</span><span class=o>.</span><span class=n>kdeplot</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Train&#39;</span><span class=p>,</span> <span class=n>lw</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>sns</span><span class=o>.</span><span class=n>kdeplot</span><span class=p>(</span><span class=n>test</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Test&#39;</span><span class=p>,</span> <span class=n>lw</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>tick_params</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;both&#39;</span><span class=p>,</span> <span class=n>which</span><span class=o>=</span><span class=s1>&#39;major&#39;</span><span class=p>,</span> <span class=n>labelsize</span><span class=o>=</span><span class=mi>18</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>20</span><span class=p>);</span>
</span></span></code></pre></div><p><img src=/img/blog/subsampling-1/expo-example.png alt=expo-example></p><p>We want to subsample, say 50000, observations in our training set with the requirement that the subsample&rsquo;s distribution will match the test set&rsquo;s one. To do so we have to weight each observation in the training set to express how much it will contribute to matching the test set&rsquo;s distribution. Intuitively values that are common in the test set but are rare in the training should have a higher weight.</p><p>The algorithm that follows should be quite straightforward and is quite fast.</p><ol><li>Compute $n$ equal frequency bins from the test</li><li>Assign each observation in the training set to the matching bin</li><li>For each training set bin $b_i$, count it&rsquo;s size</li><li>Assign weight $\frac{1}{size(b_i)}$ to each observation in the training set belonging to bin $b_i$</li><li>Sample $k$ observations from the training set with the weights assigned</li></ol><p>The algorithm has two parameters which are the number of bins ($n$) and the size of the subsample ($k$). To generate the equal width bins we can use percentiles.</p><p>Now to the code, in Python of course!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>SAMPLE_SIZE</span> <span class=o>=</span> <span class=mi>50000</span>
</span></span><span class=line><span class=cl><span class=n>N_BINS</span> <span class=o>=</span> <span class=mi>300</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Obtain `N_BINS` equal frequency bins, in other words percentiles</span>
</span></span><span class=line><span class=cl><span class=n>step</span> <span class=o>=</span> <span class=mi>100</span> <span class=o>/</span> <span class=n>N_BINS</span>
</span></span><span class=line><span class=cl><span class=n>test_percentiles</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>np</span><span class=o>.</span><span class=n>percentile</span><span class=p>(</span><span class=n>test</span><span class=p>,</span> <span class=n>q</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>q</span> <span class=ow>in</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>start</span><span class=o>=</span><span class=n>step</span><span class=p>,</span> <span class=n>stop</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>step</span><span class=o>=</span><span class=n>step</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Match each observation in the training set to a bin</span>
</span></span><span class=line><span class=cl><span class=n>train_bins</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>digitize</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>test_percentiles</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Count the number of values in each training set bin</span>
</span></span><span class=line><span class=cl><span class=n>train_bin_counts</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>bincount</span><span class=p>(</span><span class=n>train_bins</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Weight each observation in the training set based on which bin it is in</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>train_bin_counts</span><span class=p>[</span><span class=n>x</span><span class=p>]</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>train_bins</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Make the weights sum up to 1</span>
</span></span><span class=line><span class=cl><span class=n>weights_norm</span> <span class=o>=</span> <span class=n>weights</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sample</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=n>SAMPLE_SIZE</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=n>weights_norm</span><span class=p>,</span> <span class=n>replace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>plt</span><span class=o>.</span><span class=n>xkcd</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>14</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>sns</span><span class=o>.</span><span class=n>kdeplot</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Train&#39;</span><span class=p>,</span> <span class=n>lw</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>sns</span><span class=o>.</span><span class=n>kdeplot</span><span class=p>(</span><span class=n>test</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Test&#39;</span><span class=p>,</span> <span class=n>lw</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>sns</span><span class=o>.</span><span class=n>kdeplot</span><span class=p>(</span><span class=n>sample</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Train resampled&#39;</span><span class=p>,</span> <span class=n>lw</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>ls</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>tick_params</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;both&#39;</span><span class=p>,</span> <span class=n>which</span><span class=o>=</span><span class=s1>&#39;major&#39;</span><span class=p>,</span> <span class=n>labelsize</span><span class=o>=</span><span class=mi>18</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>20</span><span class=p>);</span>
</span></span></code></pre></div><p><img src=/img/blog/subsampling-1/expo-resample.png alt=expo-resample></p><p>You can see that the subsample (the dashed line) has a similar distribution to the test set (the red). This is exactly what we during the competition. Our initial dataset had around 3 million rows and we actually used a subsample of 1.3 million rows. Less data but more representative data! The number of bins that you use doesn&rsquo;t matter too much in my experience, and the lower that number the faster the algorithm runs. As for the size of the subsample, the less your training set and test set distributions are difference, the less you will be able to subsample - at least if you are doing it without replacement, which is the only way that makes sense if you don&rsquo;t want to have duplicates.</p><p>I added the subsampling algorithm to my data science package called <a href=https://github.com/MaxHalford/xam>xam</a>. The next step will be to generalize the weighting and subsampling to multiple dimensions, but that&rsquo;s for part 2 because I haven&rsquo;t figured it out yet!</p></div><script type=text/javascript>var s=document.createElement("script");s.setAttribute("src","https://utteranc.es/client.js"),s.setAttribute("repo","MaxHalford/maxhalford.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",null),s.setAttribute("theme","github-light"),document.body.appendChild(s)</script><div class=footer><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.1/elevator.min.js></script>
<script>var elementButton=document.querySelector(".elevator"),elevator=new Elevator({element:elementButton,mainAudio:"/music/elevator.mp3",endAudio:"/music/ding.mp3"})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto;margin-bottom:30px}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=related-content><h3 style=margin-top:10px!important;margin-bottom:10px!important>Related posts</h3><ul style=margin-top:0><li><a href=/blog/naive-bayes/>The NaÃ¯ve Bayes classifier</a></li><li><a href=/blog/genetic-algorithms-introduction/>An introduction to genetic algorithms</a></li></ul></div></div></div></article><script></script></body></html>