<!doctype html><html lang=en>
<head>
<script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.89.0">
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=author content="Max Halford">
<meta property="og:url" content="https://maxhalford.github.io/blog/naive-bayes/">
<link rel=canonical href=https://maxhalford.github.io/blog/naive-bayes/>
<link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶î</text></svg>">
<script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"The Na√Øve Bayes classifier","headline":"The Na√Øve Bayes classifier","description":"The objective of a classifier is to decide to which class (also called label) to assign an observation based on observed data. In supervised learning, this is done by taking into account previous classifications. In other words if we know that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the training set.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2015","datePublished":"2015-09-10 00:00:00 \u002b0000 UTC","dateModified":"2015-09-10 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/naive-bayes\/","keywords":[]}</script>
<title>The Na√Øve Bayes classifier - Max Halford</title>
<meta property="og:title" content="The Na√Øve Bayes classifier - Max Halford">
<meta property="og:type" content="article">
<meta name=description content="The objective of a classifier is to decide to which class (also called label) to assign an observation based on observed data. In supervised learning, this is done by taking into account previous classifications. In other words if we know that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the training set.">
<link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css>
<link rel=stylesheet href=/css/github-markdown.min.css>
<link rel=stylesheet href=/css/highlight/github.css>
<link rel=stylesheet href=/css/index.css>
<link rel=preconnect href=https://fonts.gstatic.com>
<link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0,tags:'ams'},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll('mjx-container').forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head>
<body>
<article class=post id=article>
<div class="row center-xs" style=text-align:left>
<div class="col-xs-12 col-sm-10 col-md-7 col-lg-5">
<div class=post-header>
<header>
<div class="signatures site-title">
<a href=/>Max Halford</a>
</div>
</header>
<div class="row end-xs">
<div>
<a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a>
</div>
</div>
<div class=header-line></div>
</div>
<header class=post-header>
<h1 class=post-title>The Na√Øve Bayes classifier</h1>
<div class="row post-desc">
<div class=col-xs-12>
<time class=post-date datetime="2015-09-10 00:00:00 UTC">
2015-09-10 ¬∑ 1783 words
</time>
</div>
</div>
</header>
<div class="post-content markdown-body">
<p>The objective of a classifier is to decide to which <em>class</em> (also called <em>label</em>) to assign an observation based on observed data. In <em>supervised learning</em>, this is done by taking into account previous classifications. In other words if we <em>know</em> that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the <em>training set</em>.</p>
<p>In mathematical terms say $X$ is a list of attributes for each one of a group of $n$ observations. In the training set, each observation $X_i$ has a label $Y_i$ associated to it. The question is which $Y$ to associate to a new $X_i$?</p>
<p>The Na√Øve Bayes method is a probabilistic approach. It assigns a probability to each label $Y_i$ based on the training set. The best guess is the label with the highest probability.</p>
<h2 id=the-mathematics>The mathematics</h2>
<p>The general idea is to know $P_F(C)$ where $C$ is a discrete class/label and $F$ is a list of <em>features</em>. For example if the class was an author and the features were words in a text the conditional probability $P_F(C)$ could be read as</p>
<blockquote>
<p>&ldquo;What is the probability that a text is written by author $C$ given that words $F$ are in the text?&rdquo;</p>
</blockquote>
<p>Good question. This is where the &ldquo;Bayes&rdquo; in the title comes from <a href="https://www.wikiwand.com/en/Bayes'_theorem">Bayes' theorem</a> gives</p>
<p>$$P_F(C_j) = \frac{P(C_j) \times P_{C_j}(F)}{P(F)}$$</p>
<ul>
<li>$P(C_j)$ is called the <em>prior</em>. It is the probability we are looking for <em>before</em> the data is being considered. In philosophy the prior is the state of the knowledge on a subject before an experience is performed.</li>
<li>$P_{C_j}(F)$ is the frequency at which $F$ appeared for $C_j$. This is the observed data in the training set.</li>
<li>$P(F)$ represents the support $F$ provides for $C$.</li>
</ul>
<p>In a sense Bayes' theorem is a &ldquo;magic&rdquo; formula which gives the probability of having A (for example an author) based on the fact that we already B (for example some words). It does because we already know at what frequency B appears when A does. In other words, if the word &ldquo;Quiditch&rdquo; appears in a sentence, you have a good chance that the sentence belongs to a Harry Potter book because <em>you already know</em> that it frequently appears in Harry Potter books.</p>
<p>If we calculate $P_F(C_j)$ for every $j$ then we can sort them based on their value and choose one of them. This is called a <em>decision rule</em>. Not surprisingly the most common decision rule is to take the highest probability, this is called <em>maximum a posteriori</em> (MAP).</p>
<p>Now for calculating $P_F(C_j)$:</p>
<ul>
<li>$P(C_j)$ is simply the frequency of the $C_j$ class.</li>
<li>$P(F)$ doesn&rsquo;t depend on $C$ and so it doesn&rsquo;t have to be calculated. Indeed $P(F)$ is the same for every $C$ and won&rsquo;t affect the order of all the $P_F(C_j)$.</li>
<li>$P_{C_j}(F)$ is the only (slighly) tricky probability to compute.</li>
</ul>
<p>$P_{C_j}(F)$ is the probability of having a list of features in a text. For the sake of simplicity we can suppose that the occurrences of features are independent from each other (this is why the classifier is considered &ldquo;Na√Øve&rdquo;). Of course this is false, indeed text often use semantic fields. However it has been shown that the classifier does well even if the assumption seems foolish. Because the events are independent the mathematics become very convenient. Indeed if there are $p$ features in a text the probability that a document belongs to a class can now be written down as</p>
<p>$$P_{C_j}(F) = \prod_{k=1}^p P_{C_j}(f_k) = \prod_{k=1}^p \frac{P(C_j \cap f_k)}{P(C_j)}$$</p>
<p>It&rsquo;s important to understand that this isn&rsquo;t true if the features occurrences are not considered independent. For example, the events $A$ &ldquo;It&rsquo;s raining&rdquo; and $B$ &ldquo;It&rsquo;s cold&rdquo; are not independent. In a probabilistic sense, the event $P(A \cap B)$ (&ldquo;It&rsquo;s cold and rainy&rdquo;) is <em>not</em> equal to $P(A) \times P(B)$. This is because most of the time one implies the other. In our case it&rsquo;s mathematically convenient to consider the independence of events, that&rsquo;s all.</p>
<h2 id=python-implementation>Python implementation</h2>
<p>If you want to run the code that will come up yourself, you can simply copy/paste it all into your text editor, it will work out of the box. However you will have to download the pandas module in order to read a CSV dataframe.</p>
<p>I like using classes, you really can make most of <a href=https://www.wikiwand.com/en/Object-oriented_programming>object-oriented programming</a> for creating tidy packages. In this case there are two classes.</p>
<ul>
<li>A <code>Counter</code> class for parsing the texts and estimating the probabilities.</li>
<li>A <code>NaiveBayes</code> class for predicting the class of new texts which inherits from <code>Counter</code>.</li>
</ul>
<p>The main advantage of having a <code>Counter</code> class is that it can be reused for different classifiers. In machine learning in general it&rsquo;s a good idea to split feature extraction and prediction. This is the first thing to code.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>Counter</span><span class=p>:</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dataframe</span><span class=p>,</span> <span class=n>features</span><span class=p>,</span> <span class=n>textCol</span><span class=p>,</span> <span class=n>classCol</span><span class=p>):</span>
        <span class=s1>&#39;&#39;&#39;
</span><span class=s1>        Extract the features of a dataframe&#39;s column and classify
</span><span class=s1>        them according to another dataframe&#39;s column.
</span><span class=s1>        &#39;&#39;&#39;</span>
        <span class=c1># Keep the same feature extraction function</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>featureFunction</span> <span class=o>=</span> <span class=n>features</span>
        <span class=c1># Counts of features in a particular class</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>features</span> <span class=o>=</span> <span class=p>{}</span>
        <span class=c1># Counts of class occurences</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span> <span class=o>=</span> <span class=p>{}</span>
        <span class=c1># Classify the dataframe</span>
        <span class=n>dataframe</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>classify</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
                        <span class=n>args</span><span class=o>=</span><span class=p>(</span><span class=n>textCol</span><span class=p>,</span> <span class=n>classCol</span><span class=p>,))</span>
</code></pre></div><p>The <code>Counter</code> class takes 4 arguments. The first is a dataframe which should contain a column for the texts (3rd argument) and a column for the classes (4th argument). The <code>features</code> argument is a function that extracts the desired features from a text; it can be anything as long as it returns an iterable (list, set, array). For example one could extract <a href=https://www.wikiwand.com/en/Word_stem>stemmed</a> and/or lower cased words. We will get to that later on. In order to compute the probabilities described above we have to count the features and class occurences, the best way to do this is to use dictionaries. The last line parses all the dataframe with the following procedure.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python>    <span class=k>def</span> <span class=nf>classify</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>row</span><span class=p>,</span> <span class=n>textCol</span><span class=p>,</span> <span class=n>classCol</span><span class=p>):</span>
        <span class=s1>&#39;&#39;&#39;
</span><span class=s1>        Classify a dataframe row based on a text column and a class
</span><span class=s1>        column and a feature function.
</span><span class=s1>        &#39;&#39;&#39;</span>
        <span class=c1># Extract the category of the row</span>
        <span class=n>classification</span> <span class=o>=</span> <span class=n>row</span><span class=p>[</span><span class=n>classCol</span><span class=p>]</span>
        <span class=c1># Initialize the category count if it doesn&#39;t exist yet</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=o>.</span><span class=n>setdefault</span><span class=p>(</span><span class=n>classification</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
        <span class=c1># Increment the count for the category</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=p>[</span><span class=n>classification</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
        <span class=c1># Transform the text column into an iterable of features</span>
        <span class=k>for</span> <span class=n>feature</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>featureFunction</span><span class=p>(</span><span class=n>row</span><span class=p>[</span><span class=n>textCol</span><span class=p>]):</span>
            <span class=c1># Initialize the feature count if doesn&#39;t exist yet</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=o>.</span><span class=n>setdefault</span><span class=p>(</span><span class=n>feature</span><span class=p>,</span> <span class=p>{})</span>
            <span class=c1># Initialize the feature&#39;s category count</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=n>feature</span><span class=p>]</span><span class=o>.</span><span class=n>setdefault</span><span class=p>(</span><span class=n>classification</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
            <span class=c1># Increment the feature&#39;s category count</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=n>feature</span><span class=p>][</span><span class=n>classification</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</code></pre></div><p>The procedure takes a row as argument and analyses the text and class column. If the class has already appeared then it&rsquo;s count is incremented, else it begins at 1. The same goes for every feature extracted from the text. The <code>features</code> dictionary has two levels of depth, one for the feature and one for the class. With this implementation it&rsquo;s very easy to get the occurences of a feature in a text of a specific category: <code>features[f][c]</code>.</p>
<p>Now that the texts have been parsed the features we need a function that returns $P_{C_j}(f_k)$ in order to compute $P_{C_j}(F)$.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python>    <span class=k>def</span> <span class=nf>probability</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>classification</span><span class=p>,</span> <span class=n>feature</span><span class=p>):</span>
        <span class=s1>&#39;&#39;&#39;
</span><span class=s1>        Calculate the probability that a feature belongs to a
</span><span class=s1>        certain class, ie. P(classification | feature).
</span><span class=s1>        &#39;&#39;&#39;</span>
        <span class=c1># Count of classification occurence of a feature</span>
        <span class=k>if</span> <span class=n>feature</span> <span class=ow>not</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>:</span>
            <span class=k>return</span> <span class=mi>1</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=k>if</span> <span class=n>classification</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=n>feature</span><span class=p>]:</span>
                <span class=n>count</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=n>feature</span><span class=p>][</span><span class=n>classification</span><span class=p>]</span>
            <span class=k>else</span><span class=p>:</span>
                <span class=n>count</span> <span class=o>=</span> <span class=mi>1</span>
        <span class=c1># Total count of classification occurences</span>
        <span class=n>total</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=p>[</span><span class=n>classification</span><span class=p>]</span>
        <span class=c1># Compute ratio (will be lower then 1 and higher then 0)</span>
        <span class=k>return</span> <span class=n>count</span> <span class=o>/</span> <span class=n>total</span>
</code></pre></div><p>A problem that I haven&rsquo;t mentioned is how to deal with features that haven&rsquo;t been encountered/classified in previous texts when doing a prediction. Intuitively an unencountered feature should bias our the decision rule towards a specific class. Because $P_{C_j}(F)$ is the result of a product, multiplying it by $1$ won&rsquo;t affect it ($1$ is the <em>identity element</em> for the multiplication of real numbers).</p>
<p>The same kind of problem arises as to what probability to assign if a feature has been encountered but not for a certain class. In some sense this should be something <em>low</em>. But it shouldn&rsquo;t be 0, because then $P_{C_j}(F)$ would be equal to 0 and that is too harsh. However it&rsquo;s also reasonable to return $1$ after the <code>else</code>, which is equivalent to ignoring the feature (but not ignoring the fact that it was never associated with that class!).</p>
<p>Either the feature has already been associated with a class and we can calculate the <code>count / total</code> ratio, which is the number of times a feature has been associated with a class over the number of occurences of a class. In other words this is the frequency of the association between a feature and a class ($\frac{P(C_j \cap f_k)}{P(C_j)})$. In the other case the ratio is <code>1 / total</code>, which is very pessimistic.</p>
<p>Now we can code the <code>NaiveBayes</code> class, the one that will be used for prediction.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>NaiveBayes</span><span class=p>(</span><span class=n>Counter</span><span class=p>):</span>

    <span class=k>def</span> <span class=nf>predictSentence</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>sentence</span><span class=p>):</span>
        <span class=s1>&#39;&#39;&#39; Classify one sentence based on it&#39;s features. &#39;&#39;&#39;</span>
        <span class=n>likelihoods</span> <span class=o>=</span> <span class=p>{}</span>
        <span class=c1># For each possible classification</span>
        <span class=k>for</span> <span class=n>classification</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
            <span class=c1># Compute P(classification)</span>
            <span class=n>likelihoods</span><span class=p>[</span><span class=n>classification</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>classifications</span> <span class=o>+</span> \
            <span class=p>[</span><span class=n>classification</span><span class=p>]</span> <span class=o>/</span> <span class=o>+</span> <span class=nb>sum</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>classifications</span><span class=o>.</span><span class=n>values</span><span class=p>())</span>
            <span class=c1># Multiply iy by the product of each P(F_i)</span>
            <span class=k>for</span> <span class=n>feature</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>featureFunction</span><span class=p>(</span><span class=n>sentence</span><span class=p>):</span>
                <span class=n>likelihoods</span><span class=p>[</span><span class=n>classification</span><span class=p>]</span> <span class=o>*=</span> <span class=bp>self</span><span class=o>.</span><span class=n>probability</span> <span class=o>+</span> \
                <span class=p>(</span><span class=n>classification</span><span class=p>,</span> <span class=n>feature</span><span class=p>)</span>
        <span class=k>return</span> <span class=nb>max</span><span class=p>(</span><span class=n>likelihoods</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=n>likelihoods</span><span class=o>.</span><span class=n>get</span><span class=p>)</span>

    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>df</span><span class=p>,</span> <span class=n>textCol</span><span class=p>):</span>
        <span class=s1>&#39;&#39;&#39; Classify a test dataframe and return a series object. &#39;&#39;&#39;</span>
        <span class=n>classifications</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=n>textCol</span><span class=p>]</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>predictSentence</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>classifications</span>
</code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>re</span>

<span class=k>def</span> <span class=nf>words</span><span class=p>(</span><span class=n>sentence</span><span class=p>,</span> <span class=n>threshold</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>
    <span class=s1>&#39;&#39;&#39;
</span><span class=s1>    Obtain an iterable of words from a sentence. Lowercase the words.
</span><span class=s1>    &#39;&#39;&#39;</span>
    <span class=n>splitter</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\\</span><span class=s1>W+&#39;</span><span class=p>)</span>
    <span class=c1># Concatenate the lines into a big string</span>
    <span class=n>words</span> <span class=o>=</span> <span class=p>(</span><span class=n>word</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>splitter</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>sentence</span><span class=p>))</span>
    <span class=c1># Lower case</span>
    <span class=n>words</span> <span class=o>=</span> <span class=p>(</span><span class=n>word</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>words</span><span class=p>)</span>
    <span class=c1># Done!</span>
    <span class=k>return</span> <span class=n>words</span>
</code></pre></div><p>The first function classifies a sentence. The outer <code>for</code> loops over all the possible classes. Lines <code>9</code> and <code>10</code> compute $P_{C_j}$. The inner <code>for</code> loops iterates over all the features in order to compute $P_{C_j}(F)$. Finally the class that has the highest probability is returned. The second function applies the first function to a dataframe.</p>
<p>The only piece of the puzzle missing is a feature function. Basically, if we have a sentence, we want to be able to transform it into a vector of features (which can be words, prefixes, stems, &mldr;) in order to compute individual probabilities for each feature. A very simple approach is to split a sentence on it&rsquo;s white spaces and to put the words to lowercase.</p>
<p>Choosing a good feature function is crucial for the performance of a classifier, some people argue that it is even more important than the choice of the classifier.</p>
<p>The following piece of code puts all the pieces together.</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;www.maxhalford.com/data/datasets/authors.csv&#39;</span><span class=p>,</span>
                 <span class=n>delimiter</span><span class=o>=</span><span class=s1>&#39;</span><span class=se>\t</span><span class=s1>&#39;</span><span class=p>)</span>
<span class=n>bayes</span> <span class=o>=</span> <span class=n>NaiveBayes</span><span class=p>(</span><span class=n>dataframe</span><span class=o>=</span><span class=n>df</span><span class=p>,</span> <span class=n>features</span><span class=o>=</span><span class=n>words</span><span class=p>,</span>
                   <span class=n>textCol</span><span class=o>=</span><span class=s1>&#39;Sentence&#39;</span><span class=p>,</span> <span class=n>classCol</span><span class=o>=</span><span class=s1>&#39;Author&#39;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>bayes</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=s1>&#39;Sentence&#39;</span><span class=p>))</span>
</code></pre></div>
</div>
<script type=text/javascript>var s=document.createElement('script');s.setAttribute('src','https://utteranc.es/client.js'),s.setAttribute('repo','MaxHalford/maxhalford.github.io'),s.setAttribute('issue-term','pathname'),s.setAttribute('crossorigin','anonymous'),s.setAttribute('async',null),s.setAttribute('theme','github-light'),document.body.appendChild(s)</script>
<div class=footer>
<div class=do-the-thing>
<div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>
Back to the top
</div>
</div>
</div>
<script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.0/elevator.min.js></script>
<script>var elementButton=document.querySelector('.elevator'),elevator=new Elevator({element:elementButton,mainAudio:'/music/elevator.mp3',endAudio:'/music/ding.mp3'})</script>
<style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style>
<div class=site-footer>
<div class=site-footer-item>
<a href=/index.xml><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M12.8 16C12.8 8.978 7.022 3.2.0 3.2V0c8.777.0 16 7.223 16 16h-3.2zM2.194 11.61c1.21.0 2.195.985 2.195 2.196.0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017.0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818.0 10.606 4.79 10.606 10.607z"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://github.com/MaxHalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M8 0C3.58.0.0 3.582.0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385.0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953.0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117.0.0.67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147.0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48.0 1.07-.01 1.93-.01 2.19.0.21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://linkedin.com/in/maxhalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235.0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4.0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762.0-1.376-.617-1.376-1.377.0-.758.614-1.375 1.376-1.375.76.0 1.376.617 1.376 1.375.0.76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816.0H1.18C.528.0.0.516.0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652.0 1.185-.516 1.185-1.153V1.153C16 .516 15.467.0 14.815.0z" fill-rule="nonzero"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://twitter.com/halford_max><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812.0-3.282 1.47-3.282 3.28.0.26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21.0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26.0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04.0 9.34-5 9.34-9.33.0-.14.0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://kaggle.com/maxhalford><span class=inline-svg><svg role="img" viewBox="0 0 26 26" xmlns="http://www.w3.org/2000/svg"><title>Kaggle icon</title><path fill="currentcolor" d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187.0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236.0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234.0.351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144.0.236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.358"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=/files/resume_max_halford.pdf><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 392.533 392.533" style="enable-background:new 0 0 392.533 392.533"><g><g><path fill="currentcolor" d="M292.396 324.849H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 329.697 298.473 324.849 292.396 324.849z"/></g></g><g><g><path fill="currentcolor" d="M292.396 277.01H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 281.859 298.473 277.01 292.396 277.01z"/></g></g><g><g><path fill="currentcolor" d="M196.137 45.834c-25.859.0-46.998 21.075-46.998 46.998.0 25.859 21.139 46.933 46.998 46.933s46.998-21.075 46.998-46.998-21.139-46.933-46.998-46.933zm0 72.017c-13.77.0-25.083-11.313-25.083-25.083s11.248-25.083 25.083-25.083 25.083 11.313 25.083 25.083c0 13.769-11.313 25.083-25.083 25.083z"/></g></g><g><g><path fill="currentcolor" d="M258.521 163.362c-39.887-15.515-84.752-15.515-124.638.0-13.059 5.107-21.786 18.101-21.786 32.388v44.347c-.065 6.012 4.849 10.925 10.861 10.925h146.424c6.012.0 10.925-4.848 10.925-10.925V195.75C280.307 181.463 271.58 168.469 258.521 163.362zm0 65.874H133.883v-33.422c0-5.301 3.168-10.214 7.887-12.024 34.844-13.511 74.02-13.511 108.865.0 4.719 1.875 7.887 6.659 7.887 12.024v33.422z"/></g></g><g><g><path fill="currentcolor" d="M313.083.0H131.491c-8.404.0-16.291 3.232-22.238 9.18L57.018 61.414c-5.947 5.948-9.18 13.834-9.18 22.238v277.333c0 17.39 14.158 31.547 31.547 31.547h233.762c17.39.0 31.547-14.158 31.547-31.547V31.547C344.501 14.158 330.343.0 313.083.0zM112.032 37.236v27.022H85.01l27.022-27.022zm210.683 79.58h-40.598c-6.012.0-10.925 4.849-10.925 10.925.0 6.012 4.848 10.925 10.925 10.925h40.598v19.394h-14.869c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h14.869v181.139c0 5.366-4.331 9.697-9.632 9.697H79.192c-5.301.0-9.632-4.331-9.632-9.632V86.044h53.398c6.012.0 10.925-4.848 10.925-10.925V21.721h179.2c5.301.0 9.632 4.331 9.632 9.632v85.463z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href="https://scholar.google.com/citations?user=erRNNi0AAAAJ&hl=en"><span class=inline-svg><svg viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952.0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584.0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288.0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962.0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zm762.098 382.384c-64.358 64.424-141.86 96.57-232.572 96.57H329.144c-90.712.0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572V182.859c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712.0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159V762.033c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162.0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53.0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476.0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86.0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908.0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426.0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432.0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234.0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048.0 6.642.19 12.492.672 18.974H292.574l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382.0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994.0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376.0 103.05 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382.0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.05 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://www.imdb.com/user/ur73044771><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><g><g><path fill="currentcolor" d="M425.17 73.146c.179-1.577.268-3.169.268-4.771.0-23.25-18.915-42.165-42.165-42.165-6.849.0-13.444 1.619-19.354 4.678-9.531-14.757-26.054-24.056-44.185-24.056-11.414.0-22.244 3.62-31.163 10.214C280.754 6.589 268.271.0 254.743.0c-13.007.0-25.097 6.068-32.975 15.906-8.628-5.856-18.899-9.075-29.515-9.075-18.126.0-34.646 9.302-44.176 24.06-5.913-3.06-12.508-4.682-19.351-4.682-23.25.0-42.166 18.915-42.166 42.165.0 1.603.088 3.195.266 4.769-21.658 6.642-36.909 26.605-36.909 50.229.0 11.817 3.952 23.172 11.184 32.401l43.004 347.699c.603 4.871 4.74 8.528 9.648 8.528h284.485c4.907.0 9.046-3.658 9.648-8.528l43.004-347.7c7.238-9.225 11.194-20.578 11.194-32.4C462.083 99.751 446.832 79.789 425.17 73.146zM122.346 492.557 81.465 162.025h44.764l26.618 330.532H122.346zm50.007.0-26.618-330.532H201.2l3.161 104.598c-17.168 14.634-28.086 36.393-28.086 60.667.0 26.007 12.521 49.142 31.849 63.703l3.065 101.563H172.353zm108.993.0h-50.704l-2.736-90.671c8.743 3.303 18.205 5.125 28.09 5.125 9.887.0 19.352-1.823 28.096-5.128L281.346 492.557zm-25.35-104.989c-33.237.0-60.277-27.04-60.277-60.277s27.04-60.277 60.277-60.277 60.277 27.04 60.277 60.277-27.04 60.277-60.277 60.277zM220.653 162.025h70.696l-2.797 92.523c-9.95-4.469-20.962-6.977-32.555-6.977-11.591.0-22.602 2.507-32.548 6.975L220.653 162.025zm80.144 330.532 3.076-101.568c19.325-14.562 31.844-37.694 31.844-63.698.0-24.27-10.915-46.027-28.078-60.661l3.16-104.605h55.457l-26.618 330.532H300.797zm88.847.0h-30.501l26.618-330.532h44.766L389.644 492.557zm46.819-349.975H75.531c-3.986-5.588-6.171-12.266-6.171-19.21.0-15.23 10.052-28.041 24.192-31.923 5.136 7.347 14.332 15.089 27.853 15.089 1.412.0 2.87-.084 4.376-.262 5.086-.601 9.169-4.831 9.023-9.951-.169-5.868-5.336-10.151-11.005-9.396-10.908 1.458-15.263-7.828-16.038-9.747-.013-.032-.029-.062-.042-.095-.012-.027-.017-.057-.029-.084-.779-1.9-1.29-3.885-1.53-5.925-1.495-12.753 8.151-24.497 20.961-25.373 6.62-.452 12.93 1.908 17.606 6.533 1.819 1.799 4.249 2.929 6.806 2.967 4.322.064 8.035-2.652 9.379-6.564 4.598-13.379 17.192-22.369 31.338-22.369 9.432.0 18.433 4.035 24.706 11.075 1.729 1.94 4.172 3.177 6.769 3.314 4.373.231 8.223-2.412 9.675-6.342 3.285-8.897 11.862-14.876 21.341-14.876 9.948.0 18.848 6.611 21.743 16.113.844 2.769 2.688 5.187 5.322 6.388 4.237 1.929 9.045.642 11.766-2.852 6.344-8.145 15.88-12.817 26.163-12.817 12.52.0 23.811 7.04 29.437 17.921-1.79 2.84-3.768 6.681-5.037 11.285-3.664 13.302.342 26.651 11.28 37.59 1.898 1.899 4.386 2.848 6.874 2.848 2.445.0 4.889-.916 6.775-2.749 3.908-3.798 3.579-10.276-.227-14.178-5.623-5.764-7.618-11.593-6.097-17.81.165-.673.362-1.321.582-1.939 1.859-5.235 5.847-9.536 10.945-11.741 3.526-1.524 7.439-2.141 11.453-1.724 11.438 1.189 20.292 11.127 20.277 22.625-.004 2.949-.569 5.829-1.682 8.559-.872 2.139-.985 4.557-.26 6.751 1.247 3.773 4.581 6.325 8.393 6.66 16.932 1.484 30.195 15.978 30.195 33C442.642 130.32 440.454 136.998 436.463 142.582z"/></g></g><g><g><path fill="currentcolor" d="M261.257 62.341c-7.484.0-14.638 1.996-20.875 5.741-6.452-5.745-14.886-9.073-23.702-9.073-19.656.0-35.646 15.99-35.646 35.646s15.989 35.646 35.644 35.646c5.369.0 9.721-4.353 9.721-9.721.0-5.369-4.353-9.721-9.721-9.721-8.935.0-16.203-7.268-16.203-16.203s7.268-16.203 16.203-16.203c5.773.0 10.987 2.99 13.945 7.999 1.542 2.612 4.217 4.354 7.229 4.71 3.015.358 6.02-.714 8.128-2.893 4.047-4.182 9.473-6.484 15.277-6.484 11.725.0 21.264 9.539 21.264 21.266.0 5.369 4.351 9.722 9.721 9.722s9.721-4.353 9.721-9.722C301.965 80.603 283.704 62.341 261.257 62.341z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://play.spotify.com/user/1166811350><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 168 168"><path fill="currentcolor" d="m83.996.277C37.747.277.253 37.77.253 84.019c0 46.251 37.494 83.741 83.743 83.741 46.254.0 83.744-37.49 83.744-83.741.0-46.246-37.49-83.738-83.745-83.738l.001-.004zm38.404 120.78c-1.5 2.46-4.72 3.24-7.18 1.73-19.662-12.01-44.414-14.73-73.564-8.07-2.809.64-5.609-1.12-6.249-3.93-.643-2.81 1.11-5.61 3.926-6.25 31.9-7.291 59.263-4.15 81.337 9.34 2.46 1.51 3.24 4.72 1.73 7.18zm10.25-22.805c-1.89 3.075-5.91 4.045-8.98 2.155-22.51-13.839-56.823-17.846-83.448-9.764-3.453 1.043-7.1-.903-8.148-4.35-1.04-3.453.907-7.093 4.354-8.143 30.413-9.228 68.222-4.758 94.072 11.127 3.07 1.89 4.04 5.91 2.15 8.976v-.001zm.88-23.744c-26.99-16.031-71.52-17.505-97.289-9.684-4.138 1.255-8.514-1.081-9.768-5.219-1.254-4.14 1.08-8.513 5.221-9.771 29.581-8.98 78.756-7.245 109.83 11.202 3.73 2.209 4.95 7.016 2.74 10.733-2.2 3.722-7.02 4.949-10.73 2.739z"/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=https://www.goodreads.com/user/show/67553795-lemax><span class=inline-svg><svg id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 463 463" style="enable-background:new 0 0 463 463"><g><path fill="currentcolor" d="M270.615 229.128l-24-72c-1.021-3.063-3.887-5.128-7.115-5.128s-6.094 2.066-7.115 5.128l-24 72c-.513 1.54-.513 3.204.0 4.743l24 72c1.021 3.063 3.887 5.128 7.115 5.128s6.094-2.066 7.115-5.128l24-72C271.128 232.332 271.128 230.668 270.615 229.128zM239.5 279.783 223.406 231.5l16.094-48.283 16.094 48.283L239.5 279.783z"/><path fill="currentcolor" d="M375.5 48h-64c-2.997.0-5.862.57-8.5 1.597V23.5C303 10.542 292.458.0 279.5.0h-80C186.542.0 176 10.542 176 23.5v42.097C173.362 64.57 170.497 64 167.5 64h-80C74.542 64 64 74.542 64 87.5v352c0 12.958 10.542 23.5 23.5 23.5h80c6.177.0 11.801-2.399 16-6.31 4.199 3.911 9.823 6.31 16 6.31h80c6.177.0 11.801-2.399 16-6.31 4.199 3.911 9.823 6.31 16 6.31h64c12.958.0 23.5-10.542 23.5-23.5v-368C399 58.542 388.458 48 375.5 48zM79 135h97v257H79V135zM191 87.5V87h97v289h-97V87.5zm97-16V72h-97V55h97V71.5zM191 391h97v17h-97V391zM303 119h81v273h-81V119zm8.5-56h64c4.687.0 8.5 3.813 8.5 8.5V104h-81V71.5C303 66.813 306.813 63 311.5 63zm-112-48h80c4.687.0 8.5 3.813 8.5 8.5V40h-97V23.5C191 18.813 194.813 15 199.5 15zM87.5 79h80c4.687.0 8.5 3.813 8.5 8.5V120H79V87.5c0-4.687 3.813-8.5 8.5-8.5zm80 369h-80c-4.687.0-8.5-3.813-8.5-8.5V407h97v32.5C176 444.187 172.187 448 167.5 448zm112 0h-80c-4.687.0-8.5-3.813-8.5-8.5V423h97v16.5C288 444.187 284.187 448 279.5 448zm96 0h-64c-4.687.0-8.5-3.813-8.5-8.5V407h81v32.5C384 444.187 380.187 448 375.5 448z"/><path fill="currentcolor" d="M374.615 253.128l-24-72c-1.021-3.063-3.887-5.128-7.115-5.128s-6.094 2.066-7.115 5.128l-24 72c-.513 1.54-.513 3.204.0 4.743l24 72c1.021 3.063 3.887 5.128 7.115 5.128s6.094-2.066 7.115-5.128l24-72C375.128 256.332 375.128 254.668 374.615 253.128zM343.5 303.783 327.406 255.5l16.094-48.283 16.094 48.283L343.5 303.783z"/><path fill="currentcolor" d="M158.615 261.128l-24-72c-1.021-3.063-3.887-5.128-7.115-5.128s-6.094 2.066-7.115 5.128l-24 72c-.513 1.54-.513 3.204.0 4.743l24 72c1.021 3.063 3.887 5.128 7.115 5.128s6.094-2.066 7.115-5.128l24-72C159.128 264.332 159.128 262.668 158.615 261.128zM127.5 311.783 111.406 263.5l16.094-48.283 16.094 48.283L127.5 311.783z"/></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg>
</span>
</a>
</div>
<div class=site-footer-item>
<a href=mailto:maxhalford25@gmail.com><span class=inline-svg><svg viewBox="0 0 15 20" xmlns="http://www.w3.org/2000/svg"><title>mail</title><path fill="currentcolor" d="M0 4v8c0 .55.45 1 1 1h12c.55.0 1-.45 1-1V4c0-.55-.45-1-1-1H1c-.55.0-1 .45-1 1zm13 0L7 9 1 4h12zM1 5.5l4 3-4 3v-6zM2 12l3.5-3L7 10.5 8.5 9l3.5 3H2zm11-.5-4-3 4-3v6z" fill="#000" fill-rule="evenodd"/></svg>
</span>
</a>
</div>
</div>
<div style=margin-bottom:50px;display:flex;justify-content:center>
<iframe src=https://github.com/sponsors/MaxHalford/button title="Sponsor MaxHalford" height=35 width=116 style=border:0></iframe>
</div>
</div>
</div>
</article>
<script></script>
</body>
</html>