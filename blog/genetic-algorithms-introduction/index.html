<!doctype html><html lang=en><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-63302552-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-63302552-1')</script><script async defer data-website-id=6023252a-3a97-470f-b4ee-5082d242bb9a src=https://umami.pourtan.eu/umami.js></script><meta charset=utf-8><meta name=generator content="Hugo 0.83.1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Max Halford"><meta property="og:url" content="https://maxhalford.github.io/blog/genetic-algorithms-introduction/"><link rel=canonical href=https://maxhalford.github.io/blog/genetic-algorithms-introduction/><link rel=icon href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦”</text></svg>"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maxhalford.github.io\/"},"articleSection":"blog","name":"An introduction to genetic algorithms","headline":"An introduction to genetic algorithms","description":"The goal of genetic algorithms (GAs) is to solve problems whose solutions are not easily found (ie. NP problems, nonlinear optimization, etc.). For example, finding the shortest path from A to B in a directed graph is easily done with Djikstra\u0026rsquo;s algorithm, it can be solved in polynomial time. However the time to find the smallest path that joins all points on a non-directed graph, also known as the Travelling Salesman Problem (TSP) increases exponentially as the number of points increases.","inLanguage":"en-US","author":"Max Halford","creator":"Max Halford","publisher":"Max Halford","accountablePerson":"Max Halford","copyrightHolder":"Max Halford","copyrightYear":"2015","datePublished":"2015-08-02 00:00:00 \u002b0000 UTC","dateModified":"2015-08-02 00:00:00 \u002b0000 UTC","url":"https:\/\/maxhalford.github.io\/blog\/genetic-algorithms-introduction\/","keywords":[]}</script><title>An introduction to genetic algorithms - Max Halford</title><meta property="og:title" content="An introduction to genetic algorithms - Max Halford"><meta property="og:type" content="article"><meta name=description content="The goal of genetic algorithms (GAs) is to solve problems whose solutions are not easily found (ie. NP problems, nonlinear optimization, etc.). For example, finding the shortest path from A to B in a directed graph is easily done with Djikstra&rsquo;s algorithm, it can be solved in polynomial time. However the time to find the smallest path that joins all points on a non-directed graph, also known as the Travelling Salesman Problem (TSP) increases exponentially as the number of points increases."><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.min.css><link rel=stylesheet href=/css/highlight/github.css><link rel=stylesheet href=/css/index.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@400;700&family=Permanent+Marker&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0,tags:'ams'},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll('mjx-container').forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><article class=post id=article><div class="row center-xs" style=text-align:left><div class="col-xs-12 col-sm-10 col-md-7 col-lg-5"><div class=post-header><header><div class="signatures site-title"><a href=/>Max Halford</a></div></header><div class="row end-xs"><div><a class=header-link href=/>Blog</a>
<a class=header-link href=/links/>Links</a>
<a class=header-link href=/bio/>Bio</a></div></div><div class=header-line></div></div><header class=post-header><h1 class=post-title>An introduction to genetic algorithms</h1><div class="row post-desc"><div class=col-xs-12><time class=post-date datetime="2015-08-02 00:00:00 UTC">2015-08-02 Â· 16 minute read</time></div></div></header><div class="post-content markdown-body"><p>The goal of genetic algorithms (GAs) is to solve problems whose solutions are not easily found (ie. NP problems, nonlinear optimization, etc.). For example, finding the shortest path from A to B in a directed graph is easily done with <em>Djikstra&rsquo;s algorithm</em>, it can be solved in polynomial time. However the time to find the smallest path that joins all points on a non-directed graph, also known as the <a href=http://www.wikiwand.com/en/Travelling_salesman_problem>Travelling Salesman Problem</a> (TSP) increases exponentially as the number of points increases. More generally, GAs are useful for problems where an analytical approach is complicated or even impossible. By giving up on perfection they manage to find a good approximation of the optimal solution.</p><p>GAs are part of a higher class of algorithms: heuristic algorithms. Heuristic algorithms give up precision for power. They are not designed to find the optimal solution, instead they return a very good approximation of the solution. If the search space for the answer is quite smooth they even have a chance of finding the optimal solution. This is the point where some people give up reading because they want a simple algorithm that can solve anything. For those I recommend reading <a href=https://www.wikiwand.com/en/No_free_lunch_theorem>the following</a>.</p><p>Obviously GAs are not the only heuristic problem solvers out there, actually there are a lot of them. The two main benchmarks that rank them are <em>speed</em> and the <em>intelligence</em> of the program. Heuristic algorithms are greedy in resources, that&rsquo;s one of the reasons they were not very popular at a time when computers were not as powerful as they are today.</p><p>I have a lot of experience with GAs. I believe that they solve problems very well if they are properly tuned. With GAs I&rsquo;ve programmed a polynomial curve fitter, a TSP solver, a function optimization program and a racetrack car.</p><p>The first part of this notebook might seem a bit theoretical but I have to give some background to the examples given further on. For every application there is a different GA, the core stays the same but the randomness changes. It&rsquo;s important to understand the problem that to be solved and how the GA operates in order to make it reach it&rsquo;s full potential.</p><h2 id=approach>Approach</h2><h3 id=fitness-function>Fitness function</h3><p>To find the best solution we have to be able to measure the <em>performance</em> of a candidate solution. We want to be able to rank candidate solutions.</p><ul><li>In the case of the TSP this is the sum of distances between each consecutive node.</li><li>In the case of a function minimization problem, the measure is simply the function itself.</li><li>In the case of fitting a polynomial to a curve we can measure the least squares error.</li></ul><p>It is interesting to note that without knowing the global solution to a problem we know what properties it has. It might seem obvious that the best polynomial that fits a curve is the one that has the lowest least squares error, but for more complicated problems the fitness functions are not as obvious. In any case, pay attention when choosing the fitness function, it&rsquo;s crucial. On a side note, fitness functions are also called objective functions in the literature.</p><h3 id=dna>DNA</h3><p>By DNA I mean the parameters that are inputed into the objective function we want to optimize, these are the candidate solutions.</p><ul><li>In the case of the TSP the DNA is a list of consecutives nodes.</li><li>In the case of a function minimization problem, the DNA is a vector.</li><li>In the case of fitting a polynomial to a curve the DNA is a list of coefficients.</li></ul><p>The objective is to find the DNA that optimizes the chosen objective function.</p><h3 id=crossover>Crossover</h3><p>Say we have two DNAs that are <em>good</em> according to a fitness function. The idea of the crossover is to produce a new DNA by mixing up both DNAs. In the case of two vectors you could split both in half and create a new individual by sticking the first half of the first vector with the second part of the second vector. However crossovers are not always possible. For example you cannot take the first half of nodes in a TSP path and glue it with the second half of another path because there could be repetitions of nodes. It is mostly down to common sense and creativity, thus understanding the problem to be solved greatly helps. Crossovers can be very good at generating better DNAs, but if they are not based on some mathematical or logical background they could possibly make things worse.</p><h3 id=mutation>Mutation</h3><p>Some resources will briefly mention mutation, instead they will focus on crossovers. In my experience, mutations have had more success that crossovers. Mutations modify part of the DNA, they don&rsquo;t require mixing DNA up. For example for minimizing a multivariate function, we could modify a little bit one parameter and see if the fitness function returns a better value, if so then we can keep the mutation. By a little bit I mean, well, that&rsquo;s the problem, anything. If we suppose that we don&rsquo;t anything about what the solution looks like, then we don&rsquo;t have a preference between increasing or reducing the parameter, neither do we have any preference for the amplitude of the mutation. Therefore the mutation is <em>random</em>. The examples further on will make this clearer.</p><h3 id=population>Population</h3><p>A population is a group of potential solutions to a problem. At the beginning the population is composed of random DNAs (called individuals). Every step in time is called a generation. At each generation we have to pick a group of individuals that will reproduce to produce new (hopefully better) solutions. The new solutions are called <em>offsprings</em>. After having judiciously chosen $k$ individuals, every individual can generate $m$ offsprings. Thus every generation will be of size $k \times m$.</p><h3 id=selection>Selection</h3><p>If we have measured every individual in a population according to a fitness function we need a method for keeping a good subset that will produce new individuals. The most basic method is called <em>elitism selection</em>, very naively it simply keeps the $k$ best individuals. While this is a good short term selection method, nothing is certain on the long term. If at every generation the same lineage of individuals is selected then the population will not be very heterogenous. In other words the search space will not be fully explored and the candidate solutions will converge to a local optimum. The idea is to have a selection method that preserves variety in the population in order to have the most chances of finding the global optimum.</p><p>The literature often mentions <em>roulette selection</em>. Basically, you assign a relative fitness to each DNA (for example the DNA&rsquo;s fitness divided by the sum of all fitness), generate a random number between 0 and 1 and see where it lies. Of course the DNA with the highest fitness will get selected with a higher probability, however by experience I feel that this method still converges to local optima too fast. But don&rsquo;t take my word for granted! It&rsquo;s important to test each selection method for every problem. On a more or less convex space converging quickly can be a good thing because there are not many local optima to converge to.</p><p>My favorite selection method is called <em>tournament selection</em>. The idea is the following, select at random a sample of DNA of size $n$ and only keep the best individual. It&rsquo;s very simple, and yet it allows adjusting the convergence of the algorithm very easily. Indeed, if $n$ is small, then the best DNAs don&rsquo;t have a lot of chance of being selected and so it doesn&rsquo;t <em>kill</em> the other solutions quickly, thus preserving variety in the population. On the contrary, if $n$ is large then the best DNA quickly dominate and the algorithm converges quickly.</p><h3 id=convergence>Convergence</h3><p>We have to tell the algorithm when to stop. There are two ways of doing this:</p><ul><li>Generate offsprings a given number of times.</li><li>Stop when the population doesn&rsquo;t get better for a given period of time.</li></ul><h3 id=method>Method</h3><p>The algorithm runs as follows:</p><ol><li>Generate a random initial population.</li><li>Measure every individual of the population.</li><li>Select $k$ individuals with an appropriate method.</li><li>Cross/mutate them and create new individuals.</li><li>Save the best individuals from the old population and the new population.</li><li>Start over from step 2 as many times as you wish or when the population converges.</li></ol><p>I don&rsquo;t think I&rsquo;m being obnoxious in saying that the previous pseudo-algorithm is fairly simple. I won&rsquo;t comment on it, I think the best is to jump into some code and learn it the hard way. If you want to run the following code yourself you will have to <a href=https://github.com/MaxHalford/Genetic-Curve-Fitting>download it from GitHub</a>.</p><h2 id=example>Example</h2><h3 id=fitting-a-polynomial-curve-to-a-list-of-points>Fitting a polynomial curve to a list of points</h3><p>As an example I&rsquo;m going to be building a <em>polynomial curve fitter</em>. First of all the <a href=http://mathworld.wolfram.com/WeierstrassApproximationTheorem.html>Weierstrass Approximation Theorem</a> tells us that any smooth function can be pretty well approximated by a polynomial function. The idea is to find a curve that fits a list of points without knowing what function produced them.</p><p>A one variable polynomial of degree $n$ polynomial looks like</p><p>$$P(x) = a_0 + a_1x + a_2x^2 + a_3x^3 + \dots + a_nx^n$$</p><p>For the sake of simplicity we won&rsquo;t consider multivariate polynomials, however they are implemented. Also we won&rsquo;t consider non-linear multivariate polynomials, <em>ie.</em> where $x$ can multiply $y$ but this would be a good thing to do yourself.</p><p>The question is the following: which tuple of size $n+1$ $(a_0, a_1, a_2, \dots, a_n)$ gives the polynomial that best approximates a function $f(X)$, where $X$ is a vector of any size.</p><p>Let&rsquo;s define <em>the polynomial that bests approximates a function</em>. A good measure for this is the least squares. Indeed if we have a list of $k$ points $(x_i, y_i)$, we simply have to measure a candidate polynomial $P$ in every $x_i$ and measure the squared difference between $P(x_i)$ and $y_i$. One could also use the absolute value function instead of the square function.</p><h3 id=initialization>Initialization</h3><p>First of all let&rsquo;s generate a dataframe that contains a list of points. Do as you please! For this example I computed $x^2$ for $x$ between (-2, 2) with a step of size $0.2$. In this case we know which function generated the sample of points to fit so we can measure the performance of the GA. In this case if should return $x^2$.</p><p>Let&rsquo;s open it with the <em>pandas</em> module and convert it to a lookup table, <em>ie.</em> a dictionary where we can easily check for point values.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>pandas</span> <span class=kn>as</span> <span class=nn>pd</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=c1># Create a dataframe</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mf>2.2</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>),</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>])</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;square&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>]</span> <span class=o>**</span> <span class=mi>2</span>
<span class=c1># Create the lookup table</span>
<span class=n>lookupTable</span> <span class=o>=</span> <span class=p>{}</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>record</span> <span class=ow>in</span> <span class=n>df</span><span class=o>.</span><span class=n>iterrows</span><span class=p>():</span>
    <span class=n>key</span> <span class=o>=</span> <span class=n>record</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>]</span>
    <span class=n>lookupTable</span><span class=p>[</span><span class=n>key</span><span class=p>]</span> <span class=o>=</span> <span class=n>record</span><span class=p>[</span><span class=s1>&#39;square&#39;</span><span class=p>]</span>
</code></pre></div><h3 id=coding-the-genetic-algorithm>Coding the genetic algorithm</h3><p>Now let&rsquo;s create our genetic algorithm. I will organize everything in classes, so as to keep the syntax clean tidy. First of all let&rsquo;s import the needed modules.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>numpy.random</span> <span class=kn>as</span> <span class=nn>rand</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=kn>from</span> <span class=nn>copy</span> <span class=kn>import</span> <span class=n>deepcopy</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=kn>as</span> <span class=nn>plt</span>
</code></pre></div><p>And then let&rsquo;s create an <em>Individual</em> class, which is basically the DNA I was talking about earlier. It will be the object of what is composed a <em>Population</em>. The individual has two parameters:</p><ul><li>The list of coefficients that define a polynomial.</li><li>It&rsquo;s fitness.</li></ul><p>We can start by generating $c$ random coefficients where $c$ is the degree of the polynomial and is specified by the user. Indeed in real applications there is no reasonable way to know to which degree a polynomial should in order to best fit a given sample of points. Also, if we know that the points are sampled from a very high degree polynomial we may wish to fit the points with a lower degree, simpler, polynomial. Of course the degree could be taken into as another variable in the optimization process! However this is a tad more complicated to put in place and further users are welcome to put this in place for themselves.</p><p>The number of variables, <code>d</code>, is 1 in our case.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>Individual</span><span class=p>:</span>
    <span class=c1># c is the number of coefficients</span>
    <span class=c1># d is the number of variables</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>d</span><span class=p>):</span>
        <span class=c1># Generate normal distributed coefficients for each variable</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>values</span> <span class=o>=</span> <span class=p>[[</span><span class=n>rand</span><span class=o>.</span><span class=n>normal</span><span class=p>()</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>c</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)]</span>
                       <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>d</span><span class=p>)]</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fitness</span> <span class=o>=</span> <span class=bp>None</span>
</code></pre></div><p>Next we have to able to evaluate the performance (or <em>fitness</em>) of an individual, in other words how good it fits a list of points. For this we compute the squared error described previously.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python>    <span class=k>def</span> <span class=nf>evaluate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>lookupTable</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>fitness</span> <span class=o>=</span> <span class=mi>0</span>
        <span class=c1># For each input</span>
        <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>lookupTable</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
            <span class=n>image</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=c1># For each variable</span>
            <span class=k>for</span> <span class=n>variable</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>values</span><span class=p>:</span>
                <span class=c1># For each coefficient</span>
                <span class=k>for</span> <span class=n>power</span><span class=p>,</span> <span class=n>coefficient</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>variable</span><span class=p>):</span>
                    <span class=c1># Compute polynomial image</span>
                    <span class=n>image</span> <span class=o>+=</span> <span class=n>coefficient</span> <span class=o>*</span> <span class=n>x</span> <span class=o>**</span> <span class=n>power</span>
            <span class=c1># Compute squared error</span>
            <span class=n>target</span> <span class=o>=</span> <span class=n>lookupTable</span><span class=p>[</span><span class=n>x</span><span class=p>]</span>
            <span class=n>mse</span> <span class=o>=</span> <span class=p>(</span><span class=n>target</span> <span class=o>-</span> <span class=n>image</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>fitness</span> <span class=o>+=</span> <span class=n>mse</span>
</code></pre></div><p>In this case I didn&rsquo;t work out any good crossover, instead I only implemented a simple mutation. Very naively, each coefficient can take a random value in its neighborhood. This is very crude and should be sophisticated for more precision. However as we will see it does the job for simple cases.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python>    <span class=k>def</span> <span class=nf>mutate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rate</span><span class=p>):</span>
        <span class=c1># Coefficients take a random value in their neighborhood</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>values</span> <span class=o>=</span> <span class=p>[[</span><span class=n>rand</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=n>c</span> <span class=o>-</span> <span class=n>rate</span><span class=p>,</span> <span class=n>c</span> <span class=o>+</span> <span class=n>rate</span><span class=p>)</span>
                       <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>variable</span><span class=p>]</span>
                       <span class=k>for</span> <span class=n>variable</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>values</span><span class=p>]</span>
</code></pre></div><p>We also will need to extract the information about the individual in a clean manner. I won&rsquo;t delve into the next function, it&rsquo;s simply string manipulation.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python>    <span class=k>def</span> <span class=nf>display</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
            <span class=n>intercept</span> <span class=o>=</span> <span class=mi>0</span>
            <span class=k>print</span> <span class=p>(</span><span class=s1>&#39;Polynomial form&#39;</span><span class=p>)</span>
            <span class=k>print</span> <span class=p>(</span><span class=s1>&#39;---------------&#39;</span><span class=p>)</span>
            <span class=k>for</span> <span class=n>index</span><span class=p>,</span> <span class=n>variable</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>values</span><span class=p>):</span>
                <span class=n>intercept</span> <span class=o>+=</span> <span class=n>variable</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
                <span class=k>for</span> <span class=n>power</span><span class=p>,</span> <span class=n>coefficient</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>variable</span><span class=p>[</span><span class=mi>1</span><span class=p>:]):</span>
                    <span class=k>print</span><span class=p>(</span><span class=nb>str</span><span class=p>(</span><span class=n>coefficient</span><span class=p>)</span> <span class=o>+</span> <span class=s1>&#39; * &#39;</span> <span class=o>+</span> <span class=s1>&#39;x&#39;</span> <span class=o>+</span> \
                          <span class=nb>str</span><span class=p>(</span><span class=n>index</span><span class=p>)</span> <span class=o>+</span> <span class=s1>&#39;**&#39;</span> <span class=o>+</span> <span class=nb>str</span><span class=p>(</span><span class=n>power</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=s1>&#39; + &#39;</span><span class=p>)</span>
            <span class=k>print</span> <span class=p>(</span><span class=n>intercept</span><span class=p>)</span>
</code></pre></div><p>Now that we have defined an individual (you can call it DNA or polynomial if you wish, I just find that <em>individual</em> is more general), we have to create a list of individuals from which we will be able to compare and select individuals. Basically we have to create a list of individuals.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>Population</span><span class=p>:</span>

    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>d</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
        <span class=c1># Create individuals</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>individuals</span> <span class=o>=</span> <span class=p>[</span><span class=n>Individual</span><span class=p>(</span><span class=n>c</span><span class=p>,</span> <span class=n>d</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>size</span><span class=p>)]</span>
        <span class=c1># Store the best individuals</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>best</span> <span class=o>=</span> <span class=p>[</span><span class=n>Individual</span><span class=p>(</span><span class=n>c</span><span class=p>,</span> <span class=n>d</span><span class=p>)]</span>
        <span class=c1># Mutation rate</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>rate</span> <span class=o>=</span> <span class=mf>0.1</span>
</code></pre></div><p>The first parameter is a list comprehension that generates random individuals from the class created above. We will store the best individual so as not to lose it through unlucky mutations. We also define a mutation rate for the <code>mutate(rate)</code> procedure of the <code>Individual</code> class. The greater this rate the higher the amplitude of the changes of each coefficient of the polynomials. This avoids getting <em>stuck</em> and enables the population to generate more varied offsprings.</p><p>Next up we have to be able to sort the population according to each individual&rsquo;s fitness. This can be done with a one-liner in Python.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python>    <span class=k>def</span> <span class=nf>sort</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>individuals</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>individuals</span><span class=p>,</span>
                                  <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>indi</span><span class=p>:</span> <span class=n>indi</span><span class=o>.</span><span class=n>fitness</span><span class=p>)</span>
</code></pre></div><p>However in order to sort the population we have to able to evaluate the population. We have already created a procedure to evaluate one individual so it is easy to generalize.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python>    <span class=k>def</span> <span class=nf>evaluate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>lookupTable</span><span class=p>):</span>
        <span class=k>for</span> <span class=n>indi</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>individuals</span><span class=p>:</span>
            <span class=n>indi</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>lookupTable</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>sort</span><span class=p>()</span>
</code></pre></div><p>Now for the (slightly) trickier part. We now have all the tools to evaluate, sort and modify a population. Let&rsquo;s put all the pieces together.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>enhance</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>lookupTable</span><span class=p>):</span>
        <span class=n>newIndividuals</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=c1># Go through top 10 individuals</span>
        <span class=k>for</span> <span class=n>individual</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>individuals</span><span class=p>[:</span><span class=mi>10</span><span class=p>]:</span>
            <span class=c1># Create 1 exact copy of each top 10 individuals</span>
            <span class=n>newIndividuals</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>individual</span><span class=p>))</span>
            <span class=c1># Create 4 mutated individuals</span>
            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>4</span><span class=p>):</span>
                <span class=n>newIndividual</span> <span class=o>=</span> <span class=n>deepcopy</span><span class=p>(</span><span class=n>individual</span><span class=p>)</span>
                <span class=n>newIndividual</span><span class=o>.</span><span class=n>mutate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>rate</span><span class=p>)</span>
                <span class=n>newIndividuals</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>newIndividual</span><span class=p>)</span>
        <span class=c1># Replace the old population with the new population</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>individuals</span> <span class=o>=</span> <span class=n>newIndividuals</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>lookupTable</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>sort</span><span class=p>()</span>
        <span class=c1># Store the new best individual</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>best</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>individuals</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
        <span class=c1># Increment the mutation rate if the population didn&#39;t change</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>best</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>fitness</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>best</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>fitness</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>rate</span> <span class=o>+=</span> <span class=mf>0.01</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>rate</span> <span class=o>=</span> <span class=mf>0.1</span>
</code></pre></div><p>I think that the process speaks for itself but I will go through it in plain english. We create a list of new individuals. We go through each one of the top 10 individuals (this is elitism). We keep a copy of the individual and add it to the new population. Then we create 4 mutated versions of the individual (called <em>offsprings</em>) and add them to new population. Once we have generated our 50 new individuals ($10 \times (1 + 4)$) we replace the old population. Now we can evaluate the new population, sort it, add the new best individual to a list and keep on going.</p><p>A few remarks:</p><ul><li><p>The use of <code>deepcopy()</code> is out of the scope of this tutorial. Basically when we mutate a new individual we don&rsquo;t want to mutate the old individual. If we coded <code>newIndividual = individual</code> instead of <code>newIndividual = deepcopy(individual)</code> then both would be changed, voilÃ .</p></li><li><p>The last part of the enhancement is not essential but it starts addressing a larger problem: getting stuck in a local optima. The idea behind the four last lines is to check increase the mutation rate if the the new population doesn&rsquo;t contain a new best individual.</p></li><li><p>There is a of lot of room to move in this script. The number of offsprings, the number of individuals that are saved, the number of iterations, the mutation rate&mldr; These parameters should not affect the result too much, however if you are not satisfied with the end result then these are the culprits. In general it is always tinkering about with the parameters of the GA in order to get a satisfying result. This is the common of all heuristic algorithms: you need to hold their hand.</p></li></ul><h3 id=applying-it-to-our-simple-example>Applying it to our simple example</h3><p>Now, for the fun part, let&rsquo;s put it all into practice. As a refresher we are trying to fit a polynomial to the mono-variable square function. Let&rsquo;s define some parameters.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>generations</span> <span class=o>=</span> <span class=mi>300</span>
<span class=n>degrees</span> <span class=o>=</span> <span class=mi>2</span>
<span class=n>variables</span> <span class=o>=</span> <span class=mi>1</span>
</code></pre></div><p>The <code>generations</code> parameter simply defines the number of times the GA will try and enhance the population. The more the better. Usually, after a certain number of iterations, the GA will get stuck on a local optima and no better offsprings will be produced, this is called a <code>convergence</code>.</p><p>Let&rsquo;s create an initial population and evaluate it a first time.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>polynomials</span> <span class=o>=</span> <span class=n>Population</span><span class=p>(</span><span class=n>degrees</span><span class=p>,</span> <span class=n>variables</span><span class=p>)</span>
<span class=n>polynomials</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>lookupTable</span><span class=p>)</span>
<span class=n>polynomials</span><span class=o>.</span><span class=n>sort</span><span class=p>()</span>
</code></pre></div><p>Let&rsquo;s enhance it 100 times.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Iterate through generations</span>
<span class=k>for</span> <span class=n>g</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>generations</span><span class=p>):</span>
    <span class=c1># Enhance the population</span>
    <span class=n>polynomials</span><span class=o>.</span><span class=n>enhance</span><span class=p>(</span><span class=n>lookupTable</span><span class=p>)</span>
</code></pre></div><p>And display the best polynomial.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>polynomials</span><span class=o>.</span><span class=n>best</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>display</span><span class=p>()</span>
</code></pre></div><pre><code>Polynomial form
---------------
0.010587779398717412 * x0**1 +
0.9804601148158121 * x0**2 +
0.04182274968452471
</code></pre><p>We wanted to find $x^2$ so this is pretty close! However the example was fairly simple, let&rsquo;s try it with something a tad more complicated.</p><h3 id=applying-it-to-more-complicated-data>Applying it to more complicated data</h3><p>Let&rsquo;s try to these a polynomial of degree 4 with more complex data.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>pandas</span> <span class=kn>as</span> <span class=nn>pd</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=c1># Create a dataframe</span>
<span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mf>2.2</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>),</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>])</span>
<span class=n>df</span><span class=p>[</span><span class=s1>&#39;square&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>]</span> <span class=o>**</span> <span class=mi>3</span> <span class=o>+</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>]</span> <span class=o>**</span> <span class=mi>2</span>
<span class=c1># Create the lookup table</span>
<span class=n>lookupTable</span> <span class=o>=</span> <span class=p>{}</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>record</span> <span class=ow>in</span> <span class=n>df</span><span class=o>.</span><span class=n>iterrows</span><span class=p>():</span>
    <span class=n>key</span> <span class=o>=</span> <span class=n>record</span><span class=p>[</span><span class=s1>&#39;x&#39;</span><span class=p>]</span>
    <span class=n>lookupTable</span><span class=p>[</span><span class=n>key</span><span class=p>]</span> <span class=o>=</span> <span class=n>record</span><span class=p>[</span><span class=s1>&#39;square&#39;</span><span class=p>]</span>

<span class=c1># Parameters</span>
<span class=n>generations</span> <span class=o>=</span> <span class=mi>300</span>
<span class=n>degrees</span> <span class=o>=</span> <span class=mi>4</span>
<span class=n>variables</span> <span class=o>=</span> <span class=mi>1</span>

<span class=c1># Initialize a population</span>
<span class=n>polynomials</span> <span class=o>=</span> <span class=n>Population</span><span class=p>(</span><span class=n>degrees</span><span class=p>,</span> <span class=n>variables</span><span class=p>)</span>
<span class=n>polynomials</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>lookupTable</span><span class=p>)</span>
<span class=n>polynomials</span><span class=o>.</span><span class=n>sort</span><span class=p>()</span>

<span class=c1># Iterate through generations</span>
<span class=k>for</span> <span class=n>g</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>generations</span><span class=p>):</span>
    <span class=c1># Enhance the population</span>
    <span class=n>polynomials</span><span class=o>.</span><span class=n>enhance</span><span class=p>(</span><span class=n>lookupTable</span><span class=p>)</span>

<span class=n>polynomials</span><span class=o>.</span><span class=n>best</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>display</span><span class=p>()</span>
</code></pre></div><p>As a measure of how well the algorithm did check out the following graph.</p><p><img src=http://i.imgur.com/wAI6j3U.png alt="Genetic Curve fitting to more complicated data"></p><p>Not too shabby!</p><h2 id=remarks>Remarks</h2><p>There is still a lot to be said for this specific use case and many improvements can be made. However I think that the algorithm is not too complicated to put in place and doesn&rsquo;t require complicated mathematics. The example tries to mimic a more general problem: <em>interpolation</em>, a more mathematical approach to curve fitting that gets very messy once the number of points increases. The nice feature of this script is that the user can specify the degree of the polynomial he wants to fit.</p><p>The two main ways for improving the algorithm better are</p><ul><li>coding a better mutation.</li><li>implementing different selection methods.</li></ul><p>It&rsquo;s important to understand the philosophy of these algorithms, the idea is that the solutions they spit out will never be the same because they are heuristic in nature.</p><p>All the code is available <a href=https://github.com/MaxHalford/Genetic-Curve-Fitting>on my GitHub</a>. I added a few more things that were not essential for this tutorial (for example the code for producing plots). You&rsquo;ll need Python 3 to use it. Also everything is nicely packaged so that you can easily use it for other projects.</p><p>Thanks for reading this, I hope you enjoyed it.</p></div><script type=text/javascript>var s=document.createElement('script');s.setAttribute('src','https://utteranc.es/client.js'),s.setAttribute('repo','MaxHalford/maxhalford.github.io'),s.setAttribute('issue-term','pathname'),s.setAttribute('crossorigin','anonymous'),s.setAttribute('async',null),s.setAttribute('theme','github-light'),document.body.appendChild(s)</script><div class=footer><div class=do-the-thing><div class=elevator><svg class="sweet-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" enable-background="new 0 0 100 100" height="100" width="100"><path d="M70 47.5H30c-1.4.0-2.5 1.1-2.5 2.5v40c0 1.4 1.1 2.5 2.5 2.5h40c1.4.0 2.5-1.1 2.5-2.5V50C72.5 48.6 71.4 47.5 70 47.5zm-22.5 40h-5v-25h5v25zm10 0h-5v-25h5v25zm10 0h-5V60c0-1.4-1.1-2.5-2.5-2.5H40c-1.4.0-2.5 1.1-2.5 2.5v27.5h-5v-35h35v35z"/><path d="M50 42.5c1.4.0 2.5-1.1 2.5-2.5V16l5.7 5.7c.5.5 1.1.7 1.8.7s1.3-.2 1.8-.7c1-1 1-2.6.0-3.5l-10-10c-1-1-2.6-1-3.5.0l-10 10c-1 1-1 2.6.0 3.5 1 1 2.6 1 3.5.0l5.7-5.7v24c0 1.4 1.1 2.5 2.5 2.5z"/></svg>Back to the top</div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/elevator.js/1.0.0/elevator.min.js></script><script>var elementButton=document.querySelector('.elevator'),elevator=new Elevator({element:elementButton,mainAudio:'/music/elevator.mp3',endAudio:'/music/ding.mp3'})</script><style>.down-arrow{font-size:120px;margin-top:90px;margin-bottom:90px;text-shadow:0 -20px #0c1f31,0 0 #c33329;color:transparent;-webkit-transform:scaleY(.8);-moz-transform:scaleY(.8);transform:scaleY(.8)}.elevator{text-align:center;cursor:pointer;width:140px;margin:auto}.elevator:hover{opacity:.7}.elevator svg{width:40px;height:40px;display:block;margin:auto;margin-bottom:5px}</style><div class=site-footer><div class=site-footer-item><a href=https://github.com/MaxHalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M8 0C3.58.0.0 3.582.0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385.0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953.0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117.0.0.67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147.0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48.0 1.07-.01 1.93-.01 2.19.0.21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg></span></a></div><div class=site-footer-item><a href=https://linkedin.com/in/maxhalford><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235.0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4.0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762.0-1.376-.617-1.376-1.377.0-.758.614-1.375 1.376-1.375.76.0 1.376.617 1.376 1.375.0.76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816.0H1.18C.528.0.0.516.0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652.0 1.185-.516 1.185-1.153V1.153C16 .516 15.467.0 14.815.0z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://twitter.com/halford_max><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812.0-3.282 1.47-3.282 3.28.0.26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21.0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26.0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04.0 9.34-5 9.34-9.33.0-.14.0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg></span></a></div><div class=site-footer-item><a href=https://kaggle.com/maxhalford><span class=inline-svg><svg role="img" viewBox="0 0 26 26" xmlns="http://www.w3.org/2000/svg"><title>Kaggle icon</title><path fill="currentcolor" d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187.0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236.0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234.0.351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144.0.236.06.285.18.046.149.034.255-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.07.358"/></svg></span></a></div><div class=site-footer-item><a href="https://scholar.google.com/citations?user=erRNNi0AAAAJ&hl=en"><span class=inline-svg><svg viewBox="0 0 1755 1755" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" transform="translate(0 1610) scale(1 -1)" d="M896.76 1130.189c-27.618 30.838-59.618 46.19-95.802 46.19-40.952.0-72.382-14.738-94.288-44.15-21.906-29.322-32.864-64.848-32.864-106.584.0-35.548 5.998-71.738 18-108.64 11.958-36.886 31.524-69.814 58.954-98.838 27.334-29.096 59.144-43.616 95.284-43.616 40.288.0 71.76 13.502 94.332 40.492 22.476 26.954 33.756 60.98 33.756 101.962.0 34.904-5.954 71.454-17.906 109.664-11.894 38.262-31.752 72.784-59.466 103.52zm762.098 382.384c-64.358 64.424-141.86 96.57-232.572 96.57H329.144c-90.712.0-168.14-32.146-232.572-96.57-64.424-64.286-96.57-141.86-96.57-232.572V182.859c0-90.712 32.146-168.288 96.57-232.712 64.432-64.146 142-96.432 232.572-96.432h1097.142c90.712.0 168.214 32.286 232.572 96.57 64.432 64.432 96.644 141.86 96.644 232.572v1097.142c0 90.712-32.22 168.288-96.644 232.572zM1297.81 1154.159V762.033c0-18.154-14.856-33.016-33.016-33.016h-12.156c-18.162.0-33.016 14.856-33.016 33.016v392.126c0 16.12-2.34 29.578 20.188 32.41v52.172l-173.43-142.24c2.004-3.716 3.906-6.092 5.712-9.208 15.242-26.976 23.004-60.526 23.004-101.53.0-31.43-5.238-59.662-15.858-84.598-10.57-24.928-23.428-45.29-38.43-60.972-15.002-15.74-30.048-30.128-45.092-43.074-15.046-12.976-27.904-26.506-38.436-40.55-10.614-14-15.894-28.474-15.894-43.476.0-15.024 6.854-30.288 20.524-45.67 13.62-15.426 30.376-30.376 50.19-45.144 19.85-14.666 39.658-30.946 59.472-48.662 19.858-17.694 36.52-40.456 50.14-68.096 13.722-27.744 20.568-58.288 20.568-91.86.0-44.288-11.294-84.282-33.806-119.882-22.58-35.446-51.998-63.73-88.144-84.472-36.242-20.882-75-36.6-116.334-47.214-41.42-10.518-82.52-15.806-123.568-15.806-25.908.0-52.048 1.996-78.336 6.1-26.382 4.096-52.81 11.33-79.426 21.526-26.668 10.262-50.286 22.864-70.758 37.998-20.524 14.98-37.046 34.312-49.716 57.856-12.668 23.552-18.958 50.022-18.958 79.426.0 34.882 9.714 67.24 29.192 97.404 19.478 29.944 45.282 54.952 77.378 74.76 55.998 34.838 143.858 56.364 263.432 64.498-27.334 34.172-41.048 66.334-41.048 96.432.0 17.122 4.476 35.474 13.334 55.288-14.284-1.996-28.994-3.124-44.002-3.124-64.234.0-118.476 20.882-162.524 62.932-44.046 41.976-66.048 94.522-66.048 158.048.0 6.642.19 12.492.672 18.974H292.574l393.618 342.17h651.856l-60.24-47.024v-82.996c22.368-2.874 20.004-16.318 20.004-32.394zM900.382 544.929c-7.52 1.36-18.088 2.122-31.708 2.122-29.382.0-58.288-2.596-86.666-7.782-28.38-5.046-56.378-13.568-83.998-25.592-27.722-11.952-50.096-29.528-67.146-52.766-17.144-23.208-25.666-50.542-25.666-81.994.0-29.974 7.52-56.714 22.572-80.004 15.002-23.142 34.808-41.26 59.428-54.236 24.62-12.998 50.432-22.814 77.378-29.264 26.998-6.408 54.476-9.736 82.476-9.736 55.376.0 103.05 12.47 143.046 37.406 39.906 24.928 59.904 63.422 59.904 115.382.0 10.928-1.522 21.686-4.528 32.19-3.138 10.62-6.24 19.712-9.282 27.26-3.05 7.41-8.858 16.332-17.43 26.616-8.522 10.314-15.046 17.934-19.434 23.004-4.476 5.238-12.852 12.712-25.19 22.594-12.236 9.926-20.048 16.114-23.522 18.402-3.43 2.406-12.332 8.908-26.668 19.456-14.328 10.634-22.184 16.274-23.566 16.94z"/></svg></span></a></div><div class=site-footer-item><a href=/files/resume_max_halford.pdf><span class=inline-svg><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 392.533 392.533" style="enable-background:new 0 0 392.533 392.533"><g><g><path fill="currentcolor" d="M292.396 324.849H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 329.697 298.473 324.849 292.396 324.849z"/></g></g><g><g><path fill="currentcolor" d="M292.396 277.01H99.879c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h192.582c6.012.0 10.925-4.849 10.925-10.925C303.321 281.859 298.473 277.01 292.396 277.01z"/></g></g><g><g><path fill="currentcolor" d="M196.137 45.834c-25.859.0-46.998 21.075-46.998 46.998.0 25.859 21.139 46.933 46.998 46.933s46.998-21.075 46.998-46.998-21.139-46.933-46.998-46.933zm0 72.017c-13.77.0-25.083-11.313-25.083-25.083s11.248-25.083 25.083-25.083 25.083 11.313 25.083 25.083c0 13.769-11.313 25.083-25.083 25.083z"/></g></g><g><g><path fill="currentcolor" d="M258.521 163.362c-39.887-15.515-84.752-15.515-124.638.0-13.059 5.107-21.786 18.101-21.786 32.388v44.347c-.065 6.012 4.849 10.925 10.861 10.925h146.424c6.012.0 10.925-4.848 10.925-10.925V195.75C280.307 181.463 271.58 168.469 258.521 163.362zm0 65.874H133.883v-33.422c0-5.301 3.168-10.214 7.887-12.024 34.844-13.511 74.02-13.511 108.865.0 4.719 1.875 7.887 6.659 7.887 12.024v33.422z"/></g></g><g><g><path fill="currentcolor" d="M313.083.0H131.491c-8.404.0-16.291 3.232-22.238 9.18L57.018 61.414c-5.947 5.948-9.18 13.834-9.18 22.238v277.333c0 17.39 14.158 31.547 31.547 31.547h233.762c17.39.0 31.547-14.158 31.547-31.547V31.547C344.501 14.158 330.343.0 313.083.0zM112.032 37.236v27.022H85.01l27.022-27.022zm210.683 79.58h-40.598c-6.012.0-10.925 4.849-10.925 10.925.0 6.012 4.848 10.925 10.925 10.925h40.598v19.394h-14.869c-6.012.0-10.925 4.848-10.925 10.925.0 6.012 4.849 10.925 10.925 10.925h14.869v181.139c0 5.366-4.331 9.697-9.632 9.697H79.192c-5.301.0-9.632-4.331-9.632-9.632V86.044h53.398c6.012.0 10.925-4.848 10.925-10.925V21.721h179.2c5.301.0 9.632 4.331 9.632 9.632v85.463z"/></g></g><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/><g/></svg></span></a></div><div class=site-footer-item><a href=https://play.spotify.com/user/1166811350><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 168 168"><path fill="currentcolor" d="m83.996.277C37.747.277.253 37.77.253 84.019c0 46.251 37.494 83.741 83.743 83.741 46.254.0 83.744-37.49 83.744-83.741.0-46.246-37.49-83.738-83.745-83.738l.001-.004zm38.404 120.78c-1.5 2.46-4.72 3.24-7.18 1.73-19.662-12.01-44.414-14.73-73.564-8.07-2.809.64-5.609-1.12-6.249-3.93-.643-2.81 1.11-5.61 3.926-6.25 31.9-7.291 59.263-4.15 81.337 9.34 2.46 1.51 3.24 4.72 1.73 7.18zm10.25-22.805c-1.89 3.075-5.91 4.045-8.98 2.155-22.51-13.839-56.823-17.846-83.448-9.764-3.453 1.043-7.1-.903-8.148-4.35-1.04-3.453.907-7.093 4.354-8.143 30.413-9.228 68.222-4.758 94.072 11.127 3.07 1.89 4.04 5.91 2.15 8.976v-.001zm.88-23.744c-26.99-16.031-71.52-17.505-97.289-9.684-4.138 1.255-8.514-1.081-9.768-5.219-1.254-4.14 1.08-8.513 5.221-9.771 29.581-8.98 78.756-7.245 109.83 11.202 3.73 2.209 4.95 7.016 2.74 10.733-2.2 3.722-7.02 4.949-10.73 2.739z"/></svg></span></a></div><div class=site-footer-item><a href=mailto:maxhalford25@gmail.com><span class=inline-svg><svg viewBox="0 0 15 20" xmlns="http://www.w3.org/2000/svg"><title>mail</title><path fill="currentcolor" d="M0 4v8c0 .55.45 1 1 1h12c.55.0 1-.45 1-1V4c0-.55-.45-1-1-1H1c-.55.0-1 .45-1 1zm13 0L7 9 1 4h12zM1 5.5l4 3-4 3v-6zM2 12l3.5-3L7 10.5 8.5 9l3.5 3H2zm11-.5-4-3 4-3v6z" fill="#000" fill-rule="evenodd"/></svg></span></a></div><div class=site-footer-item><a href=/index.xml><span class=inline-svg><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path fill="currentcolor" d="M12.8 16C12.8 8.978 7.022 3.2.0 3.2V0c8.777.0 16 7.223 16 16h-3.2zM2.194 11.61c1.21.0 2.195.985 2.195 2.196.0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017.0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818.0 10.606 4.79 10.606 10.607z"/></svg></span></a></div></div></div></div></article><script></script></body></html>